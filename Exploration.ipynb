{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "path_root = Path( '/project_ghent/HADSP/reservoirpy/')\n",
    "sys.path.append(str(path_root))\n",
    "path_root = Path( '/project_ghent/HADSP/hadsp/')\n",
    "sys.path.append(str(path_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f790cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from seaborn import heatmap, color_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67127097-7065-4551-9b82-3d12c176a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "# Utility functions\n",
    "from reservoir.utility import show_matrice\n",
    "\n",
    "# SEED\n",
    "SEED = 49387"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7bc227",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reservoir functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f980d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T09:31:36.544255Z",
     "start_time": "2022-06-10T09:31:36.544249Z"
    },
    "init_cell": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from reservoir.reservoir import update_reservoir, ridge_regression, train, run\n",
    "from reservoir.activation_functions import tanh, heaviside, sigmoid\n",
    "\n",
    "# the activation function choosen for the rest of the experiment\n",
    "# activation_function = lambda x : sigmoid(2*(x-0.5))\n",
    "activation_function = lambda x : tanh(x)\n",
    "\n",
    "plt.plot(np.linspace(0, 2, 100), activation_function(np.linspace(0, 2, 100)))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44384e-7cbc-4611-a015-e1c073f61aa9",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "Lots of different on availabale : https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad\n",
    "\n",
    "Classification: \n",
    "https://arxiv.org/abs/1803.07870\n",
    "\n",
    "https://github.com/FilippoMB/Time-series-classification-and-clustering-with-Reservoir-Computing\n",
    "\n",
    "Multivariate:\n",
    "https://www.timeseriesclassification.com/dataset.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc3d35-72f1-4fc8-a452-e0699b4d507d",
   "metadata": {},
   "source": [
    "## Mackey-Glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb2dcf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from reservoirpy.datasets import mackey_glass\n",
    "\n",
    "timesteps = 10000\n",
    "mg_inputs = mackey_glass(timesteps, tau=17, a=0.2, b=0.1, n=10, x0=1.2, h=1, seed=None)\n",
    "\n",
    "# Define the time step of your Mackey-Glass system\n",
    "dt = 0.00001\n",
    "\n",
    "# Compute the equivalent sampling rate\n",
    "sampling_rate = 1 / dt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(range(1000), mg_inputs[:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016e3f6-92e5-4403-a145-9eaf9af0cdac",
   "metadata": {},
   "source": [
    "## Free Spoken Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0675739-5d2e-497f-b990-008b7d0e5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.load_datasets import load_FSDD_dataset\n",
    "\n",
    "data_dir = 'datasets/fsdd/free-spoken-digit-dataset-master/recordings'  # Path to the extracted dataset\n",
    "sampling_rate, X_train, X_test, Y_train, Y_test = load_FSDD_dataset(data_dir, seed=SEED)\n",
    "# Check the shapes of the datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468ca5c",
   "metadata": {},
   "source": [
    "# New inputs creation with band filters\n",
    "\n",
    "Spectrograms_vs_Cochleagrams : \n",
    "* https://www.researchgate.net/publication/340510607_Speech_recognition_using_very_deep_neural_networks_Spectrograms_vs_Cochleagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008da94-d239-4c67-b563-db3cbdfa7895",
   "metadata": {},
   "source": [
    "## Pretrain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ba2b7-c2d0-4659-8106-296607b228b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a long time (15min with half the samples, instant with 20 which is enought for pretraining)\n",
    "X_pretrain = np.concatenate(X_train[:20], axis=0)\n",
    "print(X_pretrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e57e07-e48c-4e28-80e9-ad95894d0a2e",
   "metadata": {},
   "source": [
    "## Spectral density and peak selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ad0f4-4dbc-4b97-8d28-8ffb357715d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.multivariate_generation import generate_multivariate_dataset, extract_peak_frequencies\n",
    "\n",
    "filtered_peak_freqs = extract_peak_frequencies(X_pretrain.flatten(), sampling_rate, nperseg=1024, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766304bc-3c47-4813-9828-91d8fed5845b",
   "metadata": {},
   "source": [
    "## Applying normal band pass filter on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690a984-394f-4be3-912c-ee2ca4cc1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pretrain_band, X_train_band, X_test_band = generate_multivariate_dataset(filtered_peak_freqs, X_pretrain, X_train, X_test, sampling_rate, nb_jobs=-1, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b859e5-c6dd-4086-ac56-42a7de3ed25e",
   "metadata": {},
   "source": [
    "## Standardizing the amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45e001-4920-47e1-8dbb-6681cf9b32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# pretrain\n",
    "filtered_data = scaler.fit_transform(X_pretrain_band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f35f5-0636-4732-86f4-5c60e9cd5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train_band = [np.array([scaler.fit_transform(time_serie.reshape(-1, 1)).flatten() for time_serie in x]) for x in X_train_band]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff37f7-38d5-4fcf-a36e-7f9edb22d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "X_test_band = [np.array([scaler.fit_transform(time_serie.reshape(-1, 1)).flatten() for time_serie in x]) for x in X_test_band]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3946e-5918-489b-a18d-a92a8f751adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_band[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93f662-f197-4b0a-a140-090f3c0909d2",
   "metadata": {},
   "source": [
    "# Generating reservoirs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385de4f8",
   "metadata": {},
   "source": [
    "## Creating from HADSP + bandfilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b39f1d-d8d6-451b-8d4d-b8f8f74a498d",
   "metadata": {},
   "source": [
    "### Plot  pretrain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab965ec2-e31c-475e-b452-05c9dbfde13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min window size to get all the dynamics ? \n",
    "min_window_size = sampling_rate/filtered_peak_freqs[-1]\n",
    "\n",
    "min_window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd00d3f-9b0c-475f-bac9-b46e5f92adfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Compute the moving average \n",
    "window_size = 5\n",
    "weights = np.repeat(1.0, window_size)/window_size\n",
    "ma = np.array([np.convolve(d, weights, 'valid') for d in (filtered_data)])\n",
    "\n",
    "#CPlot the two for different frequencies\n",
    "NB_1 = 3\n",
    "fig, ax = plt.subplots(2, 1, figsize=(24,12))\n",
    "ax[0].plot(range(500), filtered_data[NB_1, 1000:1500], label='Time serie')\n",
    "ax[0].plot(range(500), ma[NB_1, 1000:1500], label='Moving average')\n",
    "NB_2 = 10\n",
    "ax[0].legend(fontsize=26)\n",
    "ax[1].plot(range(500), filtered_data[NB_2, 1000:1500], label='Time serie')\n",
    "ax[1].plot(range(500), ma[NB_2, 1000:1500], label='Moving average')\n",
    "\n",
    "ax[0].spines['top'].set_visible(False)\n",
    "ax[0].spines['right'].set_visible(False)\n",
    "ax[1].spines['top'].set_visible(False)\n",
    "ax[1].spines['right'].set_visible(False)\n",
    "ax[0].tick_params(axis='both', labelsize=26)\n",
    "ax[1].tick_params(axis='both', labelsize=26)\n",
    "\n",
    "\n",
    "# draw vertical lines to represent the window for some points\n",
    "for x in range(100, 500, 100):\n",
    "    ax[0].axvspan(x, x+window_size, color='g', alpha=0.2)\n",
    "for x in range(100, 500, 100):\n",
    "    ax[1].axvspan(x, x+window_size, color='g', alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df206ba-36a2-4468-b719-7bc819114b1a",
   "metadata": {},
   "source": [
    "## Construct matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6435e2-6f19-4987-9309-ab74694678fd",
   "metadata": {},
   "source": [
    "### Shared parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f44547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "# We want the size of the reservoir to be at least 200\n",
    "K = math.ceil(200 / filtered_peak_freqs.shape[0])\n",
    "n = filtered_peak_freqs.shape[0] * K\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f51d2-97d0-49dd-8aa2-889b4ceb9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reservoir.reservoir\n",
    "from reservoir.reservoir import update_reservoir\n",
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.utility import compute_synaptic_change\n",
    "from connexion_generation.bounded_adsp import bounded_adsp\n",
    "\n",
    "\n",
    "class TwoDimArrayWrapper:\n",
    "    def __init__(self, input_data):\n",
    "        if input_data.ndim != 2:\n",
    "            raise ValueError(\"Expected a 2D array.\")\n",
    "        self.input_data = input_data\n",
    "        self.shape = input_data.shape\n",
    "        self.size = input_data.shape[1]\n",
    "        self.flat_data = input_data.flatten()\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # Handle single element access\n",
    "        return self.input_data[:, key]\n",
    "        \n",
    "def run_HADSP_algorithm(W, Win, bias, input_data, visualize=False):\n",
    "    # last_state\n",
    "    state = np.random.uniform(0, 1, n)\n",
    "    state_history = []\n",
    "    \n",
    "    total_add = 0\n",
    "    total_prun = 0\n",
    "    add = []\n",
    "    prun = []\n",
    "    step=0\n",
    "\n",
    "    for i in range(INCREMENT*5):\n",
    "        state = update_reservoir(W, Win, input_data[i], state, leaky_rate, bias, activation_function)\n",
    "        state_history.append(state)\n",
    "\n",
    "    # size of simulation \n",
    "    number_steps = int((input_data.size-INCREMENT*5)/INCREMENT)\n",
    "    for k in range(number_steps): \n",
    "        delta_z = compute_synaptic_change(state_history[-INCREMENT:], target_rate, growth_parameter, average=\"WHOLE\")\n",
    "        W, _, nb_new_add, nb_new_prun = bounded_adsp(W, state, delta_z, VALUE)\n",
    "    \n",
    "        for i in range(INCREMENT):\n",
    "            state = update_reservoir(W, Win, input_data[INCREMENT*(k+5)+i], state, leaky_rate, bias, activation_function)\n",
    "            state_history.append(state)\n",
    "            \n",
    "        total_add += nb_new_add\n",
    "        total_prun += nb_new_prun\n",
    "        add.append(total_add)\n",
    "        prun.append(total_prun)\n",
    "        step += 1\n",
    "        \n",
    "    add = np.array(add)\n",
    "    prun = np.array(prun)\n",
    "\n",
    "    if visualize:\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(step)*INCREMENT, add, label=\"total number of added connexion\")\n",
    "        plt.plot(np.arange(step)*INCREMENT, prun, label=\"total number of prunned connexion\")\n",
    "        plt.plot(np.arange(step)*INCREMENT, add-prun, label=\"difference\")\n",
    "        plt.plot(np.arange(step)*INCREMENT, [0]*step, linestyle=(0, (1, 10)))\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    return W\n",
    "\n",
    "def initialise_and_train(input_scaling, n, bias_scaling, seed, training_set, visualize=False):\n",
    "    Win, W, bias = init_matrices(n, 1, 0, seed=seed)\n",
    "    bias *= bias_scaling\n",
    "    Win *= input_scaling\n",
    "    \n",
    "    W = run_HADSP_algorithm(W, Win, bias, training_set, visualize=visualize)\n",
    "\n",
    "    connectivity =  W.count_nonzero() / (W.shape[0] * W.shape[1])\n",
    "    eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "    sr = max(abs(eigen))\n",
    "    \n",
    "    return Win, W, bias, connectivity, sr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7404a-686a-4252-88b2-c139c55e4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reservoirpy.nodes import Reservoir, Ridge, Input, ESN\n",
    "from scipy.sparse import csr_matrix\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tqdm\n",
    "tqdm.tqdm = lambda *args, **kwargs: args[0]  # Mocks tqdm\n",
    "\n",
    "N_JOBS = 4\n",
    "\n",
    "def train_and_predict_model(W, Win, bias, activation_function, ridge_coef, compute_data, predict_data):\n",
    "    # To remember : \n",
    "    #  For reservoirpy   pre_s = W @ r + Win @ (u + noise_gen(dist=dist, shape=u.shape, gain=g_in)) + bias\n",
    "    \n",
    "    reservoir = Reservoir(units=n, \n",
    "                          W =csr_matrix(W), \n",
    "                          Win=csr_matrix(np.diag(Win.toarray().flatten())), \n",
    "                          bias=csr_matrix(bias).T, \n",
    "                          activation=activation_function,\n",
    "                          equation='external'\n",
    "                         )\n",
    "    readout = Ridge(ridge=ridge_coef)\n",
    "    model = ESN(reservoir=reservoir, readout=readout)\n",
    "    \n",
    "    states_train = []\n",
    "\n",
    "    def compute_state(x):\n",
    "        return reservoir.run(x, reset=True)[-1, np.newaxis].flatten()\n",
    "    \n",
    "    states_train = Parallel(n_jobs=N_JOBS)(delayed(compute_state)(x) for x in compute_data)\n",
    "\n",
    "    readout.fit(np.array(states_train), Y_train)\n",
    "\n",
    "    Y_pred = []\n",
    "    def predict(x):\n",
    "        states = reservoir.run(x, reset=True)\n",
    "        y = readout.run(states[-1, np.newaxis])\n",
    "        return y\n",
    "\n",
    "    Y_pred = Parallel(n_jobs=N_JOBS)(delayed(predict)(x) for x in predict_data)\n",
    "\n",
    "    return Y_pred\n",
    "\n",
    "def compute_score(Y_pred, Y_test, model_name):\n",
    "    Y_pred_class = [np.argmax(y_p) for y_p in Y_pred]\n",
    "    Y_test_class = [np.argmax(y_t) for y_t in Y_test]\n",
    "\n",
    "    score = accuracy_score(Y_test_class, Y_pred_class)\n",
    "\n",
    "    print(f\"Accuracy for {model_name}: {score * 100:.3f} %\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45058f-89f6-4fb3-bfe7-b1799f31614b",
   "metadata": {},
   "source": [
    "### Multivariate matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7fe500-b3fd-46e3-ab6c-1233d8091648",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_bands = np.repeat(filtered_data, K, axis=0)\n",
    "\n",
    "frequency_bands = TwoDimArrayWrapper(frequency_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d3f56-63f5-4b68-8fdc-5d19dcdfadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the size of the reservoir to be at least 200\n",
    "caca = []\n",
    "caca_test = []\n",
    "for i in range(len(X_train_band)):\n",
    "    #caca.append(np.repeat(X_train[i], K, axis=1))\n",
    "    caca.append(np.repeat(X_train_band[i], K, axis=0).T) # axis still depend of X_train shape\n",
    "for i in range(len(X_test_band)):\n",
    "    #caca_test.append(np.repeat(X_test[i], K, axis=1))\n",
    "    caca_test.append(np.repeat(X_test_band[i], K, axis=0).T)\n",
    "\n",
    "\n",
    "# Create a list to store the arrays\n",
    "\n",
    "pipi = []\n",
    "pipi_test = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    pipi.append(np.repeat(X_train[i], n, axis=1))\n",
    "    #pipi.append(np.repeat(X_train[i], n, axis=0).T) # axis still depend of X_train shape\n",
    "    \n",
    "for i in range(len(X_test)):\n",
    "    pipi_test.append(np.repeat(X_test[i], n, axis=1))\n",
    "    #pipi_test.append(np.repeat(X_test[i], n, axis=0).T)\n",
    "    \n",
    "pipi[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b9832-7af6-4b76-8d33-7971ad3b8cb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_hadsp_multi = []\n",
    "scores_random_multi = []\n",
    "scores_hadsp_uni = []\n",
    "scores_random_uni = []\n",
    "\n",
    "INCREMENT = 5\n",
    "VALUE = 0.05\n",
    "target_rate = 0.7\n",
    "growth_parameter = 0.3\n",
    "\n",
    "bias_scaling = 1\n",
    "leaky_rate = 1\n",
    "ridge_coef = 1e-6\n",
    "\n",
    "for input_scaling in [0.1, 0.2, 0.3]: \n",
    "    # HADSP + multi\n",
    "    Win_hasp_band, W_hasp_band, bias_hasp_band, connectivity_band, sr_adsp = initialise_and_train(input_scaling, n, bias_scaling, SEED, frequency_bands)\n",
    "    \n",
    "    # 3rd (random + multi)\n",
    "    Win_3, W_3, bias_3 =  init_matrices(n, 1, connectivity_band, sr_adsp)\n",
    "    bias_3= bias_3*bias_scaling\n",
    "    Win_3= Win_3*input_scaling\n",
    "    \n",
    "    eigen_3 = sparse.linalg.eigs(W_3, k=1, which=\"LM\", maxiter=W_3.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "    sr_3 = max(abs(eigen_3))\n",
    "    \n",
    "    # HADSP + uni\n",
    "    Win_hadsp_uni, W_hadsp_uni, bias_hadsp_uni, connectivity_hadsp_uni, sr_hadsp_uni = initialise_and_train(input_scaling, n, bias_scaling, SEED, X_pretrain.flatten())\n",
    "\n",
    "    # random + uni\n",
    "    Win_normal, W_normal, bias_normal =  init_matrices(n, 1, connectivity_hadsp_uni)\n",
    "    bias_normal= bias_normal*bias_scaling\n",
    "    Win_normal= Win_normal*input_scaling   \n",
    "\n",
    "    eigen_normal = sparse.linalg.eigs(W_normal, k=1, which=\"LM\", maxiter=W_normal.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "    sr_normal = max(abs(eigen_normal))\n",
    "\n",
    "\n",
    "    \n",
    "    # Spectral radius normalisation\n",
    "    normal_sr = 1.2\n",
    "    W_normal = W_normal/sr_normal*normal_sr\n",
    "    W_hadsp_uni = W_hadsp_uni/sr_hadsp_uni*normal_sr\n",
    "    W_3 = W_3/sr_3*normal_sr\n",
    "    W_hasp_band = W_hasp_band/sr_adsp*normal_sr\n",
    "\n",
    "    \n",
    "    #  HADSP + multivariate dataset\n",
    "    Y_pred_hasp_band = train_and_predict_model(W_hasp_band, Win_hasp_band, bias_hasp_band, activation_function, ridge_coef, caca, caca_test)\n",
    "    # random + multivariate dataset\n",
    "    Y_pred_3 = train_and_predict_model(W_3, Win_3, bias_3, activation_function, ridge_coef, caca, caca_test)\n",
    "    \n",
    "    # HADSP + univariate dataset\n",
    "    Y_pred_hadsp_uni = train_and_predict_model(W_hadsp_uni, Win_hadsp_uni, bias_hadsp_uni, activation_function, ridge_coef, pipi, pipi_test)\n",
    "    # random + multivariate dataset\n",
    "    Y_pred_normal = train_and_predict_model(W_normal, Win_normal, bias_normal, activation_function, ridge_coef, pipi, pipi_test)\n",
    "\n",
    "    \n",
    "    scores_hadsp_multi.append(compute_score(Y_pred_hasp_band, Y_test, \"HADSP multi\"))\n",
    "    scores_random_multi.append(compute_score(Y_pred_3, Y_test, \"random multi\"))\n",
    "\n",
    "    scores_hadsp_uni.append(compute_score(Y_pred_hadsp_uni, Y_test, \"HADSP uni\"))\n",
    "    scores_random_uni.append(compute_score(Y_pred_normal, Y_test, \"random uni\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3525c31-24fc-450e-8a17-f5801e42e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the mean and standard deviation of the scores\n",
    "mean_hadsp_multi = np.mean(scores_hadsp_multi)\n",
    "std_hadsp_multi = np.std(scores_hadsp_multi)\n",
    "print(f'Mean and Std for HADSP multi: {mean_hadsp_multi}, {std_hadsp_multi}')\n",
    "\n",
    "mean_random_multi = np.mean(scores_random_multi)\n",
    "std_random_multi = np.std(scores_random_multi)\n",
    "print(f'Mean and Std for random multi: {mean_random_multi}, {std_random_multi}')\n",
    "\n",
    "mean_hadsp_uni = np.mean(scores_hadsp_uni)\n",
    "std_hadsp_uni = np.std(scores_hadsp_uni)\n",
    "print(f'Mean and Std for HADSP uni: {mean_hadsp_uni}, {std_hadsp_uni}')\n",
    "\n",
    "mean_random_uni = np.mean(scores_random_uni)\n",
    "std_random_uni = np.std(scores_random_uni)\n",
    "print(f'Mean and Std for random uni: {mean_random_uni}, {std_random_uni}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e97b50-735c-4413-a7bf-e136679d53fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f56088-dfe1-4646-a413-87108460cfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79b55e-ab4b-4f76-9e71-5f4274666e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26155189-0804-473d-b69d-1c80b40bd2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f621a-c8db-4b6e-92ff-d9e1c9ef9e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688bdf0b-2ad2-4526-b259-7b50c77dfe09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567fe1b-db79-44d1-abd8-88d7cee34441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd657f-d0fe-42b5-a3c1-c0d5db353913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2478c-bb79-4bc0-855f-b028109c8022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf0ae95-2438-4b45-8abd-4614e0281439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
