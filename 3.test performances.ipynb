{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f790cf",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "SEED = 923984"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44384e-7cbc-4611-a015-e1c073f61aa9",
   "metadata": {},
   "source": [
    "# Datasets loading\n",
    "\n",
    "Lots of different on availabale : https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad\n",
    "\n",
    "Regression : http://tseregression.org/ + https://arxiv.org/pdf/2012.02974"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d6272-708d-4709-8e35-5a84268bed64",
   "metadata": {},
   "source": [
    "## Prediction ahead\n",
    "\n",
    "Datasets available :\n",
    "\n",
    "* MackeyGlass\n",
    "* Lorenz\n",
    "* Sunspot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ee01d-db82-4cf0-afea-e7fbf1828d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets.load_datasets import load_dataset_prediction\n",
    "is_instances_classification = False\n",
    "dataset_name = \"Sunspot\"\n",
    "step_ahead=5\n",
    "\n",
    "is_multivariate, sampling_rate, X_train_raw, X_test_raw, Y_train_raw, Y_test = load_dataset_prediction(dataset_name, step_ahead, visualize=True)\n",
    "use_spectral_representation = False\n",
    "spectral_representation = None # Can be None, \"stft\" or \"mfcc\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a473ab10-185d-4ff3-a11c-b4e6e4ff750d",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b219b-eb64-4715-b983-7de3c392f088",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Datasets available :\n",
    "\n",
    "* Custom :  FSDD, HAART, JapaneseVowels\n",
    "* Aeon : SpokenArabicDigits, CatsDogs, LSST\n",
    "* Torchaudio: SPEECHCOMMANDS\n",
    "\n",
    "More on https://www.timeseriesclassification.com/dataset.php or https://pytorch.org/audio/stable/datasets.html"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bb9e213-15eb-4285-8e62-f08330be3b34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T16:32:19.629216Z",
     "iopub.status.busy": "2024-09-08T16:32:19.629093Z",
     "iopub.status.idle": "2024-09-08T16:32:21.193895Z",
     "shell.execute_reply": "2024-09-08T16:32:21.193584Z",
     "shell.execute_reply.started": "2024-09-08T16:32:19.629206Z"
    }
   },
   "source": [
    "from datasets.load_classification import load_dataset_classification\n",
    "is_instances_classification = True\n",
    "spectral_representation = \"mfcc\"  # Can be None, \"stft\" or \"mfcc\"\n",
    "\n",
    "dataset_name = \"FSDD\"\n",
    "\n",
    "use_spectral_representation, is_multivariate, sampling_rate, X_train_raw, X_test_raw, Y_train_raw, Y_test, groups = load_dataset_classification(dataset_name)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "ax.plot(range(len(X_train_raw[0])), X_train_raw[0])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "fontsize=30\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Time', size=fontsize)\n",
    "plt.ylabel('Value', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86961904-ad36-4af8-ad21-6d51f491da61",
   "metadata": {},
   "source": [
    "# Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a38d2df41a727",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from reservoir.activation_functions import tanh, heaviside, sigmoid\n",
    "\n",
    "# the activation function choosen for the rest of the experiment\n",
    "# activation_function = lambda x : sigmoid(2*(x-0.5))tanh(x)\n",
    "activation_function = lambda x : tanh(x)\n",
    "\n",
    "plt.plot(np.linspace(0, 3, 100), activation_function(np.linspace(0, 3, 100)))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f892fb1cccd7511",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca76d0e06f936c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math \n",
    " \n",
    "# Cross validation\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit, StratifiedGroupKFold\n",
    "from datasets.preprocessing import flexible_indexing\n",
    "\n",
    "#Preprocessing\n",
    "from datasets.multivariate_generation import generate_multivariate_dataset, extract_peak_frequencies\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets.preprocessing import scale_data\n",
    "from datasets.preprocessing import add_noise, duplicate_data\n",
    "\n",
    "# Define noise parameter\n",
    "noise_std = 0.001\n",
    "\n",
    "\n",
    "data_type = \"normal\" # \"normal\" ou \"noisy\"\n",
    "\n",
    "WINDOW_LENGTH = 10\n",
    "freq_train_data = X_train_raw\n",
    "flat_train_data = np.concatenate(freq_train_data, axis=0) if is_instances_classification else freq_train_data\n",
    "extract_peak_frequencies(flat_train_data, sampling_rate, smooth=True, window_length=WINDOW_LENGTH, threshold=1e-5, nperseg=1024, visualize=True)\n",
    "\n",
    "if is_multivariate:\n",
    "    X_train_band, X_test_band = X_train_raw, X_test_raw\n",
    "    del X_train_raw, X_test_raw\n",
    "    X_val_band = None\n",
    "else:\n",
    "    X_test, X_train = X_test_raw, X_train_raw\n",
    "    X_val, X_val_band = None, None\n",
    "    del X_train_raw, X_test_raw\n",
    "Y_train = Y_train_raw\n",
    "del Y_train_raw\n",
    "        \n",
    "# PREPROCESSING    \n",
    "freq_train_data = X_train_band if is_multivariate else X_train\n",
    "flat_train_data = np.concatenate(freq_train_data, axis=0) if is_instances_classification else freq_train_data\n",
    "peak_freqs = extract_peak_frequencies(flat_train_data, sampling_rate, smooth=True, window_length=WINDOW_LENGTH, threshold=1e-5, nperseg=1024, visualize=False)\n",
    "\n",
    "\n",
    "if not is_multivariate:\n",
    "    X_train_band = generate_multivariate_dataset(\n",
    "        X_train, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "    )\n",
    "    \n",
    "    X_test_band = generate_multivariate_dataset(\n",
    "        X_test, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "    )\n",
    "elif not use_spectral_representation:\n",
    "    X_train_band = generate_multivariate_dataset(\n",
    "        X_train_band, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "    )\n",
    "    X_test_band = generate_multivariate_dataset(\n",
    "        X_test_band, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "    )\n",
    "else:\n",
    "    print(\"Data is already spectral, nothing to do\")\n",
    "    \n",
    "scaler_multi = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_band, X_val_band, X_test_band = scale_data(X_train_band, X_val_band, X_test_band, scaler_multi, is_instances_classification)\n",
    "            \n",
    "if not is_multivariate:\n",
    "    scaler_x_uni = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train, X_val, X_test = scale_data(X_train, X_val, X_test, scaler_multi, is_instances_classification)       \n",
    "\n",
    "# NOISE\n",
    "if data_type == \"noisy\":\n",
    "    if is_instances_classification:\n",
    "        # UNI\n",
    "        if not is_multivariate:\n",
    "            X_train_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_train, desc=\"TRAIN\")]\n",
    "            X_test_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_test, desc=\"TEST\")]\n",
    "            \n",
    "        # MULTI\n",
    "        X_train_band_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_train_band, desc=\"TRAIN\")]\n",
    "        X_test_band_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_test_band, desc=\"TEST\")]\n",
    "    \n",
    "    else:  #if prediction\n",
    "        # UNI\n",
    "        if not is_multivariate:\n",
    "            X_train_noisy = add_noise(X_train, noise_std)\n",
    "            X_test_noisy = add_noise(X_test, noise_std)\n",
    "    \n",
    "        # MULTI\n",
    "        X_train_band_noisy = add_noise(X_train_band, noise_std)\n",
    "        X_test_band_noisy = add_noise(X_test_band, noise_std)\n",
    "\n",
    "# Define the number of instances you want to select\n",
    "# Define the number of instances you want to select\n",
    "x_size = len(X_train_band) if is_multivariate else len(X_train)\n",
    "num_samples_for_pretrain = 500 if x_size >= 500 else x_size\n",
    "if is_instances_classification:\n",
    "    indices = np.random.choice(x_size, num_samples_for_pretrain, replace=False)\n",
    "else:\n",
    "    indices = range(x_size)\n",
    "\n",
    "\n",
    "if data_type == \"noisy\":\n",
    "    # Defining pretrain   \n",
    "    if not is_multivariate:\n",
    "        X_pretrain_noisy = np.array(X_train_noisy)[indices]\n",
    "    X_pretrain_band_noisy = np.array(X_train_band_noisy)[indices]\n",
    "\n",
    "if not is_multivariate:\n",
    "    X_pretrain_uni = np.array(X_train)[indices]\n",
    "X_pretrain_band = np.array(X_train_band)[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c5610-8dad-4d05-8bbb-fa34031d9c9c",
   "metadata": {},
   "source": [
    "# Test data evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92b07a8-db33-46d2-a52c-f9ce16a1985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse, stats\n",
    "from numpy import random\n",
    "import os\n",
    "\n",
    "#Pretraining\n",
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.hag import run_algorithm\n",
    "\n",
    "# Evaluating\n",
    "from performances.esn_model_evaluation import init_and_train_model_for_classification, predict_model_for_classification, compute_score\n",
    "from performances.esn_model_evaluation import init_and_train_model_for_prediction\n",
    "\n",
    "import optuna\n",
    "from performances.utility import camel_to_snake\n",
    "\n",
    "if is_instances_classification:\n",
    "    file_name = \"test_results_classification.csv\"\n",
    "else: \n",
    "    file_name = \"test_results_prediction.csv\"\n",
    "    \n",
    "\n",
    "def evaluate_dataset_on_test(dataset_name, function_name, data_type):\n",
    "    # Get the best trial from the study\n",
    "    url= \"sqlite:///optuna_\" + camel_to_snake(dataset_name) + \"_db.sqlite3\"\n",
    "    study_name = function_name + \"_\" + dataset_name + \"_\" + data_type + \"_\" + variate_type\n",
    "    study = optuna.load_study(study_name=study_name, storage=url) # To load an existing study\n",
    "    best_trial = study.best_trial\n",
    "    \n",
    "     # Collect all hyperparameters in a dictionary\n",
    "    hyperparams = {param_name: param_value for param_name, param_value in best_trial.params.items()}\n",
    "\n",
    "    # Add default values or other logic adjustments\n",
    "    if 'variance_target' not in hyperparams and 'min_variance' in hyperparams:\n",
    "        hyperparams['variance_target'] = hyperparams['min_variance']\n",
    "    \n",
    "\n",
    "    if not is_instances_classification:\n",
    "        use_full_instance = None\n",
    "\n",
    "    nb_trials = 8\n",
    "    nb_jobs = 8\n",
    "    \n",
    "    start_step, end_step = 30, 500\n",
    "    SLICE_RANGE = slice(start_step, end_step)\n",
    "    min_reservoir_size = 500\n",
    "    min_window_size = sampling_rate/np.max(np.hstack(peak_freqs))\n",
    "    max_window_size = sampling_rate/np.min(np.hstack(peak_freqs))\n",
    "    RIDGE_COEF = 10**hyperparams['ridge']\n",
    "    \n",
    "    if function_name == \"hadsp\" or function_name == \"desp\":\n",
    "        max_increment_span = int(max_window_size) if int(max_window_size) - 100 < 0 else int(max_window_size) - 100\n",
    "        MAX_TIME_INCREMENT = hyperparams['time_increment'] + hyperparams['time_increment_span'] #int(max_window_size) or None or TIME_INCREMENT\n",
    "    \n",
    "    scores = [] \n",
    "    for i in range(nb_trials):\n",
    "        if variate_type == \"multi\":\n",
    "            if is_instances_classification:\n",
    "                common_index = 1\n",
    "                common_size = X_train_band[0].shape[common_index]\n",
    "            else:\n",
    "                common_index = 1\n",
    "                common_size = X_train_band.shape[common_index]\n",
    "        else:\n",
    "            common_size = len(peak_freqs)\n",
    "            \n",
    "        # We want the size of the reservoir to be at least network_size\n",
    "        K = math.ceil(hyperparams['network_size'] / common_size)\n",
    "        n = common_size * K\n",
    "    \n",
    "        pretrain_data = X_pretrain_band\n",
    "        train_data = X_train_band  # X_train_band_noisy_duplicated or X_train_band_duplicated\n",
    "        test_data = X_test_band\n",
    "    \n",
    "        # UNSUPERVISED PRETRAINING \n",
    "        if function_name == \"random_ei\":\n",
    "            Win, W, bias = init_matrices(n, hyperparams['input_connectivity'], hyperparams['connectivity'],  K, w_distribution=stats.uniform(-1, 1), seed=random.randint(0, 1000))\n",
    "        else:\n",
    "            Win, W, bias = init_matrices(n, hyperparams['input_connectivity'], hyperparams['connectivity'],  K, seed=random.randint(0, 1000))\n",
    "        bias *= hyperparams['bias_scaling']\n",
    "        Win *= hyperparams['input_scaling']\n",
    "    \n",
    "        if function_name == \"hadsp\":\n",
    "            W, (_, _, _) = run_algorithm(W, Win, bias, hyperparams['leaky_rate'], activation_function, pretrain_data, hyperparams['time_increment'], hyperparams['weight_increment'],\n",
    "                                     hyperparams['target_rate'], hyperparams['rate_spread'], function_name, is_instance=is_instances_classification, use_full_instance=use_full_instance,\n",
    "                                     max_increment=MAX_TIME_INCREMENT, max_partners=hyperparams['max_partners'], method=hyperparams['method'], n_jobs=nb_jobs)\n",
    "        elif function_name == \"desp\":\n",
    "                W, (_, _, _) = run_algorithm(W, Win, bias, hyperparams['leaky_rate'], activation_function, pretrain_data, hyperparams['time_increment'], hyperparams['weight_increment'],\n",
    "                                        hyperparams['variance_target'], hyperparams['variance_spread'], function_name, is_instance=is_instances_classification, use_full_instance = use_full_instance, \n",
    "                                        max_increment=MAX_TIME_INCREMENT, max_partners=hyperparams['max_partners'], method = hyperparams['method'], \n",
    "                                        intrinsic_saturation=hyperparams['intrinsic_saturation'], intrinsic_coef=hyperparams['intrinsic_coef'], n_jobs = nb_jobs)\n",
    "        elif function_name == \"random\" or function_name == \"random_ei\":\n",
    "            eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "            W *= hyperparams['spectral_radius'] / max(abs(eigen))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid function: {function_name}\")\n",
    "        \n",
    "    \n",
    "        # TRAINING and EVALUATION\n",
    "        if is_instances_classification:\n",
    "            reservoir, readout = init_and_train_model_for_classification(W, Win, bias, hyperparams['leaky_rate'], activation_function, train_data, Y_train, n_jobs = nb_jobs, ridge_coef=RIDGE_COEF, mode=\"sequence-to-vector\", hide_progress=True)\n",
    "            \n",
    "            Y_pred = predict_model_for_classification(reservoir, readout, test_data, n_jobs = nb_jobs, hide_progress=True)\n",
    "            score = compute_score(Y_pred, Y_test, is_instances_classification)\n",
    "        else:\n",
    "            esn = init_and_train_model_for_prediction(W, Win, bias, hyperparams['leaky_rate'], activation_function, train_data, Y_train, RIDGE_COEF)\n",
    "            \n",
    "            Y_pred =  esn.run(test_data, reset=False)\n",
    "            score = compute_score(Y_pred, Y_test, is_instances_classification)\n",
    "    \n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4957e6c-e0cc-4d90-a495-348a8bbab3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "new_results = pd.DataFrame(columns=['Dataset', 'Function', 'Average Score', 'Standard Deviation', 'Date'])\n",
    "\n",
    "variate_type = \"multi\"  # \"multi\" or \"uni\"\n",
    "\n",
    "\n",
    "# Simulate your data and loop for evaluation\n",
    "print(dataset_name)\n",
    "for function_name in [\"desp\", \"hadsp\", \"random\", \"random_ei\"]:\n",
    "    print(function_name)\n",
    "    scores = evaluate_dataset_on_test(dataset_name, function_name, data_type)\n",
    "    \n",
    "    # Compute the average and standard deviation of the scores\n",
    "    average_score = np.mean(scores)\n",
    "    std_deviation = np.std(scores)\n",
    "\n",
    "    if is_instances_classification:\n",
    "        formatted_average = f\"{round(average_score * 100, 5)} %\"\n",
    "        formatted_std = f\"± {round(std_deviation * 100, 5)} %\"\n",
    "    else:\n",
    "        formatted_average = f\"{round(average_score, 5)}\"\n",
    "        formatted_std = f\"± {round(std_deviation, 5)}\"\n",
    "    \n",
    "    # Capture the current date\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Create a new DataFrame row with the Date column\n",
    "    new_row = pd.DataFrame({\n",
    "        'Dataset': [dataset_name],\n",
    "        'Function': [function_name],\n",
    "        'Average Score': [formatted_average],\n",
    "        'Standard Deviation': [formatted_std],\n",
    "        'Date': [current_date]\n",
    "    })\n",
    "    \n",
    "    # Concatenate the new row to the results DataFrame\n",
    "    new_results = pd.concat([new_results, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "# Display the DataFrame\n",
    "print(new_results)\n",
    "\n",
    "# Load the existing CSV\n",
    "if os.path.exists(file_name):\n",
    "    previous_results = pd.read_csv(file_name)\n",
    "else:\n",
    "    columns = ['Dataset', 'Function', 'Average Score', 'Standard Deviation', 'Date']\n",
    "    previous_results = pd.DataFrame(columns=columns)\n",
    "    previous_results.to_csv(file_name, index=False)\n",
    "    print(f\"{file_name} created successfully.\")\n",
    "    \n",
    "tots_results = pd.concat([new_results, previous_results], axis=0)\n",
    "\n",
    "tots_results.to_csv(file_name, index=False)\n",
    "print(\"Results saved to 'pipeline_results.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ac7df-057c-4d1d-a906-e5be69eb63c0",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8fb13-9755-4b29-afaf-ad63e357878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "if 'file_name' not in locals() and 'file_name' not in globals():\n",
    "    file_name = \"test_results_classification.csv\"    #  test_results_classification.csv or test_results_prediction.csv\n",
    "\n",
    "\n",
    "if os.path.exists(file_name):\n",
    "    previous_results = pd.read_csv(file_name)\n",
    "else:\n",
    "    # File does not exist, create it with the necessary columns\n",
    "    columns = ['Dataset', 'Function', 'Average Score', 'Standard Deviation', 'Date']\n",
    "    previous_results = pd.DataFrame(columns=columns)\n",
    "    # Save the empty DataFrame as a CSV\n",
    "    previous_results.to_csv(file_name, index=False)\n",
    "    print(f\"{file_name} created successfully.\")\n",
    "\n",
    "print(previous_results)\n",
    "print(\"Results saved to 'pipeline_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9c516-5384-46d9-9310-026108bf6b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import heatmap, color_palette\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "all_results = pd.read_csv(file_name)\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "# Clean 'Average Score' and 'Standard Deviation' column (remove '±' and '%' and convert to float)\n",
    "df['Average Score'] = df['Average Score'].astype(str).str.replace('%', '').astype(float)\n",
    "df['Standard Deviation'] = df['Standard Deviation'].str.replace('±', '').str.replace('%', '').astype(float)\n",
    "df['Function'] = df['Function'].str.replace('desp', 'variance HAG')\n",
    "df['Function'] = df['Function'].str.replace('hadsp', 'mean HAG')\n",
    "df['Function'] = df['Function'].str.replace('random', 'E-ESN')\n",
    "df['Function'] = df['Function'].str.replace('random_ei', 'ESN')\n",
    "\n",
    "if file_name == \"test_results_classification.csv\":\n",
    "    df['Dataset'] = df['Dataset'].str.replace('SpokenArabicDigits', 'Spoken\\nArabic\\nDigits')\n",
    "    df['Dataset'] = df['Dataset'].str.replace('SPEECHCOMMANDS', 'SPEECH\\nCOMMANDS')\n",
    "    df['Dataset'] = df['Dataset'].str.replace('JapaneseVowels', 'Japanese\\nVowels')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "functions = df['Function'].unique()\n",
    "x = np.arange(len(df['Dataset'].unique()))  # The label locations\n",
    "width = 0.2  # Width of the bars\n",
    "colors = color_palette(\"tab20\")\n",
    "\n",
    "for i, func in enumerate(functions):\n",
    "    values = df[df['Function'] == func]\n",
    "    ax.bar(x + i * width, values['Average Score'], width, label=func, yerr=values['Standard Deviation'], capsize=5, color=colors[i % len(colors)])\n",
    "\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylabel('Average Score')\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Average Uncoupled Dynamics', size=fontsize)\n",
    "plt.legend(title='Generation', fontsize=fontsize)\n",
    "ax.set_xticks(x + width * (len(functions) - 1) / 2)\n",
    "ax.set_xticklabels(df['Dataset'].unique())\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4008f0-d80d-4833-a854-4b672c298339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e28aa9-acde-4f48-86ed-11dd43923c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hadsp_env",
   "language": "python",
   "name": "hadsp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
