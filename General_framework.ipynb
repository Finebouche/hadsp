{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c1d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:11:08.610165Z",
     "start_time": "2023-10-20T11:11:08.603538Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the root project folder to the python path in order to use the packages\n",
    "path_root = Path( '/project_ghent/HADSP/hadsp/')\n",
    "sys.path.append(str(path_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f790cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:11:09.109067Z",
     "start_time": "2023-10-20T11:11:09.084165Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from importlib import reload\n",
    "\n",
    "# SEED\n",
    "SEED = 49387\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from seaborn import heatmap, color_palette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7bc227",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb44384e-7cbc-4611-a015-e1c073f61aa9",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "Lots of different on availabale : https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad\n",
    "\n",
    "Classification: \n",
    "https://arxiv.org/abs/1803.07870\n",
    "\n",
    "https://github.com/FilippoMB/Time-series-classification-and-clustering-with-Reservoir-Computing\n",
    "\n",
    "Multivariate:\n",
    "https://www.timeseriesclassification.com/dataset.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4e7d3-717e-4f4b-a2fa-29fd6e272bd7",
   "metadata": {},
   "source": [
    "## Torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f91faf639d30e4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load LIBRISPEECH dataset using torchaudio\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torchaudio.datasets import VoxCeleb1Identification, YESNO\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "dataset = VoxCeleb1Identification(root=\"datasets/\", download=True)\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for sample in tqdm(dataset):\n",
    "    X.append(sample[0][0])\n",
    "    Y.append(sample[2])\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "indices[split:], indices[:split]\n",
    "\n",
    "# Use StratifiedShuffleSplit to get train/test indices\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_index, test_index = next(sss.split(X, Y))\n",
    "\n",
    "# Split data and labels using the indices\n",
    "X_train = X[train_index]\n",
    "Y_train = Y[train_index]\n",
    "X_test = X[test_index]\n",
    "Y_test = Y[test_index]\n",
    "\n",
    "sampling_rate = dataset[0][1]\n",
    "\n",
    "is_multivariate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25452c-969a-4154-a5fa-355ae022ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use StratifiedShuffleSplit to get train/test indices\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_index, test_index = next(sss.split(X, Y))\n",
    "\n",
    "print(train_index)\n",
    "# Split data and labels using the indices\n",
    "X_train = np.array([X[i].numpy() for i in train_index])\n",
    "Y_train = [Y[i] for i in train_index]\n",
    "X_test = np.array([X[i].numpy() for i in test_index])\n",
    "Y_test = [Y[i] for i in test_index]\n",
    "sampling_rate = dataset[0][1]\n",
    "\n",
    "is_multivariate = False\n",
    "X_pretrain = np.concatenate(X_train[:20], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c3783-63be-466e-8a27-45b5e064aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc3d35-72f1-4fc8-a452-e0699b4d507d",
   "metadata": {},
   "source": [
    "## Prediction ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfe4e16296a17c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Mackey-Glass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb2dcf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from reservoirpy.datasets import mackey_glass\n",
    "\n",
    "timesteps = 15000\n",
    "mg_inputs = mackey_glass(timesteps, tau=17, a=0.2, b=0.1, n=10, x0=1.2, h=1, seed=None)\n",
    "\n",
    "# Define the time step of your Mackey-Glass system\n",
    "dt = 0.00001\n",
    "\n",
    "# Compute the equivalent sampling rate\n",
    "sampling_rate = 1 / dt\n",
    "\n",
    "is_multivariate = False\n",
    "step_ahead = 5\n",
    "\n",
    "X_pretrain = mg_inputs[:5000]\n",
    "\n",
    "X_train = mg_inputs[5000:10000-step_ahead]\n",
    "Y_train = mg_inputs[5000+step_ahead:10000]\n",
    "\n",
    "X_test = mg_inputs[10000-step_ahead:15000-step_ahead-step_ahead]\n",
    "Y_test = mg_inputs[10000:15000-step_ahead]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(range(500), X_test[:500])\n",
    "plt.plot(range(500), Y_test[:500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae6a7ebe247f27",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533985e3-1953-4347-bd40-b88c367cbc1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T16:10:28.793834Z",
     "start_time": "2023-10-18T16:10:28.774346Z"
    }
   },
   "source": [
    "### Japanese voyels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868f68b-4405-48db-9bbc-34084d6fb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reservoirpy.datasets import japanese_vowels\n",
    "\n",
    "X_train_band, Y_train, X_test_band, Y_test = japanese_vowels()\n",
    "\n",
    "is_multivariate = True\n",
    "\n",
    "# Sampling rate : 10 kHz\n",
    "# Source : https://archive.ics.uci.edu/dataset/128/japanese+vowels\n",
    "sampling_rate = 10000\n",
    "\n",
    "# pretrain is the same as train\n",
    "X_pretrain_band = np.concatenate(X_train_band, axis=0).T\n",
    "\n",
    "Y_train = np.squeeze(np.array(Y_train), axis=1)\n",
    "Y_test = np.squeeze(np.array(Y_test), axis=1)\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537fa0436fc2fcff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### InsectWingbeat"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2a91019-8d72-486a-be3e-6e8c860580a6",
   "metadata": {},
   "source": [
    "from scipy.io import arff\n",
    "from io import StringIO\n",
    "\n",
    "with open('datasets/InsectWingbeat/InsectWingbeat_TRAIN.arff', 'r') as f:\n",
    "    data, meta = arff.loadarff(f)\n",
    "\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370aa8a6-4c5d-4c29-9bfe-a36dcd37d1d8",
   "metadata": {},
   "source": [
    "### MELD\n",
    "\n",
    "https://github.com/declare-lab/MELD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016e3f6-92e5-4403-a145-9eaf9af0cdac",
   "metadata": {},
   "source": [
    "### Free Spoken Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0675739-5d2e-497f-b990-008b7d0e5a59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:11:17.690581Z",
     "start_time": "2023-10-20T11:11:15.901814Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets.load_datasets import load_FSDD_dataset\n",
    "\n",
    "data_dir = 'datasets/fsdd/free-spoken-digit-dataset-master/recordings'  # Path to the extracted dataset\n",
    "sampling_rate, X_train, X_test, Y_train, Y_test = load_FSDD_dataset(data_dir, seed=SEED, visualize=True)\n",
    "# Check the shapes of the datasets\n",
    "print(f'X_train: ({len(X_train)}, {len(X_train[0])})')\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "print(f'X_test: ({len(X_test)}, {len(X_test[0])})')\n",
    "print(\"Y_test:\", Y_test.shape)\n",
    "\n",
    "#take a long time (15min with half the samples, instant with 20 which is enought for pretraining)\n",
    "X_pretrain = np.concatenate(X_train[:20], axis=0)\n",
    "print(\"X_pretrain shape:\", X_pretrain.shape)\n",
    "\n",
    "is_multivariate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468ca5c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multivariate generation if necessary\n",
    "\n",
    "Spectrograms_vs_Cochleagrams : https://www.researchgate.net/publication/340510607_Speech_recognition_using_very_deep_neural_networks_Spectrograms_vs_Cochleagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e57e07-e48c-4e28-80e9-ad95894d0a2e",
   "metadata": {},
   "source": [
    "### Spectral density and peak selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ad0f4-4dbc-4b97-8d28-8ffb357715d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:11:18.377724Z",
     "start_time": "2023-10-20T11:11:18.143305Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets.multivariate_generation import extract_peak_frequencies\n",
    "\n",
    "if is_multivariate:\n",
    "    filtered_peak_freqs = extract_peak_frequencies(X_pretrain_band[0].flatten(), sampling_rate, threshold=1e-4, nperseg=1024, visualize=True)\n",
    "else:\n",
    "    filtered_peak_freqs = extract_peak_frequencies(X_pretrain.flatten(), sampling_rate, threshold=1e-4, nperseg=1024, visualize=True)\n",
    "\n",
    "print(\"Number of frequencies selected :\", len(filtered_peak_freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766304bc-3c47-4813-9828-91d8fed5845b",
   "metadata": {},
   "source": [
    "### Applying normal band pass filter on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690a984-394f-4be3-912c-ee2ca4cc1ba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:11:39.227368Z",
     "start_time": "2023-10-20T11:11:19.130705Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets.multivariate_generation \n",
    "reload(datasets.multivariate_generation)\n",
    "from datasets.multivariate_generation import generate_multivariate_dataset, extract_peak_frequencies\n",
    "\n",
    "if not is_multivariate:\n",
    "    X_pretrain_band, X_train_band, X_test_band = generate_multivariate_dataset(\n",
    "        filtered_peak_freqs, X_pretrain, X_train, X_test, sampling_rate, nb_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b859e5-c6dd-4086-ac56-42a7de3ed25e",
   "metadata": {},
   "source": [
    "### Standardizing the amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45e001-4920-47e1-8dbb-6681cf9b32e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:12:00.856649Z",
     "start_time": "2023-10-20T11:11:39.230512Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# pretrain\n",
    "# Be really carefull of the column order here !\n",
    "filtered_data = scaler.fit_transform(X_pretrain_band.T)\n",
    "filtered_data = filtered_data.T\n",
    "\n",
    "if not is_multivariate:\n",
    "\n",
    "    if len(X_train[0].shape)>1:\n",
    "        # train\n",
    "        X_train_band = [\n",
    "            np.array([scaler.fit_transform(time_serie.reshape(-1, 1)).flatten() \n",
    "                      for time_serie in x]) for x in tqdm(X_train_band)\n",
    "        ]\n",
    "    \n",
    "        # test\n",
    "        X_test_band = [\n",
    "            np.array([scaler.fit_transform(time_serie.reshape(-1, 1)).flatten() \n",
    "                      for time_serie in x]) for x in tqdm(X_test_band)\n",
    "        ]\n",
    "    else: \n",
    "        # train\n",
    "        X_train_band = np.array([scaler.fit_transform(time_serie.reshape(-1, 1)).flatten() for time_serie in X_train_band])\n",
    "    \n",
    "        # test\n",
    "        X_test_band = np.array([scaler.fit_transform(time_serie.reshape(-1, 1)).flatten() for time_serie in X_test_band])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93f662-f197-4b0a-a140-090f3c0909d2",
   "metadata": {},
   "source": [
    "# Generating reservoirs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reservoir functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8685e0ff43b927c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from reservoir.activation_functions import tanh, heaviside, sigmoid\n",
    "\n",
    "# the activation function choosen for the rest of the experiment\n",
    "# activation_function = lambda x : sigmoid(2*(x-0.5))\n",
    "activation_function = lambda x : sigmoid(2*(x-0.5))\n",
    "\n",
    "plt.plot(np.linspace(0, 2, 100), activation_function(np.linspace(0, 2, 100)))\n",
    "plt.grid()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "615a38d2df41a727"
  },
  {
   "cell_type": "markdown",
   "id": "e2b39f1d-d8d6-451b-8d4d-b8f8f74a498d",
   "metadata": {},
   "source": [
    "## Plot  pretrain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab965ec2-e31c-475e-b452-05c9dbfde13b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:12:00.858650Z",
     "start_time": "2023-10-20T11:12:00.856783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Min window size to get all the dynamics ? \n",
    "min_window_size = sampling_rate/filtered_peak_freqs[-1]\n",
    "max_window_size = sampling_rate/filtered_peak_freqs[0]\n",
    "\n",
    "print(min_window_size)\n",
    "print(max_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd00d3f-9b0c-475f-bac9-b46e5f92adfb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-20T11:12:00.862019Z"
    },
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Compute the moving average \n",
    "window_size = 10\n",
    "\n",
    "if max_window_size <= window_size or  window_size <= min_window_size:\n",
    "    raise ValueError(f\"window_size must be greater than {min_window_size} and smaller than {max_window_size}. Current window_size is {window_size}.\")\n",
    "\n",
    "weights = np.repeat(1.0, window_size)/window_size\n",
    "ma = np.array([np.convolve(d, weights, 'valid') for d in (filtered_data)])\n",
    "\n",
    "#CPlot the two for different frequencies\n",
    "NB_1 = 3\n",
    "fig, ax = plt.subplots(3, 1, figsize=(24,18))\n",
    "ax[0].plot(range(500), filtered_data[NB_1, 1000:1500], label='Time serie')\n",
    "ax[0].plot(range(500), ma[NB_1, 1000:1500], label='Moving average')\n",
    "ax[0].legend(fontsize=26)\n",
    "\n",
    "NB_2 = 8\n",
    "ax[1].plot(range(500), filtered_data[NB_2, 1000:1500], label='Time serie')\n",
    "ax[1].plot(range(500), ma[NB_2, 1000:1500], label='Moving average')\n",
    "\n",
    "#Check that the scaler did a good job\n",
    "ax[2].plot(range(500), X_pretrain_band[NB_2, 1000:1500], label='Time serie')\n",
    "\n",
    "for i, ax in enumerate(ax):\n",
    "    # Format subplot\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='both', labelsize=26)\n",
    "    # draw vertical lines to represent the window for some points\n",
    "    for x in range(100, 500, 100):\n",
    "        ax.axvspan(x, x + window_size, color='g', alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be90fde85cfcf6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data to feed to the reservoir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770968b147c07381",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we calculate \n",
    "**common_dimension** : the number of different dimensions in the input data\n",
    " **K** : the number of euron that will receive a particular time serie as input \n",
    "**n** : the dimension of the reservoir \n",
    "\n",
    "n = K * common_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f44547",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def find_common_dimension(arrays):\n",
    "    common_dimension_size = None\n",
    "    common_dimension_index = None\n",
    "\n",
    "    for i in range(len(arrays[0].shape)):  # Loop over dimensions of the first dimension\n",
    "        size = arrays[0].shape[i]\n",
    "        if all(arr.shape[i] == size for arr in arrays[1:]):  # Check if all other arrays have the same size in this dimension\n",
    "            common_dimension_size = size\n",
    "            common_dimension_index = i\n",
    "            break  # Exit the loop once the common dimension is found\n",
    "  \n",
    "    return common_dimension_index, common_dimension_size\n",
    "\n",
    "if len(X_train_band[0].shape) > 1:\n",
    "    common_xtrain_index, common_xtrain_size = find_common_dimension(X_train_band)\n",
    "else:\n",
    "    common_xtrain_size = X_train_band.shape[0]\n",
    "    common_xtrain_index = 0        \n",
    "          \n",
    "\n",
    "print(\"Common dimension index is :\", common_xtrain_index)\n",
    "print(\"Number of different time series is :\", common_xtrain_size)\n",
    "if len(X_train_band[0].shape) > 1:\n",
    "    print(\"Check it ! First array \", X_train_band[0].shape, \" and second array\", X_train_band[1].shape)\n",
    "\n",
    "# We want the size of the reservoir to be at least 200\n",
    "K = math.ceil(200 / common_xtrain_size)\n",
    "n = common_xtrain_size * K\n",
    "print(\"Dimension of our reservoir :\", n)\n",
    "print(\"Copy of each time serie :\", K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba8673-6905-469a-b397-5c4e620f64c0",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from connexion_generation.utility import TwoDimArrayWrapper\n",
    "\n",
    "# We create an array of the same shape as X_pretrain_band but with the same time serie repeated K times\n",
    "frequency_bands = np.repeat(filtered_data, K, axis=0)\n",
    "     \n",
    "frequency_bands = TwoDimArrayWrapper(frequency_bands)\n",
    "frequency_bands.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df206ba-36a2-4468-b719-7bc819114b1a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Construct matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6435e2-6f19-4987-9309-ab74694678fd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Shared parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4957e6c-e0cc-4d90-a495-348a8bbab3cd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INCREMENT = int(min_window_size+1)\n",
    "MAX_INCREMENT = int(max_window_size)\n",
    "VALUE = 0.05\n",
    "target_rate = 0.7\n",
    "growth_parameter = 0.15\n",
    "\n",
    "bias_scaling = 1\n",
    "input_scaling = 0.1\n",
    "leaky_rate = 1\n",
    "\n",
    "if max_window_size <= INCREMENT or  INCREMENT <= min_window_size:\n",
    "    raise ValueError(f\"INCREMENT must be greater than {min_window_size} and smaller than {max_window_size}. Current INCREMENT is {INCREMENT}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fd1fc57ed9157",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Function to initialise and generate reservoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39af15",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.bounded_adsp import run_HADSP_algorithm\n",
    "import connexion_generation.bounded_adsp\n",
    "reload(connexion_generation.bounded_adsp)\n",
    "input_connectivity = 1\n",
    "connectivity = 0\n",
    "\n",
    "def initialise_and_train(input_scaling, n, input_connectivity, connectivity, bias_scaling, seed, training_set, visualize=False):\n",
    "    Win, W, bias = init_matrices(n, input_connectivity, connectivity, seed=seed)\n",
    "    bias *= bias_scaling\n",
    "    Win *= input_scaling\n",
    "        \n",
    "    W = run_HADSP_algorithm(W, Win, bias, leaky_rate, activation_function, training_set, INCREMENT, VALUE,\n",
    "                            target_rate, growth_parameter, max_increment=MAX_INCREMENT, visualize=visualize)\n",
    "    \n",
    "    connectivity =  W.count_nonzero() / (W.shape[0] * W.shape[1])\n",
    "    eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "    sr = max(abs(eigen))\n",
    "    \n",
    "    return Win, W, bias, connectivity, sr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4de89f833989d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Multivariate matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54472b03-ca39-4f13-92cf-e71e6d63c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HADSP + multi\n",
    "Win_hadsp_multi, W_hadsp_multi, bias_hadsp_multi, connectivity_band, sr_hadsp_multi = initialise_and_train(input_scaling, n, input_connectivity, connectivity, bias_scaling, SEED, frequency_bands)\n",
    "\n",
    "# random + multi\n",
    "Win_random_multi, W_random_multi, bias_random_multi =  init_matrices(n, 1, connectivity_band, sr_hadsp_multi)\n",
    "bias_random_multi= bias_random_multi*bias_scaling\n",
    "Win_random_multi= Win_random_multi*input_scaling\n",
    "\n",
    "eigen_random_multi = sparse.linalg.eigs(W_random_multi, k=1, which=\"LM\", maxiter=W_random_multi.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "sr_random_multi = max(abs(eigen_random_multi))\n",
    "\n",
    "heatmap(W_hadsp_multi.todense(), cmap=color_palette(\"cividis\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9b3d83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:36:57.885053Z",
     "start_time": "2023-10-09T13:36:57.882050Z"
    }
   },
   "source": [
    "### Univariate matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d83057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    # HADSP + uni\n",
    "    Win_hadsp_uni, W_hadsp_uni, bias_hadsp_uni, connectivity_hadsp_uni, sr_hadsp_uni = initialise_and_train(input_scaling, n,  input_connectivity, connectivity, bias_scaling, SEED, X_pretrain.flatten())\n",
    "    \n",
    "    # random + uni\n",
    "    Win_normal, W_normal, bias_normal =  init_matrices(n, 1, connectivity_hadsp_uni, sr_hadsp_uni)\n",
    "    bias_normal= bias_normal*bias_scaling\n",
    "    Win_normal= Win_normal*input_scaling   \n",
    "    \n",
    "    eigen_normal = sparse.linalg.eigs(W_normal, k=1, which=\"LM\", maxiter=W_normal.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "    sr_normal = max(abs(eigen_normal))\n",
    "    \n",
    "    heatmap(W_hadsp_uni.todense(), cmap=color_palette(\"cividis\", as_cmap=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68129bc2-1cc6-4498-9e57-ed18e037643e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:38:25.203377Z"
    }
   },
   "source": [
    "### Spectral radius normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49826ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sr_hadsp_multi)\n",
    "print(sr_random_multi)\n",
    "if not is_multivariate:\n",
    "    print(sr_normal)\n",
    "    print(sr_hadsp_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a76515-3af0-438e-8098-50ef8745f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral radius normalisation\n",
    "normal_sr = 0.99\n",
    "W_hadsp_multi = W_hadsp_multi/sr_hadsp_multi*normal_sr\n",
    "W_random_multi = W_random_multi/sr_random_multi*normal_sr\n",
    "if not is_multivariate:\n",
    "    W_normal = W_normal/sr_normal*normal_sr\n",
    "    W_hadsp_uni = W_hadsp_uni/sr_hadsp_uni*normal_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b447f6-e964-4f6c-9116-597cd29c1755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:13:26.845564Z",
     "start_time": "2023-10-09T13:13:26.821527Z"
    }
   },
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ffd9c-ffc5-47ea-885d-f2f2a3e549fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "N_JOBS = -1\n",
    "RIDGE_COEF = 1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27380a6-3976-442d-8bb0-34bfc142985a",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b3a4e-b06e-450a-b7c5-68802be8013d",
   "metadata": {},
   "source": [
    "### Classification for multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3534f99-25b5-4653-8dcf-7073183e0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We give Xtrain_band and Xtest_band the same shape as the expected input of the reservoir\n",
    "caca = []\n",
    "caca_test = []\n",
    "for i in tqdm(range(len(X_train_band))):\n",
    "    if common_xtrain_index == 1:\n",
    "        caca.append(np.repeat(X_train_band[i], K, axis=1))\n",
    "    else:\n",
    "        caca.append(np.repeat(X_train_band[i], K, axis=0).T) # correct axis depends on X_train_band shape\n",
    "for i in tqdm(range(len(X_test_band))):\n",
    "    if common_xtrain_index == 1:\n",
    "        caca_test.append(np.repeat(X_test_band[i], K, axis=1))\n",
    "    else:\n",
    "        caca_test.append(np.repeat(X_test_band[i], K, axis=0).T)\n",
    "\n",
    "print(\"caca example shape :\", caca[1].shape)     \n",
    "print(\"We should have :\", caca[0].shape[1], \"==\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9130e-a834-4256-8f29-e42add2eee19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import performances.esn_model_evaluation\n",
    "reload(performances.esn_model_evaluation)\n",
    "from performances.esn_model_evaluation import train_and_predict_model, compute_score\n",
    "# To remember : \n",
    "#  For reservoirpy   pre_s = W @ r + Win @ (u + noise_gen(dist=dist, shape=u.shape, gain=g_in)) + bias\n",
    "\n",
    "Y_pred_hadsp_multi = train_and_predict_model(W_hadsp_multi, Win_hadsp_multi, bias_hadsp_multi, activation_function, RIDGE_COEF, caca, caca_test, Y_train, N_JOBS)\n",
    "\n",
    "Y_pred_random_multi = train_and_predict_model(W_random_multi, Win_random_multi, bias_random_multi, activation_function, RIDGE_COEF, caca, caca_test, Y_train, N_JOBS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3dc743-36c8-4821-948d-f14f43af8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = compute_score(Y_pred_hadsp_multi, Y_test, \"HADSP multi\")\n",
    "\n",
    "score = compute_score(Y_pred_random_multi, Y_test, \"random multi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f3553-1d28-4340-b4b9-f3dc673bdcac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:59:05.097521Z",
     "start_time": "2023-10-09T13:59:04.992489Z"
    }
   },
   "source": [
    "### Classification for univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50e2ca-93d7-48bb-bcff-4f885e8d5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_multivariate: \n",
    "    # Create a list to store the arrays with the same shape as the expected input of the reservoir\n",
    "    pipi = []\n",
    "    pipi_test = []\n",
    "    \n",
    "    for i in tqdm(range(len(X_train))):\n",
    "        pipi.append(np.repeat(X_train[i], n, axis=1))\n",
    "\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        pipi_test.append(np.repeat(X_test[i], n, axis=1))\n",
    "\n",
    "    print(\"pipi example shape :\", pipi[0].shape)     \n",
    "    print(\"We should have :\", pipi[0].shape[1], \"==\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d01e8-8763-4bf3-ae0a-4c4569231dc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    Y_pred_hadsp_uni = train_and_predict_model(W_hadsp_uni, Win_hadsp_uni, bias_hadsp_uni, activation_function, RIDGE_COEF, pipi, pipi_test, Y_train, N_JOBS)\n",
    "    \n",
    "    Y_pred_normal = train_and_predict_model(W_normal, Win_normal, bias_normal, activation_function, RIDGE_COEF, pipi, pipi_test, Y_train, N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f4028-f457-46b4-a219-7f5fed575af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    score = compute_score(Y_pred_hadsp_uni, Y_test, \"HADSP uni\")\n",
    "\n",
    "    score = compute_score(Y_pred_normal, Y_test, \"random uni\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7ab1a-e020-489a-8ff9-9bbd689de15e",
   "metadata": {},
   "source": [
    "## Prediction ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bcdbf",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c90ff0-b5cd-446a-9e47-62c705311b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "caca = np.repeat(np.squeeze(np.array(X_train_band)), K, axis=0)\n",
    "caca_test = np.repeat(np.squeeze(np.array(X_test_band)), K, axis=0)\n",
    "\n",
    "print(\"Step ahead : \", step_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e221c1d-5610-4d26-a23d-5eeaa439719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "caca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15374eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import reservoir.reservoir\n",
    "from reservoir.reservoir import train\n",
    "reload(reservoir.reservoir)\n",
    "\n",
    "# To remember\n",
    "np.zeros(2000)\n",
    "\n",
    "# to generate the evaluation dataset\n",
    "\n",
    "# Training random + MG\n",
    "Wout_normal, b_out_normal, last_state_normal = train(W_normal, Win_normal, bias_normal, X_train, Y_train, activation_function, ridge_coef = RIDGE_COEF)\n",
    "\n",
    "# Training for HADSP + MG\n",
    "Wout_hadsp_uni, b_out_hadsp_uni, last_state_hadsp_uni = train(W_hadsp_uni, Win_hadsp_uni, bias_hadsp_uni, X_train, Y_train, activation_function, ridge_coef = RIDGE_COEF)\n",
    "\n",
    "# Training random + bandfilter\n",
    "Wout_random_multi, b_out_random_multi, last_state_random_multi = train(W_random_multi, Win_random_multi, bias_random_multi, caca, Y_train, activation_function, ridge_coef = RIDGE_COEF)\n",
    "\n",
    "# Training output HASDP + bandfilter\n",
    "Wout_hadsp_multi, b_out_hadsp_multi, last_state_hadsp_multi = train(W_hadsp_multi, Win_hadsp_multi, bias_hadsp_multi, caca, Y_train, activation_function, ridge_coef = RIDGE_COEF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcfe07-1ad0-4d8f-827f-bb9776e8086e",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dac84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from reservoir.reservoir import run\n",
    "from reservoir.reservoirpy_util import plot_results, plot_readout\n",
    "\n",
    "# Prediction for random + MG\n",
    "y_pred_random_uni = run(W_normal, Win_normal, bias_normal, Wout_normal, X_test, activation_function, b_out_normal, last_state_normal)\n",
    "\n",
    "# Prediction for HADSP + MG\n",
    "y_pred_hadsp_uni = run(W_hadsp_uni, Win_hadsp_uni, bias_hadsp_uni, Wout_hadsp_uni, X_test, activation_function, b_out_hadsp_uni, last_state_hadsp_uni)\n",
    "\n",
    "# Prediction for random + bandfilter\n",
    "y_pred_random_multi = run(W_random_multi, Win_random_multi, bias_random_multi, Wout_random_multi, caca_test, activation_function, b_out_random_multi, last_state_random_multi)\n",
    "\n",
    "# Prediction for HADSP + bandfilter\n",
    "y_pred_hadsp_multi = run(W_hadsp_multi, Win_hadsp_multi, bias_hadsp_multi, Wout_hadsp_multi, caca_test, activation_function, b_out_hadsp_multi, last_state_hadsp_multi)\n",
    "\n",
    "\n",
    "plot_results(y_pred_random_multi, Y_test, sample=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61165568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import reservoir.losses\n",
    "from reservoir.losses import rmse, nrmse\n",
    "reload(reservoir.losses)\n",
    "\n",
    "print(\"nrmse normal        :\", float(nrmse(Y_test[:300], y_pred_random_uni[:300])))\n",
    "print(\"nrmse hadsp     :\", float(nrmse(Y_test[:300], y_pred_hadsp_uni[:300])))\n",
    "print(\"nrmse random + band :\", float(nrmse(Y_test[:300], y_pred_random_multi[:300])))\n",
    "print(\"nrmse hadsp + band   :\", float(nrmse(Y_test[:300], y_pred_hadsp_multi[:300])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc19b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nrmse_array_random_uni = []\n",
    "nrmse_array_hadsp_uni = []\n",
    "nrmse_array_random_multi = []\n",
    "nrmse_array_hadsp_multi = []\n",
    "\n",
    "for i in range(len(Y_test)-100- step_ahead):\n",
    "    Y_test_i = Y_test[i:100+i]\n",
    "    nrmse_array_random_uni.append(nrmse(Y_test_i, y_pred_random_uni[i:100+i]))\n",
    "    nrmse_array_hadsp_uni.append(nrmse(Y_test_i, y_pred_hadsp_uni[i:100+i]))\n",
    "    nrmse_array_random_multi.append(nrmse(Y_test_i, y_pred_random_multi[i:100+i]))\n",
    "    nrmse_array_hadsp_multi.append(nrmse(Y_test_i, y_pred_hadsp_multi[i:100+i]))\n",
    "    \n",
    "log10_nrmse_random_uni= np.log10(nrmse_array_random_uni)\n",
    "log10_nrmse_hadsp_uni = np.log10(nrmse_array_hadsp_uni)\n",
    "log10_nrmse_random_multi = np.log10(nrmse_array_random_multi)\n",
    "log10_nrmse_hadsp_multi = np.log10(nrmse_array_hadsp_multi)\n",
    "plt.figure()\n",
    "plt.plot(log10_nrmse_random_uni[:1000])\n",
    "plt.plot(log10_nrmse_hadsp_uni[:1000])\n",
    "plt.plot(log10_nrmse_random_multi[:1000])\n",
    "plt.plot(log10_nrmse_hadsp_multi[:1000])\n",
    "\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Log10 NRMSE')\n",
    "plt.legend([\"HADSP+band\", \"random\", \" random + bandfilter\", \"HADSP\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27da37",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2478c-bb79-4bc0-855f-b028109c8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# pretrain\n",
    "# Be really carefull of the column order here !\n",
    "df_data = scaler.fit_transform(X_pretrain_band.T)\n",
    "df_data = df_data.T\n",
    "df = pd.DataFrame(df_data.T)\n",
    "# Initialize a progress bar for total number of series\n",
    "progress_bar = tqdm(total=df.shape[1]**2, position=0, leave=True)\n",
    "\n",
    "# Initialize an empty correlation matrix\n",
    "correlation_matrix = pd.DataFrame(index=df.columns, columns=df.columns)\n",
    "\n",
    "# Calculate the correlation for each pair of series\n",
    "for col1 in df.columns:\n",
    "    progress_bar.set_description(f\"Processing {col1}\")\n",
    "    for col2 in df.columns:\n",
    "        correlation_matrix.loc[col1, col2] = df[col1].corr(df[col2], method='pearson', min_periods=5)\n",
    "\n",
    "        progress_bar.update(1)  # Update the progress bar after processing each series\n",
    "    \n",
    "progress_bar.close()\n",
    "\n",
    "# Convert correlation_matrix to numeric as it is stored as objects due to tqdm\n",
    "correlation_matrix = correlation_matrix.apply(pd.to_numeric)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linked = linkage(correlation_matrix, 'single')\n",
    "\n",
    "# Get the order of rows/columns after hierarchical clustering\n",
    "row_order = leaves_list(linked)\n",
    "\n",
    "# Reorder the correlation matrix\n",
    "sorted_corr_matrix = correlation_matrix.iloc[row_order, :].iloc[:, row_order]\n",
    "\n",
    "# Visualize the sorted correlation matrix with a heatmap\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(sorted_corr_matrix, annot=False, cmap='vlag', vmin=-1, vmax=1)\n",
    "plt.title('Clustered Pairwise Correlation of Time Series')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec25bb-f3e8-4cdb-87ee-9772460a8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "row_order_r = np.array([i + k for i in row_order*K for k in range(K)])\n",
    "\n",
    "# Convert the sparse matrix to a dense format (if memory allows)\n",
    "dense_matrix = W_hadsp_multi.toarray()\n",
    "\n",
    "# Reorder the dense matrix using the repeated ordering\n",
    "reordered_matrix = dense_matrix[np.ix_(row_order_r, row_order_r)]\n",
    "\n",
    "# Convert the reordered dense matrix back to a sparse format if needed\n",
    "sparse_reordered_matrix = coo_matrix(reordered_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901730be-d407-4d6d-973d-9fc24951bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(sparse_reordered_matrix.todense(), cmap=color_palette(\"vlag\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e46145-b737-4e62-a8ed-9860be34eb2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T12:52:00.989091Z",
     "start_time": "2023-10-10T12:52:00.984122Z"
    }
   },
   "source": [
    "## Motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78de13-7b2d-4ea0-9854-c7ef05c9780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.topology\n",
    "reload(analysis.topology)\n",
    "from analysis.topology import motif_distribution, draw_motifs_distribution\n",
    "\n",
    "motifs_count = motif_distribution(W_hadsp_multi.A)\n",
    "draw_motifs_distribution(motifs_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4480d-5e4f-4603-90c0-e2f0576e0288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9b17e-9381-429e-9a86-94f32a800d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hadsp_env",
   "language": "python",
   "name": "hadsp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
