{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c1d64",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-10T10:40:49.539850Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the root project folder to the python path in order to use the packages\n",
    "path_root = Path( '/project_ghent/HADSP/hadsp/')\n",
    "sys.path.append(str(path_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f790cf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-10T10:40:49.541339Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from importlib import reload\n",
    "\n",
    "# SEED\n",
    "SEED = 49387\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from seaborn import heatmap, color_palette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44384e-7cbc-4611-a015-e1c073f61aa9",
   "metadata": {},
   "source": [
    "# Datasets loading\n",
    "\n",
    "Lots of different on availabale : https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad\n",
    "\n",
    "Classification: \n",
    "https://arxiv.org/abs/1803.07870\n",
    "\n",
    "https://github.com/FilippoMB/Time-series-classification-and-clustering-with-Reservoir-Computing\n",
    "\n",
    "Multivariate:\n",
    "https://www.timeseriesclassification.com/dataset.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4e7d3-717e-4f4b-a2fa-29fd6e272bd7",
   "metadata": {},
   "source": [
    "## Torchaudio\n",
    "\n",
    "https://pytorch.org/audio/stable/datasets.html\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21945fed-8f00-4796-ab80-5f5fba7c012e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# load dataset using torchaudio\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torchaudio.datasets import VoxCeleb1Identification, SPEECHCOMMANDS\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "dataset = SPEECHCOMMANDS(root=\"datasets/\", download=True)\n",
    "\n",
    "sampling_rate = dataset[0][1]\n",
    "X = [sample[0][0] for sample in dataset]\n",
    "Y = [sample[2] for sample in dataset]\n",
    "\n",
    "dataset_size = len(dataset)  # Total number of samples in the dataset\n",
    "\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "\n",
    "# Use StratifiedShuffleSplit to get train/test indices\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "train_index, test_index = next(sss.split(X, Y))\n",
    "\n",
    "# Split data and labels using the indices\n",
    "X_train = X[train_index]\n",
    "Y_train = Y[train_index]\n",
    "X_test = X[test_index]\n",
    "Y_test = Y[test_index]\n",
    "\n",
    "\n",
    "is_multivariate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc3d35-72f1-4fc8-a452-e0699b4d507d",
   "metadata": {},
   "source": [
    "## Prediction ahead\n",
    "\n",
    "Datasets available :\n",
    "\n",
    "* MackeyGlass\n",
    "* Lorenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33230f-1ad4-4b2c-99a5-b249c9efdb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.load_datasets import load_dataset_prediction\n",
    "is_instances_classification = False\n",
    "dataset_name = \"MackeyGlass\"\n",
    "step_ahead=5\n",
    "\n",
    "is_multivariate, sampling_rate, X_train_raw, X_test_raw, Y_train_raw, Y_test = load_dataset_prediction(dataset_name, step_ahead, visualize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae6a7ebe247f27",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Classification\n",
    "\n",
    "Datasets available :\n",
    "\n",
    "* FSDD\n",
    "* HAART\n",
    "* JapaneseVowels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d322db7-9d62-4d60-9514-41d11822f03b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from datasets.load_datasets import load_dataset_classification\n",
    "is_instances_classification = True\n",
    "dataset_name = \"JapaneseVowels\"\n",
    "\n",
    "is_multivariate, sampling_rate, X_train_raw, X_test_raw, Y_train_raw, Y_test, groups = load_dataset_classification(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbdeb64-a393-46aa-b744-03bde8624cba",
   "metadata": {},
   "source": [
    "# Reservoir parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c676f-999b-4f75-9991-0e4de2ecf3ad",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a38d2df41a727",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from reservoir.activation_functions import tanh, heaviside, sigmoid\n",
    "\n",
    "# the activation function choosen for the rest of the experiment\n",
    "# activation_function = lambda x : sigmoid(2*(x-0.5))tanh(x)\n",
    "activation_function = lambda x : tanh(x)\n",
    "\n",
    "plt.plot(np.linspace(0, 1.1, 100), activation_function(np.linspace(0, 1.1, 100)))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55bde5-5bb3-41ea-ae0d-10d7bc7401f4",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90237f4e-2418-48a9-a067-aece8c922912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit, StratifiedGroupKFold\n",
    "from datasets.preprocessing import flexible_indexing\n",
    "\n",
    "# CROSS-VALIDATION METHODS\n",
    "val_per=0.2\n",
    "val_size = int(len(X_train_raw)*val_per)\n",
    "if is_instances_classification:\n",
    "    if groups is None:\n",
    "        splits = StratifiedKFold(n_splits=2, shuffle=True).split(X_train_raw, np.argmax(Y_train_raw, axis=1))\n",
    "    else:\n",
    "        splits = StratifiedGroupKFold(n_splits=2, shuffle=True).split(X_train_raw, np.argmax(Y_train_raw, axis=1), groups)\n",
    "else: #prediction\n",
    "    splits = TimeSeriesSplit(n_splits=2).split(X_train_raw)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(splits):\n",
    "    X_train = flexible_indexing(X_train_raw, train_index)\n",
    "    X_val = flexible_indexing(X_train_raw, val_index)\n",
    "    Y_train = flexible_indexing(Y_train_raw, train_index)\n",
    "    Y_val = flexible_indexing(Y_train_raw, val_index)\n",
    "    # SPLITS\n",
    "    if is_multivariate:\n",
    "        X_train_band, X_val_band, X_test_band = X_train, X_val, X_test_raw\n",
    "        del X_train, X_val\n",
    "    else:\n",
    "        X_test = X_test_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468ca5c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "Spectrograms_vs_Cochleagrams : https://www.researchgate.net/publication/340510607_Speech_recognition_using_very_deep_neural_networks_Spectrograms_vs_Cochleagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766304bc-3c47-4813-9828-91d8fed5845b",
   "metadata": {},
   "source": [
    "## Multivariate generation (if not multivariate) and train_validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ad0f4-4dbc-4b97-8d28-8ffb357715d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:11:18.377724Z",
     "start_time": "2023-10-20T11:11:18.143305Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets.multivariate_generation import generate_multivariate_dataset, extract_peak_frequencies\n",
    "\n",
    "freq_train_data = X_train_band if is_multivariate else X_train\n",
    "flat_train_data = np.concatenate(freq_train_data, axis=0) if is_instances_classification else freq_train_data\n",
    "filtered_peak_freqs = extract_peak_frequencies(flat_train_data, sampling_rate, threshold=1e-5, nperseg=1024, visualize=True)\n",
    "\n",
    "if not is_multivariate:\n",
    "    X_train_band = generate_multivariate_dataset(\n",
    "        filtered_peak_freqs, X_train, sampling_rate, is_instances_classification, nb_jobs=-1\n",
    "    )\n",
    "    X_val_band = generate_multivariate_dataset(\n",
    "        filtered_peak_freqs, X_val, sampling_rate, is_instances_classification, nb_jobs=-1\n",
    "    )\n",
    "    X_test_band = generate_multivariate_dataset(\n",
    "        filtered_peak_freqs, X_test, sampling_rate, is_instances_classification, nb_jobs=-1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b9c52-341b-44b0-a97f-e1a5bf37db60",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5278c-9bf6-42b5-8845-c0c41667924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_instances_classification:\n",
    "    # Concatenate train and test arrays for plotting\n",
    "    combined_data = np.concatenate((X_train_band, X_val_band), axis=0)\n",
    "    combined_Y =np.concatenate((Y_train, Y_val), axis=0)\n",
    "    \n",
    "    # Calculate the merge point index\n",
    "    merge_point_index = X_train_band.shape[0]\n",
    "    \n",
    "    # Define the range around the merge point to plot\n",
    "    start_index = merge_point_index - 1000\n",
    "    end_index = merge_point_index + 1000\n",
    "    steps = range(start_index, end_index)\n",
    "    \n",
    "    print(start_index, end_index)\n",
    "    # Plot for a subset N features within a range arround transition from train to test\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i in [0, 2, 10]: \n",
    "        plt.plot(steps, combined_data[steps, i], label=f'Feature {i}')\n",
    "    #plt.plot(steps, combined_Y[steps], label=\"Prediction\")\n",
    "    plt.title('Feature Values Around Merge Point')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Feature Value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b859e5-c6dd-4086-ac56-42a7de3ed25e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Standardizing the amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45e001-4920-47e1-8dbb-6681cf9b32e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:12:00.856649Z",
     "start_time": "2023-10-20T11:11:39.230512Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets.preprocessing import scale_data\n",
    "\n",
    "scaler_multi = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_band, X_val_band, X_test_band = scale_data(X_train_band, X_val_band, X_test_band, scaler_multi, is_instances_classification)\n",
    "            \n",
    "if not is_multivariate:\n",
    "    scaler_x_uni = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train, X_val, X_test = scale_data(X_train, X_val, X_test, scaler_multi, is_instances_classification)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde977ab-e2ee-43a7-a916-3ae2f4a95780",
   "metadata": {},
   "source": [
    "## Noizing and duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a01ef-7b49-4493-aa96-3ea65b18b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define noise parameter\n",
    "noise_std = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97747959-8de6-4fdc-85e8-f8e6425b3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.preprocessing import add_noise, duplicate_data\n",
    "\n",
    "# PRETRAIN NOISE\n",
    "# UNI\n",
    "if not is_multivariate:\n",
    "    X_pretrain_uni = X_train.flatten()\n",
    "    X_pretrain_noisy = (add_noise(X_train, noise_std)).flatten()\n",
    "\n",
    "# MULTI\n",
    "X_pretrain_band = X_train_band\n",
    "X_pretrain_band_noisy = [add_noise(instance, noise_std) for instance in X_pretrain_band]\n",
    "\n",
    "\n",
    "#Train/Val/Test\n",
    "if is_instances_classification:\n",
    "    # NOISE\n",
    "    # UNI\n",
    "    if not is_multivariate:\n",
    "        X_train_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_train, desc=\"TRAIN\")]\n",
    "        X_val_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_val, desc=\"VAL\")]\n",
    "        X_test_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_test, desc=\"TEST\")]\n",
    "        \n",
    "    # MULTI\n",
    "    X_train_band_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_train_band, desc=\"TRAIN\")]\n",
    "    X_val_band_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_val_band, desc=\"VAL\")]\n",
    "    X_test_band_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_test_band, desc=\"TEST\")]\n",
    "\n",
    "else:  #if prediction\n",
    "    # NOISE\n",
    "    # UNI\n",
    "    if not is_multivariate:\n",
    "        X_train_noisy = add_noise(X_train, noise_std)\n",
    "        X_test_noisy = add_noise(X_test, noise_std)\n",
    "        X_val_noisy = add_noise(X_val, noise_std)\n",
    "\n",
    "    # MULTI\n",
    "    X_train_band_noisy = add_noise(X_train_band, noise_std)\n",
    "    X_val_band_noisy = add_noise(X_val_band, noise_std)\n",
    "    X_test_band_noisy = add_noise(X_test_band, noise_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93f662-f197-4b0a-a140-090f3c0909d2",
   "metadata": {},
   "source": [
    "# Generating reservoirs\n",
    "\n",
    "We are interrested in two technique to genereate reservoir. \n",
    "* One is called HADSP, was studied in previous paper, and recombines inputs based on their activity to reach a given activation target.\n",
    "* The other called DESP recombines inputs in order to reach a given standard deviation of activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b33224-cc12-4edd-948c-235f415a4ab8",
   "metadata": {},
   "source": [
    "## Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa63393-5c4e-4cce-9ed0-7caf47af8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_multivariate:\n",
    "    if is_instances_classification:\n",
    "        common_index = 1\n",
    "        print(\"Common index for multivariate classification should be 1\")\n",
    "        print(\"\\nCheck it ! \\nFirst array \", X_train_raw[0].shape, \" and second array\", X_train_raw[2].shape)\n",
    "        common_size = X_train_raw[0].shape[common_index]\n",
    "    else:\n",
    "        common_index = 0\n",
    "        print(\"Common index for multivariate prediction should be 0\")\n",
    "        common_size = X_train_raw.shape[common_index]\n",
    "else:\n",
    "    common_size = len(filtered_peak_freqs)\n",
    "\n",
    "        \n",
    "\n",
    "RESERVOIR_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24988d03-a16b-46f7-b3f1-372452272905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min window size to get all the dynamics ? \n",
    "min_window_size = sampling_rate/np.max(np.hstack(filtered_peak_freqs))\n",
    "max_window_size = sampling_rate/np.min(np.hstack(filtered_peak_freqs))\n",
    "\n",
    "print(min_window_size)\n",
    "print(max_window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ddcde6-215a-464a-8634-624f11da4380",
   "metadata": {},
   "source": [
    "### Visualisation checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0645f-3208-464d-b303-85a2ad9d1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_instances_classification:\n",
    "    train_band_data =  np.concatenate(X_train_band, axis=0).T\n",
    "else:\n",
    "    train_band_data =  X_train_band.T\n",
    "\n",
    "#Compute the moving average \n",
    "window_size = 10\n",
    "if max_window_size <= window_size or  window_size <= min_window_size:\n",
    "    raise ValueError(f\"window_size must be greater than {min_window_size} and smaller than {max_window_size}. Current window_size is {window_size}.\")\n",
    "weights = np.repeat(1.0, window_size)/window_size\n",
    "ma = np.array([np.convolve(d, weights, 'valid') for d in (train_band_data)])\n",
    "\n",
    "START = 0\n",
    "END = 200\n",
    "DIFF = END - START\n",
    "#CPlot the two for different frequencies\n",
    "NB_1 = 0\n",
    "fig, ax = plt.subplots(2, 1, figsize=(24,12))\n",
    "ax[0].plot(range(DIFF), train_band_data[NB_1, START:END], label='Time serie')\n",
    "ax[0].plot(range(DIFF), ma[NB_1, START:END], label='Moving average')\n",
    "ax[0].legend(fontsize=26)\n",
    "\n",
    "NB_2 = 11\n",
    "ax[1].plot(range(DIFF), train_band_data[NB_2, START:END], label='Time serie')\n",
    "ax[1].plot(range(DIFF), ma[NB_2, START:END], label='Moving average')\n",
    "\n",
    "for i, ax in enumerate(ax):\n",
    "    # Format subplot\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='both', labelsize=26)\n",
    "    # draw vertical lines to show the window_size\n",
    "    for x in range(START, END, 100):\n",
    "        ax.axvspan(x, x + window_size, color='g', alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89500f15-9959-4dcf-afbf-f6ca24364343",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_instances_classification:\n",
    "    xtrain_to_plot =  np.concatenate(X_train_band, axis=0).T\n",
    "else:\n",
    "    xtrain_to_plot = X_train_band.T\n",
    "\n",
    "plt.figure(figsize=(30,14))\n",
    "plt.suptitle(f'Dynamics', fontsize=16)\n",
    "\n",
    "# NEURON ACTIVITY PLOT \n",
    "ax0 = plt.subplot2grid((4, 3), (0, 0), colspan=3, rowspan=2)\n",
    "random_neurons_indices =  np.sort(np.random.randint(common_size, size=11)) #Size max is 19 because there is not enough colors\n",
    "colors = color_palette(\"tab20\")\n",
    "# NUMBER_OF_STEP_TO_WATCH\n",
    "WATCH_FROM = 700\n",
    "WATCH_TO = 1000\n",
    "x =range(WATCH_FROM, WATCH_TO)\n",
    "j = 0\n",
    "for i in random_neurons_indices:\n",
    "    ax0.plot(x, xtrain_to_plot[i][x], label=str(i), color=colors[j])\n",
    "    j += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6435e2-6f19-4987-9309-ab74694678fd",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Shared parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4dc28-34d5-4a98-a047-7fa6e4e9da47",
   "metadata": {},
   "source": [
    "**common_size** : the number of different dimensions in the input data\n",
    "\n",
    "**K** : the number of neurons that will receive a particular time serie as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433d6d3-acc2-4bbc-8373-12d692eda119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "# We want the size of the reservoir to be at least RESERVOIR_SIZE\n",
    "K = math.ceil(RESERVOIR_SIZE / common_size)\n",
    "n = common_size * K\n",
    "print(\"Dimension of our reservoir :\", n)\n",
    "print(\"Copy of each time serie :\", K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4957e6c-e0cc-4d90-a495-348a8bbab3cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "TIME_INCREMENT = 100 # int(max_window_size) or int(min_window_size+1)\n",
    "MAX_TIME_INCREMENT = TIME_INCREMENT #int(max_window_size) or None  or TIME_INCREMENT\n",
    "WEIGHT_INCREMENT = 0.08\n",
    "target_rate = 0.79\n",
    "RATE_SPREAD = 0.03\n",
    "\n",
    "min_variance = 0.004\n",
    "max_variance = 0.01\n",
    "\n",
    "bias_scaling = 0.01\n",
    "input_scaling = 0.01\n",
    "leaky_rate = 1\n",
    "\n",
    "if int(max_window_size) < TIME_INCREMENT or TIME_INCREMENT < min_window_size:\n",
    "    raise ValueError(f\"INCREMENT must be greater than {min_window_size} and smaller than {max_window_size}. Current INCREMENT is {TIME_INCREMENT}.\")\n",
    "\n",
    "TIME_INCREMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fd1fc57ed9157",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Function to initialise and generate reservoir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39af15",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.bounded_hadsp import run_hadsp_algorithm\n",
    "from connexion_generation.desp import run_desp_algorithm\n",
    "from scipy import sparse\n",
    "\n",
    "input_connectivity = 1\n",
    "connectivity = 0\n",
    "\n",
    "def initialise_and_train(input_scaling, n, input_connectivity, connectivity, K, bias_scaling, seed, training_set):\n",
    "    \n",
    "    Win, W, bias = init_matrices(n, input_connectivity, connectivity,  K, seed=seed)\n",
    "    bias *= bias_scaling\n",
    "    Win *= input_scaling\n",
    "\n",
    "    W, state_history = run_hadsp_algorithm(W, Win, bias, leaky_rate, activation_function, training_set, TIME_INCREMENT, WEIGHT_INCREMENT,\n",
    "                          target_rate, RATE_SPREAD, instances=is_instances_classification, max_increment=MAX_TIME_INCREMENT, mi_based=False, visualize=False)\n",
    "    variance_history=None\n",
    "    #W, state_history, variance_history = run_desp_algorithm(W, Win, bias, leaky_rate, activation_function, training_set, TIME_INCREMENT, WEIGHT_INCREMENT,\n",
    "    #                    min_variance, max_variance, instances=is_instances_classification, max_increment=MAX_TIME_INCREMENT, mi_based=True,\n",
    "    #                                                    n_jobs = 1, visualize=False)\n",
    "\n",
    "    connectivity =  W.count_nonzero() / (W.shape[0] * W.shape[1])\n",
    "    eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "    sr = np.max(np.abs(eigen))\n",
    "    \n",
    "    return Win, W, bias, connectivity, sr, state_history, variance_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4de89f833989d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Multivariate case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54472b03-ca39-4f13-92cf-e71e6d63c135",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HADSP\n",
    "(Win_hadsp_multi, \n",
    " W_hadsp_multi, \n",
    " bias_hadsp_multi, \n",
    " connectivity_band, \n",
    " sr_hadsp_multi, \n",
    " state_history_multi,\n",
    " variance_history) = initialise_and_train(input_scaling, n, input_connectivity, connectivity, K, bias_scaling, SEED, X_pretrain_band)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "custom_colormap = ListedColormap(np.vstack((plt.cm.cividis(0.0), plt.cm.cividis(np.linspace(0.5, 1, 128)))))\n",
    "heatmap(W_hadsp_multi.todense(), cmap=custom_colormap, cbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d44149-8fd2-4528-9cdf-1e384eff50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random + multi\n",
    "Win_random_multi, W_random_multi, bias_random_multi =  init_matrices(n, 1, connectivity_band, K, sr_hadsp_multi)\n",
    "bias_random_multi= bias_random_multi*bias_scaling\n",
    "Win_random_multi= Win_random_multi*input_scaling\n",
    "\n",
    "eigen_random_multi = sparse.linalg.eigs(W_random_multi, k=1, which=\"LM\", maxiter=W_random_multi.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "sr_random_multi = np.max(np.abs(eigen_random_multi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be4078-9172-48f0-b39f-9b9c634c136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if variance_history is not None:\n",
    "    plt.figure(figsize=(30,14))\n",
    "    plt.suptitle(f'Dynamics', fontsize=16)\n",
    "    \n",
    "    # NEURON ACTIVITY PLOT\n",
    "    ax0 = plt.subplot2grid((4, 3), (0, 0), colspan=3, rowspan=2)\n",
    "    random_neurons_indices =  np.sort(np.random.randint(RESERVOIR_SIZE, size=19)) #Size max is 19 because there is not enough colors\n",
    "    random_neurons_indices = np.append(random_neurons_indices, 51)\n",
    "    colors = color_palette(\"tab20\")\n",
    "    # NUMBER_OF_STEP_TO_WATCH \n",
    "    WATCH_FROM = 0\n",
    "    WATCH_TO = 2000\n",
    "    neurons_evolution = np.array(variance_history)[WATCH_FROM:WATCH_TO]\n",
    "    x =range(len(state_history_multi))[WATCH_FROM:WATCH_TO]\n",
    "    j = 0\n",
    "    for i in random_neurons_indices:\n",
    "        ax0.plot(x, neurons_evolution[: ,i], label=str(i), color=colors[j])\n",
    "        ax0.text(-3+WATCH_FROM, neurons_evolution[0, i],  str(i), color=\"black\", fontsize=12)\n",
    "        j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef5f04-6a58-46eb-a958-3937fa36947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,14))\n",
    "plt.suptitle(f'Dynamics', fontsize=16)\n",
    "\n",
    "# NEURON ACTIVITY PLOT \n",
    "ax0 = plt.subplot2grid((4, 3), (0, 0), colspan=3, rowspan=2)\n",
    "random_neurons_indices =  np.sort(np.random.randint(RESERVOIR_SIZE, size=4)) #Size max is 19 because there is not enough colors\n",
    "colors = color_palette(\"tab20\")\n",
    "# NUMBER_OF_STEP_TO_WATCH \n",
    "WATCH_FROM = 0\n",
    "WATCH_TO = 4000\n",
    "neurons_evolution = np.array(state_history_multi)[WATCH_FROM:WATCH_TO]\n",
    "x =range(len(state_history_multi))[WATCH_FROM:WATCH_TO]\n",
    "j = 0\n",
    "for i in random_neurons_indices:\n",
    "    ax0.plot(x, neurons_evolution[: ,i], label=str(i), color=colors[j])\n",
    "    ax0.text(-3+WATCH_FROM, neurons_evolution[0, i],  str(i), color=\"black\", fontsize=12)\n",
    "    j += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9b3d83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:36:57.885053Z",
     "start_time": "2023-10-09T13:36:57.882050Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Univariate case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d83057",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    # HADSP + uni\n",
    "    (Win_hadsp_uni, \n",
    "     W_hadsp_uni, \n",
    "     bias_hadsp_uni, \n",
    "     connectivity_hadsp_uni, \n",
    "     sr_hadsp_uni,\n",
    "     state_history_hadsp_uni,\n",
    "     _) = initialise_and_train(input_scaling, n, input_connectivity, connectivity, n, bias_scaling, SEED, X_pretrain_uni)\n",
    "    \n",
    "    # random + uni\n",
    "    Win_normal, W_normal, bias_normal = init_matrices(n, 1, connectivity_hadsp_uni, n, sr_hadsp_uni)\n",
    "    bias_normal= bias_normal*bias_scaling\n",
    "    Win_normal= Win_normal*input_scaling\n",
    "    \n",
    "    eigen_normal = sparse.linalg.eigs(W_normal, k=1, which=\"LM\", maxiter=W_normal.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "    sr_normal = np.max(np.abs(eigen_normal))\n",
    "    \n",
    "    from matplotlib.colors import ListedColormap\n",
    "    \n",
    "    custom_colormap = ListedColormap(np.vstack((plt.cm.cividis(0.0), plt.cm.cividis(np.linspace(0.5, 1, 128)))))\n",
    "    heatmap(W_hadsp_uni.todense(), cmap=custom_colormap, cbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e52b1b1-fc44-43ad-930a-dc99417cd585",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(connectivity_band)\n",
    "if not is_multivariate:\n",
    "    print(connectivity_hadsp_uni)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68129bc2-1cc6-4498-9e57-ed18e037643e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:38:25.203377Z"
    }
   },
   "source": [
    "## Spectral radius normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49826ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sr_hadsp_multi)\n",
    "print(sr_random_multi)\n",
    "if not is_multivariate:\n",
    "    print(sr_normal)\n",
    "    print(sr_hadsp_uni)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41d2e41f-cbfd-493e-bad0-bbb651ca46e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T15:00:13.802481Z",
     "iopub.status.busy": "2024-03-21T15:00:13.801956Z",
     "iopub.status.idle": "2024-03-21T15:00:13.813032Z",
     "shell.execute_reply": "2024-03-21T15:00:13.812057Z",
     "shell.execute_reply.started": "2024-03-21T15:00:13.802451Z"
    }
   },
   "source": [
    "# Spectral radius normalisation\n",
    "normal_sr = 0.9\n",
    "W_hadsp_multi = W_hadsp_multi/sr_hadsp_multi*normal_sr\n",
    "W_random_multi = W_random_multi/sr_random_multi*normal_sr\n",
    "if not is_multivariate:\n",
    "    W_normal = W_normal/sr_normal*normal_sr\n",
    "    W_hadsp_uni = W_hadsp_uni/sr_hadsp_uni*normal_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b447f6-e964-4f6c-9116-597cd29c1755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:13:26.845564Z",
     "start_time": "2023-10-09T13:13:26.821527Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ffd9c-ffc5-47ea-885d-f2f2a3e549fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "N_JOBS = -1\n",
    "RIDGE_COEF= 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27380a6-3976-442d-8bb0-34bfc142985a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b3a4e-b06e-450a-b7c5-68802be8013d",
   "metadata": {},
   "source": [
    "### Classification for multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3534f99-25b5-4653-8dcf-7073183e0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_band_duplicated example shape :\", X_train_band_duplicated[1].shape)     \n",
    "print(\"We should have :\", X_train_band_duplicated[0].shape[1], \"==\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9130e-a834-4256-8f29-e42add2eee19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import init_and_train_model_for_classification\n",
    "# To remember : \n",
    "#  For reservoirpy   pre_s = W @ r + Win @ (u + noise_gen(dist=dist, shape=u.shape, gain=g_in)) + bias\n",
    "\n",
    "train_data_multi = X_train_band_duplicated # X_train_band_noisy_duplicated or X_train_band_duplicated\n",
    "\n",
    "reservoir_hadsp_multi, readout_hadsp_multi = init_and_train_model_for_classification(W_hadsp_multi, np.diag(Win_hadsp_multi.A.T[0]), bias_hadsp_multi, leaky_rate, activation_function, train_data_multi, Y_train, N_JOBS, RIDGE_COEF, mode=\"sequence-to-vector\")\n",
    "\n",
    "reservoir_random_multi, readout_random_multi = init_and_train_model_for_classification(W_random_multi, np.diag(Win_random_multi.A.T[0]), bias_random_multi, leaky_rate, activation_function, train_data_multi, Y_train, N_JOBS, RIDGE_COEF, mode=\"sequence-to-vector\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e344e-de91-4709-bfce-467b1d049546",
   "metadata": {},
   "source": [
    "#### noisy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3dc743-36c8-4821-948d-f14f43af8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import predict_model_for_classification, compute_score\n",
    "\n",
    "test_data_multi = X_test_band_noisy_duplicated # X_test_band_noisy_duplicated or X_test_band_duplicated\n",
    "\n",
    "Y_pred_hadsp_multi = predict_model_for_classification(reservoir_hadsp_multi, readout_hadsp_multi, test_data_multi, N_JOBS)\n",
    "score = compute_score(Y_pred_hadsp_multi, Y_test, is_instances_classification, \"HADSP multi\", verbosity=1)\n",
    "\n",
    "Y_pred_random_multi = predict_model_for_classification(reservoir_random_multi, readout_random_multi, test_data_multi, N_JOBS)\n",
    "score = compute_score(Y_pred_random_multi, Y_test, is_instances_classification, \"random multi\", verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6287d16-2f61-4d2a-97e3-0a0844918879",
   "metadata": {},
   "source": [
    "#### normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49735fb0-8701-4a4f-8d0c-a319c142f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_multi = X_test_band_duplicated # X_test_band_noisy_duplicated or X_test_band_duplicated\n",
    "\n",
    "Y_pred_hadsp_multi = predict_model_for_classification(reservoir_hadsp_multi, readout_hadsp_multi, test_data_multi, -1)\n",
    "score = compute_score(Y_pred_hadsp_multi, Y_test, is_instances_classification, \"HADSP multi\", verbosity=1)\n",
    "\n",
    "Y_pred_random_multi = predict_model_for_classification(reservoir_random_multi, readout_random_multi, test_data_multi, N_JOBS)\n",
    "score = compute_score(Y_pred_random_multi, Y_test, is_instances_classification, \"random multi\", verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f3553-1d28-4340-b4b9-f3dc673bdcac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:59:05.097521Z",
     "start_time": "2023-10-09T13:59:04.992489Z"
    }
   },
   "source": [
    "### Classification for univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50e2ca-93d7-48bb-bcff-4f885e8d5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_multivariate: \n",
    "    # Create a list to store the arrays with the same shape as the expected input of the reservoir\n",
    "\n",
    "    train_data_uni = [ts.reshape(-1, 1) for ts in X_train]\n",
    "    test_data_uni = [ts.reshape(-1, 1) for ts in X_test]\n",
    "\n",
    "    print(\"number of instances in train_data_uni :\", len(train_data_uni), \"should be equal to\", len(X_train))     \n",
    "    print(\"example of train_data_uni train shape :\", train_data_uni[0].shape)     \n",
    "    print(\"We should have :\", train_data_uni[0].shape[1], \"==\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d01e8-8763-4bf3-ae0a-4c4569231dc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    reservoir_hadsp_uni, readout_hadsp_uni = init_and_train_model_for_classification(W_hadsp_uni, Win_hadsp_uni, bias_hadsp_uni, activation_function, RIDGE_COEF, train_data_uni, Y_train, N_JOBS)\n",
    "    \n",
    "    reservoir_random_uni, readout_random_uni = init_and_train_model_for_classification(W_normal, Win_normal, bias_normal, activation_function, RIDGE_COEF, train_data_uni, Y_train, N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f4028-f457-46b4-a219-7f5fed575af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    Y_pred_hadsp_uni = predict_model_for_classification(reservoir_hadsp_uni, readout_hadsp_uni, test_data_uni, N_JOBS)\n",
    "    score = compute_score(Y_pred_hadsp_uni, Y_test, \"HADSP uni\")\n",
    "    \n",
    "    Y_pred_normal = predict_model_for_classification(reservoir_random_uni, readout_random_uni, test_data_uni, N_JOBS)\n",
    "    score = compute_score(Y_pred_normal, Y_test, \"random uni\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7ab1a-e020-489a-8ff9-9bbd689de15e",
   "metadata": {},
   "source": [
    "## Prediction ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c06e5-71c1-4f72-ac0f-7e4f1f15afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_instances_classification:\n",
    "    raise ValueError(\"This is not the right Classification section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11c5fc-45b0-4180-9953-bb13800da95b",
   "metadata": {},
   "source": [
    "### Plot datasets\n",
    "Noisy or normal dataset can be ploted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d45a7-49c9-4e1e-a473-652bc90d64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test arrays for plotting\n",
    "combined_data = np.concatenate((X_train_band, X_val_band), axis=0)\n",
    "\n",
    "# noisy version\n",
    "combined_data_noisy = np.concatenate((X_train_band, X_val_band_noisy), axis=0)\n",
    "combined_Y =np.concatenate((Y_train, Y_val), axis=0)\n",
    "\n",
    "# Calculate the merge point index\n",
    "merge_point_index = X_train_band.shape[0]\n",
    "\n",
    "# Define the range around the merge point to plot\n",
    "start_index = merge_point_index - 100\n",
    "end_index = merge_point_index + 100\n",
    "\n",
    "# Plot for a subset N features within a range arround transition from train to test\n",
    "N = 3\n",
    "plt.figure(figsize=(16, 5))\n",
    "for i in [1, 13, 17]: \n",
    "    plt.plot(range(start_index, end_index), combined_data_noisy[start_index:end_index, i], label=f'Feature {i}')\n",
    "plt.plot(range(start_index, end_index), combined_Y[start_index:end_index], label=\"Prediction\")\n",
    "plt.title('Feature Values Around Merge Point')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Feature Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bcdbf",
   "metadata": {},
   "source": [
    "### Training\n",
    "Noisy or normal dataset can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d012581-4559-4924-8ba1-9d1b6dea2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import init_and_train_model_for_prediction\n",
    "\n",
    "if not is_multivariate:\n",
    "    train_data_uni = X_train # X_train_noisy or X_train\n",
    "    \n",
    "    # Training random + MG\n",
    "    esn_random_uni = init_and_train_model_for_prediction(W_normal, Win_normal, bias_normal, leaky_rate, activation_function, train_data_uni, Y_train, RIDGE_COEF)\n",
    "    \n",
    "    # Training for HADSP + MG\n",
    "    esn_hadsp_uni = init_and_train_model_for_prediction(W_hadsp_uni, Win_hadsp_uni, bias_hadsp_uni, leaky_rate, activation_function, train_data_uni, Y_train, RIDGE_COEF)\n",
    "\n",
    "train_data_multi = X_train_band # X_train_band_noisy or train_band_inputs\n",
    "\n",
    "# Training random + bandfilter\n",
    "esn_random_multi = init_and_train_model_for_prediction(W_random_multi, Win_random_multi, bias_random_multi, leaky_rate, activation_function, train_data_multi, Y_train, RIDGE_COEF)\n",
    "\n",
    "# Training output HASDP + bandfilter\n",
    "esn_hadsp_multi = init_and_train_model_for_prediction(W_hadsp_multi, Win_hadsp_multi, bias_hadsp_multi, leaky_rate, activation_function, train_data_multi, Y_train, RIDGE_COEF)\n",
    "                                                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcfe07-1ad0-4d8f-827f-bb9776e8086e",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Noisy or normal dataset can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dac84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from reservoir.reservoir import run\n",
    "\n",
    "if not is_multivariate:\n",
    "    test_data_uni = X_val_noisy # X_val_noisy or X_val\n",
    "\n",
    "    # Prediction for random + MG\n",
    "    y_pred_random_uni = esn_random_uni.run(test_data_uni, reset=False) \n",
    "\n",
    "    # Prediction for HADSP + MG\n",
    "    y_pred_hadsp_uni = esn_hadsp_uni.run(test_data_uni, reset=False) \n",
    "\n",
    "test_data_multi = X_test_band # X_test_band_noisy_duplicated or X_test_band_duplicated\n",
    "\n",
    "# Prediction for random + bandfilter\n",
    "y_pred_random_multi = esn_random_multi.run(test_data_multi, reset=False)\n",
    "\n",
    "# Prediction for HADSP + bandfilter\n",
    "y_pred_hadsp_multi = esn_hadsp_multi.run(test_data_multi, reset=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61165568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from plots.plots import plot_results\n",
    "from performances.esn_model_evaluation import compute_score\n",
    "\n",
    "START_STEP = 30\n",
    "END_STEP = 500\n",
    "slice_range = slice(START_STEP, END_STEP)\n",
    "\n",
    "if not is_multivariate:\n",
    "    print(\"nrmse normal         :\", compute_score(Y_val[slice_range], y_pred_random_uni[slice_range], is_instances_classification))\n",
    "    print(\"nrmse hadsp          :\", compute_score(Y_val[slice_range], y_pred_hadsp_uni[slice_range], is_instances_classification))\n",
    "print(\"nrmse random + band  :\", compute_score(Y_val[slice_range], y_pred_random_multi[slice_range], is_instances_classification))\n",
    "print(\"nrmse hadsp + band   :\", compute_score(Y_val[slice_range], y_pred_hadsp_multi[slice_range], is_instances_classification))\n",
    "\n",
    "plot_results(y_pred_hadsp_multi, Y_test, 0, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645bd84-e26a-497c-9053-6f6dd4e69845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reservoir.reservoir import run\n",
    "\n",
    "if not is_multivariate:\n",
    "    test_data_uni = X_val_noisy # X_val_noisy or X_val\n",
    "\n",
    "    # Prediction for random + MG\n",
    "    y_pred_random_uni = esn_random_uni.run(test_data_uni, reset=False) \n",
    "\n",
    "    # Prediction for HADSP + MG\n",
    "    y_pred_hadsp_uni = esn_hadsp_uni.run(test_data_uni, reset=False) \n",
    "\n",
    "test_data_multi = X_test_band_noisy # X_test_band_noisy_duplicated or X_test_band_duplicated\n",
    "\n",
    "# Prediction for random + bandfilter\n",
    "y_pred_random_multi = esn_random_multi.run(test_data_multi, reset=False)\n",
    "\n",
    "# Prediction for HADSP + bandfilter\n",
    "y_pred_hadsp_multi = esn_hadsp_multi.run(test_data_multi, reset=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d132a1-0766-497f-bd98-635048b14b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots.plots import plot_results\n",
    "\n",
    "START_STEP = 30\n",
    "END_STEP = 500\n",
    "slice_range = slice(START_STEP, END_STEP)\n",
    "\n",
    "if not is_multivariate:\n",
    "    print(\"nrmse normal         :\", compute_score(y_pred_random_uni[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "    print(\"nrmse hadsp          :\", compute_score(y_pred_hadsp_uni[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse random + band  :\", compute_score(y_pred_random_multi[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse hadsp + band   :\", compute_score(y_pred_hadsp_multi[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "\n",
    "plot_results(y_pred_hadsp_multi, Y_test, 0, 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb0591-4213-425e-92ff-a5da80e718dc",
   "metadata": {},
   "source": [
    "#### Moving average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5bf2d-368f-4e55-b068-c42c36efe2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import compute_score\n",
    "\n",
    "# moving average of the y\n",
    "span=7\n",
    "pad_width = span // 2\n",
    "\n",
    "ave_y_random_uni = np.convolve(np.pad(y_pred_random_uni.flatten(), pad_width, mode='edge'), np.ones(span), 'valid') / span\n",
    "ave_y_hadsp_uni = np.convolve(np.pad(y_pred_hadsp_uni.flatten(), pad_width, mode='edge') , np.ones(span), 'valid') / span\n",
    "ave_y_random_multi = np.convolve(np.pad(y_pred_random_multi.flatten(), pad_width, mode='edge'), np.ones(span), 'valid') / span\n",
    "ave_y_hadsp_multi = np.convolve(np.pad(y_pred_hadsp_multi.flatten(), pad_width, mode='edge'), np.ones(span), 'valid') / span\n",
    "\n",
    "\n",
    "print(\"nrmse normal         :\", compute_score(ave_y_random_uni[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse hadsp          :\", compute_score(ave_y_hadsp_uni[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse random + band  :\", compute_score(ave_y_random_multi[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse hadsp + band   :\", compute_score(ave_y_hadsp_multi[slice_range], Y_val[slice_range], is_instances_classification))\n",
    " \n",
    "plot_results(ave_y_hadsp_multi.reshape(-1,1), Y_test, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc19b97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nrmse_array_random_uni = []\n",
    "nrmse_array_hadsp_uni = []\n",
    "nrmse_array_random_multi = []\n",
    "nrmse_array_hadsp_multi = []\n",
    "\n",
    "for i in range(len(Y_val)-100- step_ahead):\n",
    "    Y_val_i = Y_val[i:100+i]\n",
    "    nrmse_array_random_uni.append(compute_score(Y_val_i, y_pred_random_uni[i:100+i], is_instances_classification))\n",
    "    nrmse_array_hadsp_uni.append(compute_score(Y_val_i, y_pred_hadsp_uni[i:100+i], is_instances_classification))\n",
    "    nrmse_array_random_multi.append(compute_score(Y_val_i, y_pred_random_multi[i:100+i], is_instances_classification))\n",
    "    nrmse_array_hadsp_multi.append(compute_score(Y_val_i, y_pred_hadsp_multi[i:100+i], is_instances_classification))\n",
    "    \n",
    "log10_nrmse_random_uni= np.log10(nrmse_array_random_uni)\n",
    "log10_nrmse_hadsp_uni = np.log10(nrmse_array_hadsp_uni)\n",
    "log10_nrmse_random_multi = np.log10(nrmse_array_random_multi)\n",
    "log10_nrmse_hadsp_multi = np.log10(nrmse_array_hadsp_multi)\n",
    "plt.figure()\n",
    "plt.plot(log10_nrmse_random_uni[:1000])\n",
    "plt.plot(log10_nrmse_hadsp_uni[:1000])\n",
    "plt.plot(log10_nrmse_random_multi[:1000])\n",
    "plt.plot(log10_nrmse_hadsp_multi[:1000])\n",
    "\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Log10 NRMSE')\n",
    "plt.legend([\"HADSP+band\", \"random\", \" random + bandfilter\", \"HADSP\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27da37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2478c-bb79-4bc0-855f-b028109c8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# pretrain\n",
    "# Be really carefull of the column order here !\n",
    "df_data = scaler.fit_transform(X_pretrain_band.T)\n",
    "df_data = df_data.T\n",
    "df = pd.DataFrame(df_data.T)\n",
    "# Initialize a progress bar for total number of series\n",
    "progress_bar = tqdm(total=df.shape[1]**2, position=0, leave=True)\n",
    "\n",
    "# Initialize an empty correlation matrix\n",
    "correlation_matrix = pd.DataFrame(index=df.columns, columns=df.columns)\n",
    "\n",
    "# Calculate the correlation for each pair of series\n",
    "for col1 in df.columns:\n",
    "    progress_bar.set_description(f\"Processing {col1}\")\n",
    "    for col2 in df.columns:\n",
    "        correlation_matrix.loc[col1, col2] = df[col1].corr(df[col2], method='pearson', min_periods=5)\n",
    "\n",
    "        progress_bar.update(1)  # Update the progress bar after processing each series\n",
    "    \n",
    "progress_bar.close()\n",
    "\n",
    "# Convert correlation_matrix to numeric as it is stored as objects due to tqdm\n",
    "correlation_matrix = correlation_matrix.apply(pd.to_numeric)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linked = linkage(correlation_matrix, 'single')\n",
    "\n",
    "# Get the order of rows/columns after hierarchical clustering\n",
    "row_order = leaves_list(linked)\n",
    "\n",
    "# Reorder the correlation matrix\n",
    "sorted_corr_matrix = correlation_matrix.iloc[row_order, :].iloc[:, row_order]\n",
    "\n",
    "# Visualize the sorted correlation matrix with a heatmap\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(sorted_corr_matrix, annot=False, cmap='vlag', vmin=-1, vmax=1)\n",
    "plt.title('Clustered Pairwise Correlation of Time Series')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec25bb-f3e8-4cdb-87ee-9772460a8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "row_order_r = np.array([i + k for i in row_order*K for k in range(K)])\n",
    "\n",
    "# Convert the sparse matrix to a dense format (if memory allows)\n",
    "dense_matrix = W_hadsp_multi.toarray()\n",
    "\n",
    "# Reorder the dense matrix using the repeated ordering\n",
    "reordered_matrix = dense_matrix[np.ix_(row_order_r, row_order_r)]\n",
    "\n",
    "# Convert the reordered dense matrix back to a sparse format if needed\n",
    "sparse_reordered_matrix = coo_matrix(reordered_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901730be-d407-4d6d-973d-9fc24951bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(sparse_reordered_matrix.todense(), cmap=color_palette(\"vlag\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e46145-b737-4e62-a8ed-9860be34eb2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T12:52:00.989091Z",
     "start_time": "2023-10-10T12:52:00.984122Z"
    }
   },
   "source": [
    "## Motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78de13-7b2d-4ea0-9854-c7ef05c9780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.topology\n",
    "reload(analysis.topology)\n",
    "from analysis.topology import motif_distribution, draw_motifs_distribution\n",
    "\n",
    "motifs_count = motif_distribution(W_hadsp_multi.A)\n",
    "draw_motifs_distribution(motifs_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7d48e-892f-44a0-9f33-a0b561e3a1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4480d-5e4f-4603-90c0-e2f0576e0288",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson, binom\n",
    "\n",
    "def analyze_connectivity_matrix(matrix):\n",
    "    # Extract weights from the matrix (ignoring the diagonal and zeros)\n",
    "    weights = matrix.flatten()\n",
    "    weights = weights[weights != 0]\n",
    "    bin_centers, counts = np.unique(weights, return_counts=True)\n",
    "    \n",
    "    # Calculate the difference for all centers\n",
    "    diffs = np.diff(bin_centers)\n",
    "    # Add the last difference for the last bin\n",
    "    diffs = np.append(diffs, diffs[-1])\n",
    "    \n",
    "    # Calculate the bin edges based on bin centers and differences\n",
    "    bin_edges = bin_centers - diffs/2\n",
    "    # Add the last bin edge\n",
    "    bin_edges = np.append(bin_edges, bin_centers[-1] + diffs[-1]/2)\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.bar(bin_centers, counts, align='center', alpha=0.6, width=np.diff(bin_centers).min())\n",
    "    \n",
    "    # Fit to Poisson distribution\n",
    "    lambda_est = np.mean(weights)\n",
    "    plt.plot(bin_centers, poisson.pmf(range(len(bin_centers)), lambda_est)*counts[0], 'r-', label='Poisson fit')\n",
    "    \n",
    "    # Fit to Binomial distribution using derived relations\n",
    "    mean = np.mean(weights)\n",
    "    variance = np.var(weights)\n",
    "    \n",
    "    # Calculate p and n estimates\n",
    "    p_est = mean ** 2 / (n * mean - variance) if (n * mean - variance) != 0 else 0\n",
    "    n_est = int(round(mean / p_est)) if p_est != 0 else 0  # n should be integer\n",
    "\n",
    "    # Check parameter validity\n",
    "    if not(0 < p_est < 1):\n",
    "        print(\"Estimated parameters are not valid for the Binomial distribution.\")\n",
    "    else:\n",
    "        x_vals = range(len(bin_centers))\n",
    "        plt.plot(bin_centers, binom.pmf(x_vals, n_est, p_est) * counts[0], 'g-', label='Binomial fit')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return {\"Poisson\": lambda_est, \"Binomial\": (n_est, p_est)}\n",
    "\n",
    "\n",
    "# Assuming W_hadsp_multi.A is your connectivity matrix\n",
    "analyze_connectivity_matrix(W_hadsp_multi.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbe35b-18fa-4dc7-b598-dfd345e90ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_centers = np.array([1.5, 3.5, 5.5, 7.5])\n",
    "poisson.pmf(np.arange(len([1, 2, 3, 4, 5, 6])), 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6dbf8-e09f-4062-a3ee-6c320981b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6614db-e2e1-4e5c-8d97-567646d6a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hadsp_multi.A.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a3138-e1fd-497f-b380-dcf350a3faa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hadsp_env",
   "language": "python",
   "name": "hadsp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
