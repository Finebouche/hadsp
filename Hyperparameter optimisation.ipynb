{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c1d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:04:18.159981Z",
     "start_time": "2023-10-23T14:04:18.158428Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the root project folder to the python path in order to use the packages\n",
    "path_root = Path( '/project_ghent/HADSP/hadsp/')\n",
    "sys.path.append(str(path_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f790cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:04:18.164863Z",
     "start_time": "2023-10-23T14:04:18.160678Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from importlib import reload\n",
    "\n",
    "# SEED\n",
    "SEED = 49387\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from seaborn import heatmap, color_palette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44384e-7cbc-4611-a015-e1c073f61aa9",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "Lots of different on availabale : https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad\n",
    "\n",
    "Classification: \n",
    "https://arxiv.org/abs/1803.07870\n",
    "\n",
    "https://github.com/FilippoMB/Time-series-classification-and-clustering-with-Reservoir-Computing\n",
    "\n",
    "Multivariate:\n",
    "https://www.timeseriesclassification.com/dataset.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb81c7-9923-4fee-a6ed-98a84b3b7b6f",
   "metadata": {},
   "source": [
    "## Torchaudio\n",
    "\n",
    "https://pytorch.org/audio/stable/datasets.html\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb76656a-1524-43db-b431-e80fcee53dc5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# load dataset using torchaudio\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torchaudio.datasets import VoxCeleb1Identification, SPEECHCOMMANDS\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "dataset = SPEECHCOMMANDS(root=\"datasets/\", download=True)\n",
    "\n",
    "sampling_rate = dataset[0][1]\n",
    "X = [sample[0][0] for sample in dataset]\n",
    "Y = [sample[2] for sample in dataset]\n",
    "\n",
    "dataset_size = len(dataset)  # Total number of samples in the dataset\n",
    "\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "\n",
    "# Use StratifiedShuffleSplit to get train/test indices\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "train_index, test_index = next(sss.split(X, Y))\n",
    "\n",
    "# Split data and labels using the indices\n",
    "X_train = X[train_index]\n",
    "Y_train = Y[train_index]\n",
    "X_test = X[test_index]\n",
    "Y_test = Y[test_index]\n",
    "\n",
    "\n",
    "is_multivariate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d6272-708d-4709-8e35-5a84268bed64",
   "metadata": {},
   "source": [
    "## Prediction ahead\n",
    "\n",
    "Datasets available :\n",
    "\n",
    "* MackeyGlass\n",
    "* Lorenz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c17065-e9f3-4af5-b48c-7d77fb2d415e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-04T09:59:16.044618Z",
     "iopub.status.busy": "2024-04-04T09:59:16.044453Z",
     "iopub.status.idle": "2024-04-04T09:59:17.104161Z",
     "shell.execute_reply": "2024-04-04T09:59:17.103898Z",
     "shell.execute_reply.started": "2024-04-04T09:59:16.044610Z"
    },
    "scrolled": true
   },
   "source": [
    "from datasets.load_datasets import load_dataset_prediction\n",
    "is_instances_classification = False\n",
    "dataset_name = \"MackeyGlass\"\n",
    "step_ahead=5\n",
    "\n",
    "is_multivariate, sampling_rate, X_train, X_test, Y_train, Y_test, X_pretrain = load_dataset_prediction(dataset_name, step_ahead, visualize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b219b-eb64-4715-b983-7de3c392f088",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Classification\n",
    "\n",
    "Datasets available :\n",
    "\n",
    "* FSDD\n",
    "* HAART\n",
    "* JapaneseVowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999e329-f417-463e-b94b-2dd989c26177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets.load_datasets import load_dataset_classification\n",
    "is_instances_classification = True\n",
    "dataset_name = \"JapaneseVowels\"\n",
    "\n",
    "is_multivariate, sampling_rate, X_train, X_test, Y_train, Y_test, X_pretrain = load_dataset_classification(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c0280-4742-421c-b52a-40567511608b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Multivariate generation if necessary\n",
    "\n",
    "Spectrograms_vs_Cochleagrams : https://www.researchgate.net/publication/340510607_Speech_recognition_using_very_deep_neural_networks_Spectrograms_vs_Cochleagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1405c-d01a-4d1a-8764-cd0624d51f79",
   "metadata": {},
   "source": [
    "Attention ! For multivariate shape should be : (nb_of_timeseries, nb_of_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c138e4b7-ec1b-47c3-b8d4-dea6d63ea3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_multivariate:\n",
    "    X_train_band, X_test_band, X_pretrain_band = X_train, X_test, X_pretrain\n",
    "    del X_train\n",
    "    del X_test\n",
    "    del X_pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc73467-4a8b-4a76-a2e8-8c2e5012ca90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:11:18.377724Z",
     "start_time": "2023-10-20T11:11:18.143305Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets.multivariate_generation\n",
    "reload(datasets.multivariate_generation)\n",
    "\n",
    "from datasets.multivariate_generation import extract_peak_frequencies\n",
    "\n",
    "if is_multivariate:\n",
    "    filtered_peak_freqs = extract_peak_frequencies(X_pretrain_band.T, sampling_rate, threshold=1e-5, nperseg=1024, visualize=True)\n",
    "else:\n",
    "    filtered_peak_freqs = extract_peak_frequencies(X_pretrain, sampling_rate, threshold=1e-5, nperseg=1024, visualize=True)\n",
    "\n",
    "#print(\"Filtered peak frequencies: \", filtered_peak_freqs)\n",
    "print(\"Number of frequencies selected :\", len(filtered_peak_freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b05b48-0913-44b8-83a8-bfbceb2cdca4",
   "metadata": {},
   "source": [
    "### Applying normal band pass filter on data and standardisation (inside the function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727961f-11f5-4c22-be16-1375d1f79000",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:11:39.227368Z",
     "start_time": "2023-10-20T11:11:19.130705Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets.multivariate_generation import generate_multivariate_dataset, extract_peak_frequencies\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "if not is_multivariate:\n",
    "    X_pretrain_band, X_train_band, X_test_band = generate_multivariate_dataset(\n",
    "        filtered_peak_freqs, X_pretrain, X_train, X_test, sampling_rate, is_instances_classification, nb_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac1066-6a2b-42ca-a8df-b5113744526d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Standardizing the amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45720c4e-8bdd-4b99-8cbd-8033e08ad915",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:12:00.856649Z",
     "start_time": "2023-10-20T11:11:39.230512Z"
    }
   },
   "outputs": [],
   "source": [
    "# pretrain\n",
    "# Be really carefull of the column order here !\n",
    "filtered_data = scaler.fit_transform(X_pretrain_band.T)\n",
    "filtered_data = filtered_data.T\n",
    "\n",
    "\n",
    "if is_instances_classification:\n",
    "    print(\"processing for Classification\")\n",
    "    X_train_band = [scaler.fit_transform(time_series) for time_series in tqdm(X_train_band)]\n",
    "    X_test_band = [scaler.fit_transform(time_series) for time_series in tqdm(X_test_band)]\n",
    "\n",
    "    if not is_multivariate:\n",
    "        # train\n",
    "        X_train = [scaler.fit_transform(x).flatten() for x in tqdm(X_train)]\n",
    "    \n",
    "        # test\n",
    "        X_test = [scaler.fit_transform(x).flatten() for x in tqdm(X_test)]\n",
    "else :\n",
    "    print(\"processing for Prediction\")\n",
    "    train_len= X_train_band.shape[0]\n",
    "\n",
    "    concatenated_Y = np.concatenate([Y_train, Y_test])\n",
    "    standardized_Y = scaler.fit_transform(concatenated_Y)\n",
    "    Y_train = standardized_Y[:train_len]\n",
    "    Y_test = standardized_Y[train_len:]\n",
    "\n",
    "    # FOR MULTIVARIATE DATA\n",
    "    concatenated_X_band = np.concatenate([X_train_band, X_test_band])\n",
    "    standardized_X_band = []\n",
    "    for timeseries in concatenated_X_band.T:\n",
    "        standardized_X_band.append(scaler.fit_transform(timeseries.reshape(-1,1)))\n",
    "    standardized_X_band = np.array(standardized_X_band).reshape(concatenated_X_band.T.shape).T\n",
    "    X_train_band = standardized_X_band[:train_len]\n",
    "    X_test_band = standardized_X_band[train_len:]\n",
    "        \n",
    "    if not is_multivariate:\n",
    "        concatenated_X = np.concatenate([X_train.flatten(), X_test.flatten()])\n",
    "        standardized_X = scaler.fit_transform(concatenated_X.reshape(-1, 1))\n",
    "        X_train = standardized_X[:train_len, :]\n",
    "        X_test = standardized_X[train_len:, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b9189-8adf-46e6-b677-fa5c9295f830",
   "metadata": {},
   "source": [
    "## Plot pretraining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab965ec2-e31c-475e-b452-05c9dbfde13b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:12:00.858650Z",
     "start_time": "2023-10-20T11:12:00.856783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Min window size to get all the dynamics ? \n",
    "min_window_size = sampling_rate/np.max(np.hstack(filtered_peak_freqs))\n",
    "max_window_size = sampling_rate/np.min(np.hstack(filtered_peak_freqs))\n",
    "\n",
    "print(min_window_size)\n",
    "print(max_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd00d3f-9b0c-475f-bac9-b46e5f92adfb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-20T11:12:00.862019Z"
    },
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Compute the moving average \n",
    "window_size = 10\n",
    "\n",
    "if max_window_size <= window_size or  window_size <= min_window_size:\n",
    "    raise ValueError(f\"window_size must be greater than {min_window_size} and smaller than {max_window_size}. Current window_size is {window_size}.\")\n",
    "\n",
    "weights = np.repeat(1.0, window_size)/window_size\n",
    "ma = np.array([np.convolve(d, weights, 'valid') for d in (filtered_data)])\n",
    "\n",
    "END = 1500\n",
    "START = 1000\n",
    "DIFF = END - START\n",
    "#CPlot the two for different frequencies\n",
    "NB_1 = 1\n",
    "fig, ax = plt.subplots(3, 1, figsize=(24,18))\n",
    "ax[0].plot(range(DIFF), filtered_data[NB_1, START:END], label='Time serie')\n",
    "ax[0].plot(range(DIFF), ma[NB_1, START:END], label='Moving average')\n",
    "ax[0].legend(fontsize=26)\n",
    "\n",
    "NB_2 = 2\n",
    "ax[1].plot(range(DIFF), filtered_data[NB_2, START:END], label='Time serie')\n",
    "ax[1].plot(range(DIFF), ma[NB_2, START:END], label='Moving average')\n",
    "\n",
    "#Check that the scaler did a good job (this is the not scaled version)\n",
    "ax[2].plot(range(DIFF), X_pretrain_band[NB_2, START:END], label='Time serie')\n",
    "\n",
    "for i, ax in enumerate(ax):\n",
    "    # Format subplot\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='both', labelsize=26)\n",
    "    # draw vertical lines to represent the window for some points\n",
    "    for x in range(100, 500, 100):\n",
    "        ax.axvspan(x, x + window_size, color='g', alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c58ac-db3a-4ae5-804b-24bdc715bd07",
   "metadata": {},
   "source": [
    "## Find common dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2952d3b8-bfa3-484b-a6a8-cf6719f568ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_dimension(array1, array2):\n",
    "    matching_indices = None\n",
    "    matching_values = None\n",
    "\n",
    "    for i, dim in enumerate(array1.shape):\n",
    "        if dim in array2.shape:\n",
    "            matching_indices = i\n",
    "            matching_values = dim\n",
    "  \n",
    "    return matching_indices, matching_values\n",
    "\n",
    "if isinstance(X_train_band, list): # Multiple instances -> classification\n",
    "    common_xtrain_index, common_xtrain_size = find_common_dimension(X_train_band[0], filtered_data)\n",
    "else:\n",
    "    common_xtrain_index = 0\n",
    "    common_xtrain_size = filtered_data.shape[common_xtrain_index]\n",
    "\n",
    "print(\"Common dimension index is :\", common_xtrain_index)\n",
    "print(\"Number of different time series is :\", common_xtrain_size)\n",
    "if is_multivariate:\n",
    "    print(\"\\nCheck it ! \\nFirst array \", X_train_band[0].shape, \" and second array\", X_train_band[1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37643d17-fc4f-440f-867b-ce9b637b9640",
   "metadata": {},
   "source": [
    "# Generating reservoirs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e92807-74de-4686-9f90-764de15fc731",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Reservoir functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a38d2df41a727",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from reservoir.activation_functions import tanh, heaviside, sigmoid\n",
    "\n",
    "# the activation function choosen for the rest of the experiment\n",
    "# activation_function = lambda x : sigmoid(2*(x-0.5))\n",
    "activation_function = lambda x : tanh(x)\n",
    "\n",
    "plt.plot(np.linspace(0, 1.1, 100), activation_function(np.linspace(0, 1.1, 100)))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f6c87-69f8-4caa-b51f-0e471e7a48c2",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Timeseries duplications to adapt to reservoir size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f2163-c5e3-4954-97e3-39e3b3b82269",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here we calculate \n",
    "**common_dimension** : the number of different dimensions in the input data\n",
    " **K** : the number of euron that will receive a particular time serie as input \n",
    "**n** : the dimension of the reservoir \n",
    "\n",
    "n = K * common_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f44547",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "RESERVOIR_SIZE = 500\n",
    "\n",
    "# We want the size of the reservoir to be at least 200\n",
    "K = math.ceil(RESERVOIR_SIZE / common_xtrain_size)\n",
    "n = common_xtrain_size * K\n",
    "print(\"Dimension of our reservoir :\", n)\n",
    "print(\"Copy of each time serie :\", K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5923c31-23c8-4450-b014-3fcfce94a32b",
   "metadata": {},
   "source": [
    "## Datasets formating and noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a01ef-7b49-4493-aa96-3ea65b18b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define noise parameter\n",
    "noise_std = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e965109a-49c3-4409-9196-f5c3e9874368",
   "metadata": {},
   "source": [
    "### Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab577a-d763-4ad4-99a3-e62d9139a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRETRAIN\n",
    "filtered_data_noisy = []\n",
    "for instance in filtered_data:\n",
    "    # Add noise to the time series\n",
    "    filtered_data_noisy.append(instance + np.random.normal(0, noise_std, instance.shape))\n",
    "\n",
    "if not is_multivariate:\n",
    "    X_pretrain_noisy = X_pretrain + np.random.normal(0, noise_std, X_pretrain.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab2f7f-7bff-4978-8ed0-c3364c4aff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from connexion_generation.utility import TwoDimArrayWrapper\n",
    "\n",
    "# We create an array of the same shape as X_pretrain_band but with the same time serie repeated K times\n",
    "X_pretrain_multi = TwoDimArrayWrapper(np.repeat(filtered_data, K, axis=0)) # filtered_data_noisy or filtered_data\n",
    "X_pretrain_multi_noisy = TwoDimArrayWrapper(np.repeat(filtered_data_noisy, K, axis=0)) # filtered_data_noisy or filtered_data\n",
    "\n",
    "if not is_multivariate:\n",
    "    X_pretrain_uni = X_pretrain.flatten() # X_pretrain_noisy or X_pretrain\n",
    "    X_pretrain_uni_noisy = X_pretrain_noisy.flatten() # X_pretrain_noisy or X_pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a8a9b-057b-4245-a177-f8dabc3dd101",
   "metadata": {},
   "source": [
    "### For classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837508d1-26fc-4c75-aba5-f77fd6defa49",
   "metadata": {},
   "source": [
    "#### Add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d6b31-52cb-4982-a5dc-50dcdd755221",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_instances_classification:\n",
    "    # We give Xtrain_band and Xtest_band the same shape as the expected input of the reservoir\n",
    "    #TRAIN \n",
    "    X_train_band_noisy = []\n",
    "    for instance in tqdm(X_train_band):\n",
    "        # Add noise to the time series\n",
    "        X_train_band_noisy.append(instance + np.random.normal(0, noise_std, instance.shape))\n",
    "\n",
    "    #TEST\n",
    "    X_test_band_noisy = []\n",
    "    for instance in tqdm(X_test_band):\n",
    "        # Add noise to the time series\n",
    "        X_test_band_noisy.append(instance + np.random.normal(0, noise_std, instance.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f1334-9d1d-45ff-b4c8-b552db53926e",
   "metadata": {},
   "source": [
    "#### Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6177f80-2c09-41c9-bab1-80821387540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_instances_classification:\n",
    "    X_train_band_duplicated = []\n",
    "    X_train_band_noisy_duplicated = []\n",
    "    if common_xtrain_index == 1:\n",
    "        for i in tqdm(range(len(X_train_band))):\n",
    "            X_train_band_duplicated.append(np.repeat(X_train_band[i], K, axis=1))\n",
    "            X_train_band_noisy_duplicated.append(np.repeat(X_train_band_noisy[i], K, axis=1))\n",
    "    else:\n",
    "        raise ValueError(\"The data formating is not correct.\")\n",
    "\n",
    "    \n",
    "    X_test_band_duplicated = []\n",
    "    X_test_band_noisy_duplicated = []\n",
    "    if common_xtrain_index == 1:\n",
    "        for i in tqdm(range(len(X_test_band))):\n",
    "            X_test_band_duplicated.append(np.repeat(X_test_band[i], K, axis=1))\n",
    "            X_test_band_noisy_duplicated.append(np.repeat(X_test_band_noisy[i], K, axis=1))\n",
    "    else:\n",
    "        raise ValueError(\"The data formating is not correct.\")\n",
    "    X_test_band, X_test_band_noisy, X_train_band, X_train_band_noisy = None, None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad44618-5b86-4650-838c-78a361836ec1",
   "metadata": {},
   "source": [
    "### For prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab6ec6-a247-4c9f-a8d4-d4f51d2aab2e",
   "metadata": {},
   "source": [
    "#### Add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533de743-2235-4570-9f11-253751658424",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_instances_classification:\n",
    "    # UNI\n",
    "    if not is_multivariate:\n",
    "        X_train_noisy = X_train + np.random.normal(0, noise_std, X_train.shape)\n",
    "        X_test_noisy = X_test + np.random.normal(0, noise_std, X_test.shape)\n",
    "\n",
    "    # MULTI\n",
    "    X_train_band_noisy = []\n",
    "    for ts in X_train_band:\n",
    "        # Add noise to the time series\n",
    "        X_train_band_noisy.append(ts + np.random.normal(loc=0, scale=noise_std, size=ts.shape))\n",
    "    X_train_band_noisy = np.array(X_train_band_noisy)\n",
    "    \n",
    "    X_test_band_noisy = []\n",
    "    for ts in X_test_band:\n",
    "        # Add noise to the time series\n",
    "        X_test_band_noisy.append(ts + np.random.normal(loc=0, scale=noise_std, size=ts.shape))\n",
    "    X_test_band_noisy = np.array(X_test_band_noisy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f2832e-4f34-4e96-9399-0ad3b26acadc",
   "metadata": {},
   "source": [
    "#### Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713ecea-6786-4e21-90bd-61fc3ebdf592",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_instances_classification: #if prediction\n",
    "    X_train_band_duplicated = np.repeat(np.squeeze(np.array(X_train_band)), K, axis=1)\n",
    "    X_test_band_duplicated = np.repeat(np.squeeze(np.array(X_test_band)), K, axis=1)\n",
    "    X_train_band_noisy_duplicated = np.repeat(np.squeeze(np.array(X_train_band_noisy)), K, axis=1)\n",
    "    X_test_band_noisy_duplicated = np.repeat(np.squeeze(np.array(X_test_band_noisy)), K, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df206ba-36a2-4468-b719-7bc819114b1a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c5610-8dad-4d05-8bbb-fa34031d9c9c",
   "metadata": {},
   "source": [
    "## Generated matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4957e6c-e0cc-4d90-a495-348a8bbab3cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import init_and_train_model_for_classification, predict_model_for_classification, compute_score\n",
    "from joblib import Parallel, delayed\n",
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.bounded_hadsp import run_hadsp_algorithm\n",
    "from connexion_generation.utility import TwoDimArrayWrapper\n",
    "from performances.esn_model_evaluation import init_and_train_model_for_prediction\n",
    "from connexion_generation.desp import run_desp_algorithm\n",
    "import connexion_generation.desp\n",
    "reload(connexion_generation.desp)\n",
    "\n",
    "N_JOBS = -1\n",
    "\n",
    "# TO CALCULATE SCORE FOR PREDICTION\n",
    "START_STEP = 0\n",
    "END_STEP = 500\n",
    "slice_range = slice(START_STEP, END_STEP)\n",
    "\n",
    "function_name = \"hadsp\" # \"desp\" ou \"hadsp\"\n",
    "data_type = \"noisy\" # \"normal\" ou \"noisy\"\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest values for the parameters you want to optimize\n",
    "    # COMMON\n",
    "    input_scaling = trial.suggest_float('input_scaling', 0.01, 0.2, step=0.01)\n",
    "    bias_scaling = trial.suggest_float('bias_scaling', 0, 0.2, step=0.01)\n",
    "    leaky_rate = trial.suggest_float('leaky_rate', 1, 1)\n",
    "    connectivity = trial.suggest_float('connectivity', 0, 0)\n",
    "    input_connectivity = trial.suggest_float('input_connectivity', 1, 1)\n",
    "    network_size = trial.suggest_float('network_size', RESERVOIR_SIZE, RESERVOIR_SIZE)\n",
    "    weight_increment = trial.suggest_float('weight_increment', 0.01, 0.5, step=0.01)\n",
    "\n",
    "    ridge = trial.suggest_int('ridge', -10, 1)\n",
    "    RIDGE_COEF = 10**ridge\n",
    "\n",
    "    # HADSP\n",
    "    if function_name == \"hadsp\":\n",
    "        target_rate = trial.suggest_float('target_rate', 0.5, 1, step=0.01)\n",
    "        rate_spread = trial.suggest_float('rate_spread', 0.01, 0.4, step=0.01)\n",
    "        TIME_INCREMENT = int(min_window_size+1) # int(min_window_size+1) or int(max_window_size)\n",
    "        MAX_TIME_INCREMENT = int(max_window_size) #int(max_window_size) or None or TIME_INCREMENT\n",
    "    # DESP\n",
    "    else:\n",
    "        min_variance = trial.suggest_float('min_variance', 0.001, 0.01, step=0.001)\n",
    "        variance_window = trial.suggest_float('variance_window', 0.01, 0.02, step=0.005)\n",
    "        max_variance = min_variance + variance_window\n",
    "        TIME_INCREMENT = 100 # int(min_window_size+1) or int(max_window_size)\n",
    "        MAX_TIME_INCREMENT = TIME_INCREMENT #int(max_window_size) or None or TIME_INCREMENT\n",
    "\n",
    "    pretrain_data_multi = X_pretrain_multi\n",
    "    train_data_multi = X_train_band_duplicated # X_train_band_noisy_duplicated or X_train_band_duplicated\n",
    "    test_data_multi = X_test_band_noisy_duplicated if data_type == \"noisy\" else X_test_band_duplicated\n",
    "\n",
    "\n",
    "    \n",
    "    def initialise_and_train(input_scaling, n, input_connectivity, connectivity, bias_scaling, training_set, visualize=False):\n",
    "        Win, W, bias = init_matrices(n, input_connectivity, connectivity)\n",
    "        bias *= bias_scaling\n",
    "        Win *= input_scaling\n",
    "\n",
    "        if function_name == \"hadsp\":\n",
    "            W, state_history = run_hadsp_algorithm(W, Win, bias, leaky_rate, activation_function, training_set, TIME_INCREMENT, weight_increment,\n",
    "                                    target_rate, rate_spread, max_increment=MAX_TIME_INCREMENT, mi_based=False, visualize=visualize)\n",
    "        elif function_name == \"desp\":\n",
    "            W, state_history, _ = run_desp_algorithm(W, Win, bias, leaky_rate, activation_function, training_set, TIME_INCREMENT, weight_increment,\n",
    "                        min_variance, max_variance, max_increment=MAX_TIME_INCREMENT, mi_based=True, n_jobs = 1, visualize=visualize)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid function: {function}\")\n",
    "        \n",
    "        return Win, W, bias\n",
    "\n",
    "    total_score = 0\n",
    "    average_nb = 3\n",
    "    for _ in range(average_nb):  # Repeat the process three times\n",
    "        # HADSP + multi\n",
    "        (Win_hadsp_multi,\n",
    "         W_hadsp_multi,\n",
    "         bias_hadsp_multi,\n",
    "         ) = initialise_and_train(input_scaling, n, input_connectivity, connectivity, bias_scaling, pretrain_data_multi)\n",
    "\n",
    "        if is_instances_classification:\n",
    "            reservoir_hadsp_multi, readout_hadsp_multi = init_and_train_model_for_classification(W_hadsp_multi, np.diag(Win_hadsp_multi.A.T[0]), bias_hadsp_multi, leaky_rate, activation_function, train_data_multi, Y_train, N_JOBS, RIDGE_COEF, mode=\"sequence-to-vector\")\n",
    "            Y_pred = predict_model_for_classification(reservoir_hadsp_multi, readout_hadsp_multi, test_data_multi, N_JOBS)\n",
    "            score = compute_score(Y_pred, Y_test, is_instances_classification)\n",
    "        else:\n",
    "            esn_hadsp_multi = init_and_train_model_for_prediction(W_hadsp_multi, np.diag(Win_hadsp_multi.A.T[0]), bias_hadsp_multi, leaky_rate, activation_function, train_data_multi, Y_train, RIDGE_COEF)\n",
    "            Y_pred =  esn_hadsp_multi.run(test_data_multi, reset=False)\n",
    "        \n",
    "            score = compute_score(Y_pred, Y_test, is_instances_classification)\n",
    "\n",
    "        total_score += score\n",
    "\n",
    "    average_score = total_score / average_nb  # Average the score\n",
    "\n",
    "    return average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7cbad-31bd-49d2-a3c1-00d9ccb6aaab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import re\n",
    "\n",
    "def camel_to_snake(name):\n",
    "    str1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', str1).lower()\n",
    "\n",
    "storage = optuna.storages.RDBStorage(\n",
    "    url=\"sqlite:///optuna_\" + camel_to_snake(dataset_name) + \"_db.sqlite3\",\n",
    "    engine_kwargs={\"pool_size\": 20, \"connect_args\": {\"timeout\": 10}},\n",
    ")\n",
    "study_name = function_name + \"_\" + dataset_name + \"_\" + data_type\n",
    "direction = \"maximize\" if is_instances_classification else \"minimize\"\n",
    "sampler = TPESampler()\n",
    "\n",
    "def optimize_study(n_trials):\n",
    "    study = optuna.create_study(storage=storage, sampler=sampler, study_name=study_name, direction=direction, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "N_TRIALS = 400\n",
    "n_jobs = 10\n",
    "trials_per_process = N_TRIALS // n_jobs\n",
    "\n",
    "# Use joblib to parallelize the optimization\n",
    "Parallel(n_jobs=n_jobs)(\n",
    "    delayed(optimize_study)(trials_per_process) for _ in range(n_jobs)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce8fd1-b2e9-4e55-83d6-4c237fae652b",
   "metadata": {},
   "source": [
    "## Random matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5439cca-5b2b-4a62-883f-7b10f7351e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import init_and_train_model_for_classification, predict_model_for_classification, compute_score\n",
    "from joblib import Parallel, delayed\n",
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.bounded_hadsp import run_hadsp_algorithm\n",
    "from connexion_generation.utility import TwoDimArrayWrapper\n",
    "from performances.esn_model_evaluation import init_and_train_model_for_prediction\n",
    "\n",
    "N_JOBS = -1\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest values for the parameters you want to optimize\n",
    "    input_scaling = trial.suggest_float('input_scaling', 0.01, 1.0, step=0.01)\n",
    "    bias_scaling = trial.suggest_float('bias_scaling', 0, 1, step=0.05)\n",
    "    leaky_rate = trial.suggest_float('leaky_rate', 1, 1)\n",
    "    connectivity = trial.suggest_float('connectivity', 0, 1)\n",
    "    input_connectivity = trial.suggest_float('input_connectivity', 1, 1)\n",
    "    network_size = trial.suggest_float('network_size', RESERVOIR_SIZE, RESERVOIR_SIZE)\n",
    "    sr = trial.suggest_float('spectral_radius', 0.4, 1.6, step=0.01)\n",
    "    ridge = trial.suggest_int('ridge', -10, 1)\n",
    "    RIDGE_COEF = 10**ridge\n",
    "\n",
    "\n",
    "    pretrain_data_multi = X_pretrain_multi\n",
    "    train_data_multi = X_train_band_duplicated # X_train_band_noisy_duplicated or X_train_band_duplicated\n",
    "    test_data_multi = X_test_band_noisy_duplicated if data_type == \"noisy\" else X_test_band_duplicated\n",
    "\n",
    "    \n",
    "    total_score = 0\n",
    "    average_nb=3\n",
    "    for _ in range(average_nb):  # Repeat the process three times\n",
    "        \n",
    "        # random + multi\n",
    "        Win_random_multi, W_random_multi, bias_random_multi =  init_matrices(n, 1, connectivity, sr)\n",
    "        bias_random_multi= bias_random_multi*bias_scaling\n",
    "        Win_random_multi= Win_random_multi*input_scaling    \n",
    "\n",
    "        if is_instances_classification:\n",
    "            reservoir_random_multi, readout_random_multi = init_and_train_model_for_classification(W_random_multi, np.diag(Win_random_multi.A.T[0]), bias_random_multi, leaky_rate, activation_function, train_data_multi, Y_train, N_JOBS, RIDGE_COEF, mode=\"sequence-to-vector\")\n",
    "            Y_pred = predict_model_for_classification(reservoir_random_multi, readout_random_multi, test_data_multi, N_JOBS)\n",
    "            score = compute_score(Y_pred, Y_test, is_instances_classification)\n",
    "        else:\n",
    "            esn_random_multi = init_and_train_model_for_prediction(W_random_multi, np.diag(Win_random_multi.A.T[0]), bias_random_multi, leaky_rate, activation_function, train_data_multi, Y_train, RIDGE_COEF)\n",
    "            Y_pred =  esn_random_multi.run(test_data_multi, reset=False)\n",
    "        \n",
    "            score = compute_score(Y_pred, Y_test, is_instances_classification)\n",
    "\n",
    "        total_score += score\n",
    "        \n",
    "    average_score = total_score / average_nb  # Average the score\n",
    "\n",
    "    \n",
    "    return average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8853a-8c8d-4eec-bb2e-fc66e2091eb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import re\n",
    "\n",
    "def camel_to_snake(name):\n",
    "    str1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', str1).lower()\n",
    "\n",
    "storage = optuna.storages.RDBStorage(\n",
    "    url=\"sqlite:///optuna_\" + camel_to_snake(dataset_name) + \"_db.sqlite3\",\n",
    "    engine_kwargs={\"pool_size\": 20, \"connect_args\": {\"timeout\": 10}},\n",
    ")\n",
    "study_name = \"random_\" + dataset_name + \"_\" + data_type\n",
    "direction = \"maximize\" if is_instances_classification else \"minimize\"\n",
    "sampler = TPESampler()\n",
    "\n",
    "\n",
    "def optimize_study(n_trials):\n",
    "    study = optuna.create_study(storage, sampler, study_name=study_name, direction=direction, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "\n",
    "N_TRIALS = 400\n",
    "n_jobs = 10\n",
    "trials_per_process = N_TRIALS // n_jobs\n",
    "\n",
    "# Use joblib to parallelize the optimization\n",
    "Parallel(n_jobs=n_jobs)(\n",
    "    delayed(optimize_study)(trials_per_process) for _ in range(n_jobs)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea5f7c-4fda-4977-b27a-be04731e9677",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355b9b7-6625-47be-8a7e-8d388448e85b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Mackey Glass\n",
    "### Ridge parameter\n",
    "| Dataset     | Algorithm | test   | Curve shape | Best value |\n",
    "|-------------|-----------|--------|-------------|------------|\n",
    "| MackeyGlass | DESP      | normal | no extremum | 10         |\n",
    "| MackeyGlass | HADSP     | normal | no extremum | 9,10       |\n",
    "| MackeyGlass | random    | normal | no extremum | 9,10       |\n",
    "| MackeyGlass | DESP      | noisy  | bell        | 7          |\n",
    "| MackeyGlass | HADSP     | noisy  | bell        | 6          |\n",
    "| MackeyGlass | random    | noisy  | no extremum | 10         |\n",
    "\n",
    "### Results, no validation/test\n",
    "\n",
    "|           |  HADSP |  DESP  | random |\n",
    "|-----------|--------|--------|--------|\n",
    "|  normal   | 0.0399 | 0.0379 | 0.0446 |\n",
    "|  noisy    | 0.119  | 0.109  | 0.104  | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cf04a0-7ee8-44fd-a8e0-cbf632a9a2c9",
   "metadata": {},
   "source": [
    "## Japanese vowels\n",
    "\n",
    "### Ridge parameter\n",
    "\n",
    "| Dataset        | Algorithm | test   | Curve shape | Best value |\n",
    "|----------------|-----------|--------|-------------|------------|\n",
    "| JapaneseVowels | DESP      | normal | bell        | 2          |\n",
    "| JapaneseVowels | HADSP     | normal | bell        | 4          |\n",
    "| JapaneseVowels | random    | normal | bell        | 4          |\n",
    "| JapaneseVowels | DESP      | noisy  | bell        | 2          |\n",
    "| JapaneseVowels | HADSP     | noisy  | bell        | 5          |\n",
    "| JapaneseVowels | random    | noisy  | bell        | 5          |\n",
    "\n",
    "### Results, no validation/test\n",
    "\n",
    "|           |  HADSP |  DESP  | random |\n",
    "|-----------|--------|--------|--------|\n",
    "|  normal   | 0.641  | 0.649  | 0.612  |\n",
    "|  noisy    | 0.621  | 0.623  | 0.610  | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee3d4e-e3c6-43fd-9ef8-cf41c858ebac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hadsp_env",
   "language": "python",
   "name": "hadsp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
