{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c1d64",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the root project folder to the python path in order to use the packages\n",
    "#path_root = Path( '/project_ghent/HADSP/hadsp/')\n",
    "#sys.path.append(str(path_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f790cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:04:18.164863Z",
     "start_time": "2023-10-23T14:04:18.160678Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# SEED\n",
    "SEED = 49387\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from seaborn import heatmap, color_palette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44384e-7cbc-4611-a015-e1c073f61aa9",
   "metadata": {},
   "source": [
    "# Datasets loading\n",
    "\n",
    "Lots of different on availabale : https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad\n",
    "\n",
    "Classification: \n",
    "https://arxiv.org/abs/1803.07870\n",
    "\n",
    "https://github.com/FilippoMB/Time-series-classification-and-clustering-with-Reservoir-Computing\n",
    "\n",
    "Multivariate:\n",
    "https://www.timeseriesclassification.com/dataset.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb81c7-9923-4fee-a6ed-98a84b3b7b6f",
   "metadata": {},
   "source": [
    "## Torchaudio\n",
    "\n",
    "https://pytorch.org/audio/stable/datasets.html\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb76656a-1524-43db-b431-e80fcee53dc5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# load dataset using torchaudio\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torchaudio.datasets import VoxCeleb1Identification, SPEECHCOMMANDS\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "dataset = SPEECHCOMMANDS(root=\"datasets/\", download=True)\n",
    "\n",
    "sampling_rate = dataset[0][1]\n",
    "X = [sample[0][0] for sample in dataset]\n",
    "Y = [sample[2] for sample in dataset]\n",
    "\n",
    "dataset_size = len(dataset)  # Total number of samples in the dataset\n",
    "\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "\n",
    "# Use StratifiedShuffleSplit to get train/test indices\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "train_index, test_index = next(sss.split(X, Y))\n",
    "\n",
    "# Split data and labels using the indices\n",
    "X_train = X[train_index]\n",
    "Y_train = Y[train_index]\n",
    "X_test = X[test_index]\n",
    "Y_test = Y[test_index]\n",
    "\n",
    "\n",
    "is_multivariate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d6272-708d-4709-8e35-5a84268bed64",
   "metadata": {},
   "source": [
    "## Prediction ahead\n",
    "\n",
    "Datasets available :\n",
    "\n",
    "* MackeyGlass\n",
    "* Lorenz"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bee95f6c-898c-4136-97f0-887e0a8efb72",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from datasets.load_datasets import load_dataset_prediction\n",
    "is_instances_classification = False\n",
    "dataset_name = \"MackeyGlass\"\n",
    "step_ahead=5\n",
    "\n",
    "is_multivariate, sampling_rate, X_train_raw, X_test_raw, Y_train_raw, Y_test = load_dataset_prediction(dataset_name, step_ahead, visualize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b219b-eb64-4715-b983-7de3c392f088",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Classification\n",
    "\n",
    "Datasets available :\n",
    "\n",
    "* FSDD\n",
    "* HAART\n",
    "* JapaneseVowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95586d8c-0525-4b8c-9937-34226691ce63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets.load_datasets import load_dataset_classification\n",
    "is_instances_classification = True\n",
    "dataset_name = \"FSDD\"\n",
    "\n",
    "is_multivariate, sampling_rate, X_train_raw, X_test_raw, Y_train_raw, Y_test, groups = load_dataset_classification(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86961904-ad36-4af8-ad21-6d51f491da61",
   "metadata": {},
   "source": [
    "# Reservoir parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed459f0-7f4b-40e9-b32c-441769100be2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a38d2df41a727",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from reservoir.activation_functions import tanh, heaviside, sigmoid\n",
    "\n",
    "# the activation function choosen for the rest of the experiment\n",
    "# activation_function = lambda x : sigmoid(2*(x-0.5))tanh(x)\n",
    "activation_function = lambda x : tanh(x)\n",
    "\n",
    "plt.plot(np.linspace(0, 1.1, 100), activation_function(np.linspace(0, 1.1, 100)))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df206ba-36a2-4468-b719-7bc819114b1a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f892fb1cccd7511",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca76d0e06f936c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math \n",
    " \n",
    "# Cross validation\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit, StratifiedGroupKFold\n",
    "from datasets.preprocessing import flexible_indexing\n",
    "\n",
    "#Preprocessing\n",
    "from datasets.multivariate_generation import generate_multivariate_dataset, extract_peak_frequencies\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets.preprocessing import scale_data\n",
    "from datasets.preprocessing import add_noise, duplicate_data\n",
    "\n",
    "# Define noise parameter\n",
    "noise_std = 0.001\n",
    "\n",
    "nb_splits=3\n",
    "if is_instances_classification:\n",
    "    if groups is None:\n",
    "        splits = StratifiedKFold(n_splits=nb_splits, shuffle=True).split(X_train_raw, np.argmax(Y_train_raw, axis=1))\n",
    "    else:\n",
    "        splits = StratifiedGroupKFold(n_splits=nb_splits, shuffle=True).split(X_train_raw, np.argmax(Y_train_raw, axis=1), groups)\n",
    "else: #prediction\n",
    "    splits = TimeSeriesSplit(n_splits=nb_splits).split(X_train_raw)\n",
    "\n",
    "X_pretrain = []\n",
    "X_pretrain_noisy  = []\n",
    "X_train = []\n",
    "X_train_noisy = []\n",
    "X_val = []\n",
    "X_val_noisy = []\n",
    "X_pretrain_band = []\n",
    "X_pretrain_band_noisy = []\n",
    "X_train_band = []\n",
    "X_train_band_noisy = []\n",
    "X_val_band = []\n",
    "X_val_band_noisy = []\n",
    "\n",
    "Y_train = []\n",
    "Y_val = []\n",
    "\n",
    "WINDOW_LENGTH = 10\n",
    "    \n",
    "for i, (train_index, val_index) in enumerate(splits):\n",
    "    x_train = flexible_indexing(X_train_raw, train_index)\n",
    "    x_val = flexible_indexing(X_train_raw, val_index)\n",
    "    Y_train.append(flexible_indexing(Y_train_raw, train_index))\n",
    "    Y_val.append(flexible_indexing(Y_train_raw, val_index))\n",
    "    # SPLITS\n",
    "    if is_multivariate:\n",
    "        x_train_band, x_val_band = x_train, x_val\n",
    "        del x_train, x_val\n",
    "\n",
    "        \n",
    "    # PREPROCESSING        \n",
    "    freq_train_data = x_train_band if is_multivariate else x_train\n",
    "    flat_train_data = np.concatenate(freq_train_data, axis=0) if is_instances_classification else freq_train_data\n",
    "    extract_peak_frequencies(flat_train_data, sampling_rate, smooth=False, threshold=1e-5, nperseg=1024, visualize=False)\n",
    "    filtered_peak_freqs = extract_peak_frequencies(flat_train_data, sampling_rate, smooth=True, window_length=WINDOW_LENGTH, threshold=1e-5, nperseg=1024, visualize=False)\n",
    "    \n",
    "    if not is_multivariate:\n",
    "        common_size = len(filtered_peak_freqs)\n",
    "        x_train_band = generate_multivariate_dataset(\n",
    "            filtered_peak_freqs, x_train, sampling_rate, is_instances_classification, nb_jobs=-1\n",
    "        )\n",
    "        x_val_band = generate_multivariate_dataset(\n",
    "            filtered_peak_freqs, x_val, sampling_rate, is_instances_classification, nb_jobs=-1\n",
    "        )\n",
    "\n",
    "    if not is_multivariate:\n",
    "        scaler_x_uni = MinMaxScaler(feature_range=(0, 1))\n",
    "        x_train, x_val, _ = scale_data(x_train, x_val, None, scaler_x_uni, is_instances_classification)       \n",
    "        X_train.append(x_train)\n",
    "        X_val.append(x_val)\n",
    "\n",
    "    scaler_multi = MinMaxScaler(feature_range=(0, 1))\n",
    "    x_train_band, x_val_band, _ = scale_data(x_train_band, x_val_band, None, scaler_multi, is_instances_classification)\n",
    "    X_train_band.append(x_train_band)\n",
    "    X_val_band.append(x_val_band)\n",
    "             \n",
    "    # PRETRAIN NOISE\n",
    "    # UNI\n",
    "    if not is_multivariate:\n",
    "        X_pretrain.append(x_train.flatten())\n",
    "        X_pretrain_noisy.append((add_noise(x_train, noise_std)).flatten())\n",
    "    \n",
    "    # MULTI\n",
    "    X_pretrain_band.append(x_train_band)\n",
    "    X_pretrain_band_noisy.append([add_noise(instance, noise_std) for instance in X_pretrain_band[i]])\n",
    "    \n",
    "    #Train/Val/Test\n",
    "    if is_instances_classification:\n",
    "        # NOISE\n",
    "        # UNI\n",
    "        if not is_multivariate:\n",
    "            X_train_noisy.append([add_noise(instance, noise_std) for instance in x_train])\n",
    "            X_val_noisy.append([add_noise(instance, noise_std) for instance in x_val])\n",
    "            \n",
    "        # MULTI\n",
    "        X_train_band_noisy.append([add_noise(instance, noise_std) for instance in x_train_band])\n",
    "        X_val_band_noisy.append([add_noise(instance, noise_std) for instance in x_val_band])\n",
    "    \n",
    "    else:  #if prediction\n",
    "        # NOISE\n",
    "        # UNI\n",
    "        if not is_multivariate:\n",
    "            X_train_noisy.append(add_noise(x_train, noise_std))\n",
    "            X_val_noisy.append(add_noise(x_val, noise_std))\n",
    "    \n",
    "        # MULTI\n",
    "        X_train_band_noisy.append(add_noise(x_train_band, noise_std))\n",
    "        X_val_band_noisy.append(add_noise(x_val_band, noise_std))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c5610-8dad-4d05-8bbb-fa34031d9c9c",
   "metadata": {},
   "source": [
    "## Generated matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4957e6c-e0cc-4d90-a495-348a8bbab3cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "#Pretraining\n",
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.bounded_hadsp import run_hadsp_algorithm\n",
    "from connexion_generation.desp import run_desp_algorithm\n",
    "\n",
    "# Evaluating\n",
    "from performances.esn_model_evaluation import init_and_train_model_for_classification, predict_model_for_classification, compute_score\n",
    "from performances.esn_model_evaluation import init_and_train_model_for_prediction\n",
    "\n",
    "\n",
    "# score for prediction\n",
    "start_step = 30\n",
    "end_step = 500\n",
    "SLICE_RANGE = slice(start_step, end_step)\n",
    "RESERVOIR_SIZE = 500\n",
    "\n",
    "function_name = \"hadsp\" # \"desp\" ou \"hadsp\"\n",
    "data_type = \"normal\" # \"normal\" ou \"noisy\"\n",
    "variate_type = \"multi\" # \"multi\" ou \"uni\"\n",
    "if variate_type == \"uni\" and is_multivariate:\n",
    "    raise ValueError(f\"Invalid variable type: {variate_type}\")\n",
    "    \n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest values for the parameters you want to optimize\n",
    "    # COMMON\n",
    "    input_scaling = trial.suggest_float('input_scaling', 0.01, 0.2, step=0.005)\n",
    "    bias_scaling = trial.suggest_float('bias_scaling', 0, 0.2, step=0.01)\n",
    "    leaky_rate = trial.suggest_float('leaky_rate', 1, 1)\n",
    "    connectivity = trial.suggest_float('connectivity', 0, 0)\n",
    "    input_connectivity = trial.suggest_float('input_connectivity', 1, 1)\n",
    "    network_size = trial.suggest_float('network_size', RESERVOIR_SIZE, RESERVOIR_SIZE)\n",
    "    weight_increment = trial.suggest_float('weight_increment', 0.01, 0.5, step=0.01)\n",
    "\n",
    "    ridge = trial.suggest_int('ridge', -10, 1)\n",
    "    RIDGE_COEF = 10**ridge\n",
    "\n",
    "    # HADSP\n",
    "    if function_name == \"hadsp\":\n",
    "        target_rate = trial.suggest_float('target_rate', 0.5, 1, step=0.01)\n",
    "        rate_spread = trial.suggest_float('rate_spread', 0.01, 0.4, step=0.005)\n",
    "    # DESP\n",
    "    else:\n",
    "        min_variance = trial.suggest_float('min_variance', 0.001, 0.02, step=0.001)\n",
    "        variance_window = trial.suggest_float('variance_window', 0.01, 0.05, step=0.002)\n",
    "        max_variance = min_variance + variance_window\n",
    "    \n",
    "\n",
    "    \n",
    "    # CROSS-VALIDATION METHODS\n",
    "    total_score = 0\n",
    "\n",
    "    for i in range(nb_splits):\n",
    "        if variate_type == \"multi\":\n",
    "            if is_instances_classification:\n",
    "                common_index = 1\n",
    "                common_size = X_train_band[i][0].shape[common_index]\n",
    "            else:\n",
    "                common_index = 1\n",
    "                common_size = X_train_band[i].shape[common_index]\n",
    "        else:\n",
    "            common_size = len(filtered_peak_freqs)\n",
    "            \n",
    "        # We want the size of the reservoir to be at least RESERVOIR_SIZE\n",
    "        K = math.ceil(RESERVOIR_SIZE / common_size)\n",
    "        n = common_size * K\n",
    "    \n",
    " \n",
    "        min_window_size = sampling_rate/np.max(np.hstack(filtered_peak_freqs))\n",
    "        max_window_size = sampling_rate/np.min(np.hstack(filtered_peak_freqs))\n",
    "        # HADSP\n",
    "        if function_name == \"hadsp\":\n",
    "            TIME_INCREMENT = int(min_window_size+1) # int(min_window_size+1) or int(max_window_size)\n",
    "            MAX_TIME_INCREMENT = int(max_window_size) #int(max_window_size) or None or TIME_INCREMENT\n",
    "        # DESP\n",
    "        else:\n",
    "            TIME_INCREMENT = 100 # int(min_window_size+1) or int(max_window_size)\n",
    "            MAX_TIME_INCREMENT = TIME_INCREMENT #int(max_window_size) or None or TIME_INCREMENT\n",
    "\n",
    "        pretrain_data = X_pretrain_band[i]\n",
    "        train_data = X_train_band[i]  # X_train_band_noisy_duplicated or X_train_band_duplicated\n",
    "        val_data = X_val_band_noisy[i] if data_type == \"noisy\" else X_val_band[i]\n",
    "\n",
    "        # UNSUPERVISED PRETRAINING \n",
    "        def initialise_and_train(input_scaling, n, input_connectivity, connectivity, K, bias_scaling, seed, training_set):\n",
    "            Win, W, bias = init_matrices(n, input_connectivity, connectivity,  K, seed=seed)\n",
    "            bias *= bias_scaling\n",
    "            Win *= input_scaling\n",
    "    \n",
    "            if function_name == \"hadsp\":\n",
    "                W, _ = run_hadsp_algorithm(W, Win, bias, leaky_rate, activation_function, training_set, TIME_INCREMENT, weight_increment,\n",
    "                                        target_rate, rate_spread, instances=is_instances_classification, max_increment=MAX_TIME_INCREMENT, mi_based=False)\n",
    "            elif function_name == \"desp\":\n",
    "                W, _, _ = run_desp_algorithm(W, Win, bias, leaky_rate, activation_function, training_set, TIME_INCREMENT, weight_increment,\n",
    "                                        min_variance, max_variance, instances=is_instances_classification, max_increment=MAX_TIME_INCREMENT, mi_based=True, n_jobs = -1)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid function: {function_name}\")\n",
    "            \n",
    "            return Win, W, bias\n",
    "            \n",
    "        Win, W, bias= initialise_and_train(input_scaling, n, input_connectivity, connectivity,  K, bias_scaling, SEED, pretrain_data)\n",
    "\n",
    "        # EVALUATION\n",
    "        if is_instances_classification:\n",
    "            reservoir, readout = init_and_train_model_for_classification(W, Win, bias, leaky_rate, activation_function, train_data, Y_train[i], 1, RIDGE_COEF, mode=\"sequence-to-vector\")\n",
    "            \n",
    "            Y_pred = predict_model_for_classification(reservoir, readout, val_data, 1)\n",
    "            score = compute_score(Y_pred, Y_val[i], is_instances_classification)\n",
    "        else:\n",
    "            esn = init_and_train_model_for_prediction(W, Win, bias, leaky_rate, activation_function, train_data, Y_train[i], RIDGE_COEF)\n",
    "            \n",
    "            Y_pred =  esn.run(val_data, reset=False)\n",
    "            score = compute_score(Y_pred, Y_val[i], is_instances_classification)\n",
    "\n",
    "        total_score += score\n",
    "\n",
    "    average_score = total_score / nb_splits  # Average the score\n",
    "\n",
    "    return average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7cbad-31bd-49d2-a3c1-00d9ccb6aaab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import re\n",
    "\n",
    "def camel_to_snake(name):\n",
    "    str1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', str1).lower()\n",
    "\n",
    "url= \"sqlite:///optuna_\" + camel_to_snake(dataset_name) + \"_db.sqlite3\"\n",
    "print(url)\n",
    "storage = optuna.storages.RDBStorage(\n",
    "    url=url,\n",
    "    engine_kwargs={\"pool_size\": 20, \"connect_args\": {\"timeout\": 10}},\n",
    ")\n",
    "study_name = function_name + \"_\" + dataset_name + \"_\" + data_type + \"_\" + variate_type\n",
    "print(study_name)\n",
    "direction = \"maximize\" if is_instances_classification else \"minimize\"\n",
    "sampler = TPESampler()\n",
    "\n",
    "def optimize_study(n_trials):\n",
    "    study = optuna.create_study(storage=storage, sampler=sampler, study_name=study_name, direction=direction, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "N_TRIALS = 400\n",
    "n_jobs = 10\n",
    "trials_per_process = N_TRIALS // n_jobs\n",
    "\n",
    "# Use joblib to parallelize the optimization\n",
    "Parallel(n_jobs=n_jobs)(\n",
    "    delayed(optimize_study)(trials_per_process) for _ in range(n_jobs)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce8fd1-b2e9-4e55-83d6-4c237fae652b",
   "metadata": {},
   "source": [
    "## Random matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5439cca-5b2b-4a62-883f-7b10f7351e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import math \n",
    "\n",
    "# Cross validation\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, TimeSeriesSplit, GroupShuffleSplit, ShuffleSplit\n",
    "from datasets.preprocessing import flexible_indexing\n",
    "\n",
    "#Preprocessing\n",
    "from datasets.multivariate_generation import generate_multivariate_dataset, extract_peak_frequencies\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets.preprocessing import scale_data\n",
    "from datasets.preprocessing import add_noise, duplicate_data\n",
    "\n",
    "# Define noise parameter\n",
    "noise_std = 0.001\n",
    "\n",
    "#Pretraining\n",
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.bounded_hadsp import run_hadsp_algorithm\n",
    "from connexion_generation.desp import run_desp_algorithm\n",
    "\n",
    "# Evaluating\n",
    "from performances.esn_model_evaluation import init_and_train_model_for_classification, predict_model_for_classification, compute_score\n",
    "from performances.esn_model_evaluation import init_and_train_model_for_prediction\n",
    "# score for prediction\n",
    "start_step = 30\n",
    "end_step = 500\n",
    "SLICE_RANGE = slice(start_step, end_step)\n",
    "\n",
    "data_type = \"noisy\" # \"normal\" ou \"noisy\"\n",
    "variate_type = \"multi\" # \"multi\" ou \"uni\" ou \"\" (if only multi)\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest values for the parameters you want to optimize\n",
    "    input_scaling = trial.suggest_float('input_scaling', 0.01, 1.0, step=0.01)\n",
    "    bias_scaling = trial.suggest_float('bias_scaling', 0, 1, step=0.05)\n",
    "    leaky_rate = trial.suggest_float('leaky_rate', 1, 1)\n",
    "    connectivity = trial.suggest_float('connectivity', 0, 1)\n",
    "    input_connectivity = trial.suggest_float('input_connectivity', 1, 1)\n",
    "    network_size = trial.suggest_float('network_size', RESERVOIR_SIZE, RESERVOIR_SIZE)\n",
    "    sr = trial.suggest_float('spectral_radius', 0.4, 1.6, step=0.01)\n",
    "    ridge = trial.suggest_int('ridge', -10, 1)\n",
    "    RIDGE_COEF = 10**ridge\n",
    "\n",
    "    if variate_type == \"multi\":\n",
    "        if is_instances_classification:\n",
    "            common_index = 1\n",
    "            common_size = X_train_raw[0].shape[common_index]\n",
    "        else:\n",
    "            common_index = 0\n",
    "            common_size = X_train_raw.shape[common_index]\n",
    "    else:\n",
    "        common_size = 1\n",
    "\n",
    "    # CROSS-VALIDATION METHODS\n",
    "    total_score = 0\n",
    "    for i in range(nb_splits):\n",
    "        if variate_type == \"multi\":\n",
    "            if is_instances_classification:\n",
    "                common_index = 1\n",
    "                common_size = X_train_band[i][0].shape[common_index]\n",
    "            else:\n",
    "                common_index = 1\n",
    "                common_size = X_train_band[i].shape[common_index]\n",
    "        else:\n",
    "            common_size = len(filtered_peak_freqs)\n",
    "            \n",
    "        # We want the size of the reservoir to be at least RESERVOIR_SIZE\n",
    "        K = math.ceil(RESERVOIR_SIZE / common_size)\n",
    "        n = common_size * K\n",
    "        \n",
    "        train_data = X_train_band[i]  # X_train_band_noisy_duplicated or X_train_band_duplicated\n",
    "        val_data = X_val_band_noisy[i] if data_type == \"noisy\" else X_val_band[i]\n",
    "        \n",
    "        # random + multi\n",
    "        Win_random, W_random, bias_random =  init_matrices(n, 1, connectivity,  K, sr)\n",
    "        bias_random= bias_random*bias_scaling\n",
    "        Win_random= Win_random*input_scaling    \n",
    "\n",
    "        if is_instances_classification:\n",
    "            reservoir_random, readout_random = init_and_train_model_for_classification(W_random, Win_random, bias_random, leaky_rate, activation_function, train_data, Y_train[i], 1, RIDGE_COEF, mode=\"sequence-to-vector\")\n",
    "            Y_pred = predict_model_for_classification(reservoir_random, readout_random, val_data, 1)\n",
    "            score = compute_score(Y_pred, Y_val[i], is_instances_classification)\n",
    "        else:\n",
    "            esn_random = init_and_train_model_for_prediction(W_random, Win_random, bias_random, leaky_rate, activation_function, train_data, Y_train[i], RIDGE_COEF)\n",
    "            Y_pred =  esn_random.run(val_data, reset=False)\n",
    "        \n",
    "            score = compute_score(Y_pred, Y_val[i], is_instances_classification)\n",
    "\n",
    "        total_score += score\n",
    "        \n",
    "    average_score = total_score / nb_splits  # Average the score\n",
    "\n",
    "    \n",
    "    return average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8853a-8c8d-4eec-bb2e-fc66e2091eb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import re\n",
    "\n",
    "def camel_to_snake(name):\n",
    "    str1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', str1).lower()\n",
    "\n",
    "url = \"sqlite:///optuna_\" + camel_to_snake(dataset_name) + \"_db.sqlite3\"\n",
    "print(url)\n",
    "storage = optuna.storages.RDBStorage(\n",
    "    url=url,\n",
    "    engine_kwargs={\"pool_size\": 20, \"connect_args\": {\"timeout\": 10}},\n",
    ")\n",
    "\n",
    "study_name = \"random_\" + dataset_name + \"_\" + data_type + \"_\" + variate_type\n",
    "print(study_name)\n",
    "direction = \"maximize\" if is_instances_classification else \"minimize\"\n",
    "sampler = TPESampler()\n",
    "\n",
    "\n",
    "def optimize_study(n_trials):\n",
    "    study = optuna.create_study(storage, sampler, study_name=study_name, direction=direction, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "\n",
    "N_TRIALS = 400\n",
    "n_jobs = 10\n",
    "trials_per_process = N_TRIALS // n_jobs\n",
    "\n",
    "# Use joblib to parallelize the optimization\n",
    "Parallel(n_jobs=n_jobs)(\n",
    "    delayed(optimize_study)(trials_per_process) for _ in range(n_jobs)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea5f7c-4fda-4977-b27a-be04731e9677",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355b9b7-6625-47be-8a7e-8d388448e85b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Mackey Glass\n",
    "### Ridge parameter\n",
    "| Dataset     | Algorithm | test   | Curve shape | Best value |\n",
    "|-------------|-----------|--------|-------------|------------|\n",
    "| MackeyGlass | DESP      | normal | no extremum | 10         |\n",
    "| MackeyGlass | HADSP     | normal | no extremum | 9,10       |\n",
    "| MackeyGlass | random    | normal | no extremum | 9,10       |\n",
    "| MackeyGlass | DESP      | noisy  | bell        | 7          |\n",
    "| MackeyGlass | HADSP     | noisy  | bell        | 6          |\n",
    "| MackeyGlass | random    | noisy  | no extremum | 10         |\n",
    "\n",
    "### Results, no validation/test\n",
    "\n",
    "|           |  HADSP |  DESP  | random |\n",
    "|-----------|--------|--------|--------|\n",
    "|  normal   | 0.0399 | 0.0379 | 0.0446 |\n",
    "|  noisy    | 0.119  | 0.109  | 0.104  | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cf04a0-7ee8-44fd-a8e0-cbf632a9a2c9",
   "metadata": {},
   "source": [
    "## Japanese vowels\n",
    "\n",
    "### Ridge parameter\n",
    "\n",
    "| Dataset        | Algorithm | test   | Curve shape | Best value |\n",
    "|----------------|-----------|--------|-------------|------------|\n",
    "| JapaneseVowels | DESP      | normal | bell        | 2          |\n",
    "| JapaneseVowels | HADSP     | normal | bell        | 4          |\n",
    "| JapaneseVowels | random    | normal | bell        | 4          |\n",
    "| JapaneseVowels | DESP      | noisy  | bell        | 2          |\n",
    "| JapaneseVowels | HADSP     | noisy  | bell        | 5          |\n",
    "| JapaneseVowels | random    | noisy  | bell        | 5          |\n",
    "\n",
    "### Results, no validation/test\n",
    "\n",
    "|           |  HADSP |  DESP  | random |\n",
    "|-----------|--------|--------|--------|\n",
    "|  normal   | 0.641  | 0.649  | 0.612  |\n",
    "|  noisy    | 0.621  | 0.623  | 0.610  | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee3d4e-e3c6-43fd-9ef8-cf41c858ebac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hadsp_env",
   "language": "python",
   "name": "hadsp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
