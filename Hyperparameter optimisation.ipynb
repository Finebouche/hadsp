{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c1d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:04:18.159981Z",
     "start_time": "2023-10-23T14:04:18.158428Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the root project folder to the python path in order to use the packages\n",
    "path_root = Path( '/project_ghent/HADSP/hadsp/')\n",
    "sys.path.append(str(path_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f790cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:04:18.164863Z",
     "start_time": "2023-10-23T14:04:18.160678Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from importlib import reload\n",
    "\n",
    "# SEED\n",
    "SEED = 49387\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from seaborn import heatmap, color_palette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44384e-7cbc-4611-a015-e1c073f61aa9",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "Lots of different on availabale : https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad\n",
    "\n",
    "Classification: \n",
    "https://arxiv.org/abs/1803.07870\n",
    "\n",
    "https://github.com/FilippoMB/Time-series-classification-and-clustering-with-Reservoir-Computing\n",
    "\n",
    "Multivariate:\n",
    "https://www.timeseriesclassification.com/dataset.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0c7e80efd7c0f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### HAART \n",
    "https://www.cs.ubc.ca/labs/spin/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b97065ff9fe705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T16:23:04.606497Z",
     "start_time": "2023-10-23T16:23:03.459264Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Charge HAART dataset from https://www.cs.ubc.ca/labs/spin/data/HAART%20DataSet.zip if it's not already done\n",
    "# download and unzip the dataset\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists('datasets/HAART DataSet'):\n",
    "    urllib.request.urlretrieve('https://www.cs.ubc.ca/labs/spin/data/HAART%20DataSet.zip', 'datasets/HAART DataSet.zip')\n",
    "    # unzip the dataset in \"datasets/HAART DataSet\" folder\n",
    "    with zipfile.ZipFile('datasets/HAART DataSet.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('datasets/HAART DataSet')\n",
    "            \n",
    "    # delete zip\n",
    "    os.remove('datasets/HAART DataSet.zip')\n",
    "\n",
    "import datasets.load_datasets\n",
    "reload(datasets.load_datasets)\n",
    "from datasets.load_datasets import load_haart_dataset\n",
    "\n",
    "sampling_rate, X_train_band, Y_train, X_test_band, Y_test = load_haart_dataset(\"datasets/HAART DataSet/training.csv\", \"datasets/HAART DataSet/testWITHLABELS.csv\")\n",
    "\n",
    "X_pretrain_band = np.concatenate(X_train_band[200:], axis=0).T\n",
    "\n",
    "is_multivariate = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468ca5c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Multivariate generation if necessary\n",
    "\n",
    "Spectrograms_vs_Cochleagrams : https://www.researchgate.net/publication/340510607_Speech_recognition_using_very_deep_neural_networks_Spectrograms_vs_Cochleagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e57e07-e48c-4e28-80e9-ad95894d0a2e",
   "metadata": {},
   "source": [
    "### Spectral density and peak selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ad0f4-4dbc-4b97-8d28-8ffb357715d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:11:18.377724Z",
     "start_time": "2023-10-20T11:11:18.143305Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets.multivariate_generation import extract_peak_frequencies\n",
    "\n",
    "if is_multivariate:\n",
    "    filtered_peak_freqs = extract_peak_frequencies(X_pretrain_band[0].flatten(), sampling_rate, threshold=1e-5, nperseg=1024, visualize=True)\n",
    "else:\n",
    "    filtered_peak_freqs = extract_peak_frequencies(X_pretrain.flatten(), sampling_rate, threshold=1e-5, nperseg=1024, visualize=True)\n",
    "\n",
    "print(\"Number of frequencies selected :\", len(filtered_peak_freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766304bc-3c47-4813-9828-91d8fed5845b",
   "metadata": {},
   "source": [
    "### Applying normal band pass filter on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690a984-394f-4be3-912c-ee2ca4cc1ba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:11:39.227368Z",
     "start_time": "2023-10-20T11:11:19.130705Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets.multivariate_generation \n",
    "reload(datasets.multivariate_generation)\n",
    "from datasets.multivariate_generation import generate_multivariate_dataset, extract_peak_frequencies\n",
    "\n",
    "if not is_multivariate:\n",
    "    X_pretrain_band, X_train_band, X_test_band = generate_multivariate_dataset(\n",
    "        filtered_peak_freqs, X_pretrain, X_train, X_test, sampling_rate, nb_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b859e5-c6dd-4086-ac56-42a7de3ed25e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Standardizing the amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45e001-4920-47e1-8dbb-6681cf9b32e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:12:00.856649Z",
     "start_time": "2023-10-20T11:11:39.230512Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# pretrain\n",
    "# Be really carefull of the column order here !\n",
    "filtered_data = scaler.fit_transform(X_pretrain_band.T)\n",
    "filtered_data = filtered_data.T\n",
    "\n",
    "if isinstance(X_train_band, list): # Multiple instances -> classification\n",
    "    # train\n",
    "    X_train_band = [scaler.fit_transform(time_series) for time_series in tqdm(X_train_band)]\n",
    "    \n",
    "    # test\n",
    "    X_test_band = [scaler.fit_transform(time_series) for time_series in tqdm(X_test_band)]\n",
    "else: # TODO: add more check. One instance -> prediction\n",
    "    print(\"hello\")\n",
    "    # train\n",
    "    X_train_band = scaler.fit_transform(X_train_band)\n",
    "\n",
    "    # test\n",
    "    X_test_band = scaler.fit_transform(X_test_band)\n",
    "\n",
    "\n",
    "if not is_multivariate: \n",
    "    if isinstance(X_train, list): # Multiple instances -> classification\n",
    "        # train\n",
    "        X_train = [scaler.fit_transform(x).flatten() for x in tqdm(X_train)]\n",
    "    \n",
    "        # test\n",
    "        X_test = [scaler.fit_transform(x).flatten() for x in tqdm(X_test)]\n",
    "    else : # TODO: add more check. One instance -> prediction\n",
    "        print(\"hello\")\n",
    "        # train\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "        # test\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726aa2df-8352-4b1d-b504-5d113c993cf0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93f662-f197-4b0a-a140-090f3c0909d2",
   "metadata": {},
   "source": [
    "# Generating datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685e0ff43b927c3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Reservoir functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a38d2df41a727",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from reservoir.activation_functions import tanh, heaviside, sigmoid\n",
    "\n",
    "# the activation function choosen for the rest of the experiment\n",
    "# activation_function = lambda x : sigmoid(2*(x-0.5))\n",
    "activation_function = lambda x : tanh(x)\n",
    "\n",
    "plt.plot(np.linspace(0, 1.1, 100), activation_function(np.linspace(0, 1.1, 100)))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b39f1d-d8d6-451b-8d4d-b8f8f74a498d",
   "metadata": {},
   "source": [
    "## Plot  pretrain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab965ec2-e31c-475e-b452-05c9dbfde13b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:12:00.858650Z",
     "start_time": "2023-10-20T11:12:00.856783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Min window size to get all the dynamics ? \n",
    "min_window_size = sampling_rate/filtered_peak_freqs[-1]\n",
    "max_window_size = sampling_rate/filtered_peak_freqs[0]\n",
    "\n",
    "print(min_window_size)\n",
    "print(max_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd00d3f-9b0c-475f-bac9-b46e5f92adfb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-20T11:12:00.862019Z"
    },
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Compute the moving average \n",
    "window_size = 10\n",
    "\n",
    "if max_window_size <= window_size or  window_size <= min_window_size:\n",
    "    raise ValueError(f\"window_size must be greater than {min_window_size} and smaller than {max_window_size}. Current window_size is {window_size}.\")\n",
    "\n",
    "weights = np.repeat(1.0, window_size)/window_size\n",
    "ma = np.array([np.convolve(d, weights, 'valid') for d in (filtered_data)])\n",
    "\n",
    "END = 1500\n",
    "START = 1000\n",
    "DIFF = END - START\n",
    "#CPlot the two for different frequencies\n",
    "NB_1 = 1\n",
    "fig, ax = plt.subplots(3, 1, figsize=(24,18))\n",
    "ax[0].plot(range(DIFF), filtered_data[NB_1, START:END], label='Time serie')\n",
    "ax[0].plot(range(DIFF), ma[NB_1, START:END], label='Moving average')\n",
    "ax[0].legend(fontsize=26)\n",
    "\n",
    "NB_2 = 10\n",
    "ax[1].plot(range(DIFF), filtered_data[NB_2, START:END], label='Time serie')\n",
    "ax[1].plot(range(DIFF), ma[NB_2, START:END], label='Moving average')\n",
    "\n",
    "#Check that the scaler did a good job\n",
    "ax[2].plot(range(DIFF), X_pretrain_band[NB_2, START:END], label='Time serie')\n",
    "\n",
    "for i, ax in enumerate(ax):\n",
    "    # Format subplot\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='both', labelsize=26)\n",
    "    # draw vertical lines to represent the window for some points\n",
    "    for x in range(100, 500, 100):\n",
    "        ax.axvspan(x, x + window_size, color='g', alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be90fde85cfcf6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data to feed to the reservoir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770968b147c07381",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here we calculate \n",
    "**common_dimension** : the number of different dimensions in the input data\n",
    " **K** : the number of euron that will receive a particular time serie as input \n",
    "**n** : the dimension of the reservoir \n",
    "\n",
    "n = K * common_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f44547",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def find_common_dimension(array1, array2):\n",
    "    matching_indices = None\n",
    "    matching_values = None\n",
    "\n",
    "    for i, dim in enumerate(array1.shape):\n",
    "        if dim in array2.shape:\n",
    "            matching_indices = i\n",
    "            matching_values = dim\n",
    "  \n",
    "    return matching_indices, matching_values\n",
    "\n",
    "if isinstance(X_train_band, list): # Multiple instances -> classification\n",
    "    common_xtrain_index, common_xtrain_size = find_common_dimension(X_train_band[0], filtered_data)\n",
    "else:\n",
    "    common_xtrain_index = 0\n",
    "    common_xtrain_size = filtered_data.shape[common_xtrain_index]\n",
    "      \n",
    "\n",
    "print(\"Common dimension index is :\", common_xtrain_index)\n",
    "print(\"Number of different time series is :\", common_xtrain_size)\n",
    "if is_multivariate:\n",
    "    print(\"\\nCheck it ! \\nFirst array \", X_train_band[0].shape, \" and second array\", X_train_band[1].shape)\n",
    "\n",
    "# We want the size of the reservoir to be at least 200\n",
    "K = math.ceil(200 / common_xtrain_size)\n",
    "n = common_xtrain_size * K\n",
    "print(\"Dimension of our reservoir :\", n)\n",
    "print(\"Copy of each time serie :\", K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df206ba-36a2-4468-b719-7bc819114b1a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aae570c7-b714-4bae-afae-012fd8931166",
   "metadata": {},
   "source": [
    "INCREMENT = int(min_window_size+1)\n",
    "MAX_INCREMENT = None #int(max_window_size)\n",
    "VALUE = 0.05\n",
    "target_rate = 0.8\n",
    "growth_parameter = 0.15\n",
    "\n",
    "bias_scaling = 1\n",
    "input_scaling = 0.1\n",
    "leaky_rate = 1\n",
    "\n",
    "\n",
    "\n",
    "N_JOBS = -1\n",
    "RIDGE_COEF = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4957e6c-e0cc-4d90-a495-348a8bbab3cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import train_and_predict_model, compute_score\n",
    "from joblib import Parallel, delayed\n",
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.bounded_adsp import run_HADSP_algorithm\n",
    "from connexion_generation.utility import TwoDimArrayWrapper\n",
    "\n",
    "N_JOBS = -1\n",
    "\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest values for the parameters you want to optimize\n",
    "    input_scaling = trial.suggest_float('input_scaling', 0.01, 1.0, step=0.01)\n",
    "    bias_scaling = trial.suggest_float('bias_scaling', 0, 10, step=0.1)\n",
    "    leaky_rate = 0\n",
    "    connectivity = 0\n",
    "    input_connectivity = 1\n",
    "    INCREMENT = trial.suggest_int('INCREMENT', min_window_size + 1, max_window_size - 1)\n",
    "    VALUE = trial.suggest_float('VALUE', 0.01, 0.5, step=0.01)\n",
    "    target_rate = trial.suggest_float('target_rate', 0.5, 1, step=0.01)\n",
    "    growth_parameter = trial.suggest_float('growth_parameter', 0.01, target_rate, step=0.01)\n",
    "    ridge = trial.suggest_int('ridge', -10, 1)\n",
    "    RIDGE_COEF = 10**ridge\n",
    "\n",
    "    reservoir_size = trial.suggest_int('reservoir_size', 50, 1000, 50)\n",
    "\n",
    "    K = math.ceil(200 / common_xtrain_size)\n",
    "    n = common_xtrain_size * K\n",
    "\n",
    "    # We create an array of the same shape as X_pretrain_band but with the same time serie repeated K times\n",
    "    frequency_bands = np.repeat(filtered_data, K, axis=0)  \n",
    "    frequency_bands = TwoDimArrayWrapper(frequency_bands)\n",
    "    caca = []\n",
    "    caca_test = []\n",
    "    for i in range(len(X_train_band)):\n",
    "        if common_xtrain_index == 1:\n",
    "            caca.append(np.repeat(X_train_band[i], K, axis=1))\n",
    "        else:\n",
    "            caca.append(np.repeat(X_train_band[i], K, axis=0).T) # correct axis depends on X_train_band shape\n",
    "    for i in tqdm(range(len(X_test_band))):\n",
    "        if common_xtrain_index == 1:\n",
    "            caca_test.append(np.repeat(X_test_band[i], K, axis=1))\n",
    "        else:\n",
    "            caca_test.append(np.repeat(X_test_band[i], K, axis=0).T)\n",
    "\n",
    "    \n",
    "    def initialise_and_train(input_scaling, n, input_connectivity, connectivity, bias_scaling, seed, training_set, visualize=False):\n",
    "        Win, W, bias = init_matrices(n, input_connectivity, connectivity, seed=seed)\n",
    "        bias *= bias_scaling\n",
    "        Win *= input_scaling\n",
    "            \n",
    "        W, state_history = run_HADSP_algorithm(W, Win, bias, leaky_rate, activation_function, training_set, INCREMENT, VALUE,\n",
    "                                target_rate, growth_parameter, max_increment=INCREMENT, visualize=visualize)\n",
    "        \n",
    "        connectivity =  W.count_nonzero() / (W.shape[0] * W.shape[1])\n",
    "        eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "        sr = max(abs(eigen))\n",
    "        \n",
    "        return Win, W, bias, connectivity, sr\n",
    "    \n",
    "    total_score = 0\n",
    "    for _ in range(3):  # Repeat the process three times\n",
    "        # HADSP + multi\n",
    "        Win_hadsp_multi, W_hadsp_multi, bias_hadsp_multi, connectivity_band, sr_hadsp_multi = initialise_and_train(\n",
    "            input_scaling, n, input_connectivity, connectivity, bias_scaling, SEED, frequency_bands\n",
    "        )\n",
    "        \n",
    "        Y_pred_hadsp_multi = train_and_predict_model(\n",
    "            W_hadsp_multi, Win_hadsp_multi, bias_hadsp_multi, activation_function, RIDGE_COEF, caca, caca_test, Y_train, N_JOBS\n",
    "        )\n",
    "        \n",
    "        score = compute_score(Y_pred_hadsp_multi, Y_test, \"HADSP multi\", verbosity=0)\n",
    "        total_score += score\n",
    "\n",
    "    average_score = total_score / 3  # Average the score\n",
    "\n",
    "    return score\n",
    "\n",
    "storage = optuna.storages.RDBStorage(\n",
    "    url=\"sqlite:///optuna_hadsp_db.sqlite3\",\n",
    "    engine_kwargs={\"pool_size\": 20, \"connect_args\": {\"timeout\": 10}},\n",
    ")\n",
    "\n",
    "\n",
    "# Create a study object and specify the direction as 'maximize'.\n",
    "study = optuna.create_study(direction='maximize', storage=storage, study_name=\"hadsp\")\n",
    "\n",
    "# Optimize the study, the number of trials can be set as per computational resources.\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d83057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    # HADSP + uni\n",
    "    Win_hadsp_uni, W_hadsp_uni, bias_hadsp_uni, connectivity_hadsp_uni, sr_hadsp_uni = initialise_and_train(input_scaling, n,  input_connectivity, connectivity, bias_scaling, SEED, X_pretrain.flatten())\n",
    "    \n",
    "    # random + uni\n",
    "    Win_normal, W_normal, bias_normal =  init_matrices(n, 1, connectivity_hadsp_uni, sr_hadsp_uni)\n",
    "    bias_normal= bias_normal*bias_scaling\n",
    "    Win_normal= Win_normal*input_scaling   \n",
    "    \n",
    "    eigen_normal = sparse.linalg.eigs(W_normal, k=1, which=\"LM\", maxiter=W_normal.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "    sr_normal = max(abs(eigen_normal))\n",
    "    \n",
    "    heatmap(W_hadsp_uni.todense(), cmap=color_palette(\"cividis\", as_cmap=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68129bc2-1cc6-4498-9e57-ed18e037643e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-09T13:38:25.203377Z"
    }
   },
   "source": [
    "### Spectral radius normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49826ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sr_hadsp_multi)\n",
    "print(sr_random_multi)\n",
    "if not is_multivariate:\n",
    "    print(sr_normal)\n",
    "    print(sr_hadsp_uni)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a579f67-1e46-4b4b-88b3-73e086e1a106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T11:55:21.130103Z",
     "iopub.status.busy": "2023-10-24T11:55:21.129538Z",
     "iopub.status.idle": "2023-10-24T11:55:21.137834Z",
     "shell.execute_reply": "2023-10-24T11:55:21.136748Z",
     "shell.execute_reply.started": "2023-10-24T11:55:21.130073Z"
    }
   },
   "source": [
    "# Spectral radius normalisation\n",
    "normal_sr = 1.2\n",
    "W_hadsp_multi = W_hadsp_multi/sr_hadsp_multi*normal_sr\n",
    "W_random_multi = W_random_multi/sr_random_multi*normal_sr\n",
    "if not is_multivariate:\n",
    "    W_normal = W_normal/sr_normal*normal_sr\n",
    "    W_hadsp_uni = W_hadsp_uni/sr_hadsp_uni*normal_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b447f6-e964-4f6c-9116-597cd29c1755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:13:26.845564Z",
     "start_time": "2023-10-09T13:13:26.821527Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hadsp_env",
   "language": "python",
   "name": "hadsp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
