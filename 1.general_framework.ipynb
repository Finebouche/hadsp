{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c1d64",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-10T10:40:49.539850Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from importlib import reload\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from seaborn import heatmap, color_palette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44384e-7cbc-4611-a015-e1c073f61aa9",
   "metadata": {},
   "source": [
    "# Datasets loading\n",
    "\n",
    "Lots of different on availabale : https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad\n",
    "\n",
    "Review: \n",
    "https://arxiv.org/abs/2012.02974\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00291787-4082-48ea-9002-54d777cac5b6",
   "metadata": {},
   "source": [
    "## Prediction ahead\n",
    "\n",
    "Datasets available :\n",
    "\n",
    "* ReservoirPy: MackeyGlass, Lorenz\n",
    "* Custom: Sunspot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d643a04-0a92-420b-8d57-e777e365dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.load_prediction import load_dataset_prediction\n",
    "is_instances_classification = False\n",
    "dataset_name = \"Lorenz\"\n",
    "step_ahead=5\n",
    "\n",
    "is_multivariate, sampling_rate, X_train_raw, X_test_raw, Y_train_raw, Y_test = load_dataset_prediction(dataset_name, step_ahead, visualize=True)\n",
    "use_spectral_representation = False\n",
    "spectral_representation = None # Can be None, \"stft\" or \"mfcc\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae6a7ebe247f27",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a741331a-93ba-4132-8449-0e9521c4ddcc",
   "metadata": {},
   "source": [
    "Datasets available :\n",
    "\n",
    "* Custom :  FSDD, HAART, JapaneseVowels\n",
    "* Aeon : SpokenArabicDigits, CatsDogs, LSST\n",
    "* Torchaudio: SPEECHCOMMANDS\n",
    "\n",
    "More on https://www.timeseriesclassification.com/dataset.php or https://pytorch.org/audio/stable/datasets.html"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5762b15b-a599-4ea4-b6b3-cb48fdc78947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T11:57:47.080633Z",
     "iopub.status.busy": "2024-10-04T11:57:47.080493Z",
     "iopub.status.idle": "2024-10-04T11:57:49.539282Z",
     "shell.execute_reply": "2024-10-04T11:57:49.539027Z",
     "shell.execute_reply.started": "2024-10-04T11:57:47.080624Z"
    }
   },
   "source": [
    "from datasets.load_classification import load_dataset_classification\n",
    "is_instances_classification = True\n",
    "spectral_representation = \"mfcc\"  # Can be None, \"stft\" or \"mfcc\"\n",
    "\n",
    "dataset_name = \"FSDD\"\n",
    "\n",
    "use_spectral_representation, is_multivariate, sampling_rate, X_train_raw, X_test_raw, Y_train_raw, Y_test, groups = load_dataset_classification(dataset_name)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9d58795-9a5d-4b25-9237-a278de6600c6",
   "metadata": {},
   "source": [
    "from datasets.preprocessing import plot_classes_distribution\n",
    "\n",
    "# Plot data distribution\n",
    "if is_instances_classification:\n",
    "    plot_classes_distribution(Y_train_raw, Y_test, val = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55bde5-5bb3-41ea-ae0d-10d7bc7401f4",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90237f4e-2418-48a9-a067-aece8c922912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit, StratifiedGroupKFold\n",
    "from datasets.preprocessing import flexible_indexing\n",
    "from datasets.preprocessing import plot_classes_distribution\n",
    "\n",
    "# CROSS-VALIDATION METHODS\n",
    "# SEED\n",
    "SEED = 49387\n",
    "\n",
    "use_cross_validation = False\n",
    "\n",
    "if use_cross_validation: # we split train between train and val\n",
    "    if is_instances_classification:\n",
    "        if groups is None:\n",
    "            splits = StratifiedKFold(n_splits=2, shuffle=True, random_state=SEED).split(X_train_raw, np.argmax(Y_train_raw, axis=1))\n",
    "        else:\n",
    "            splits = StratifiedGroupKFold(n_splits=2, shuffle=True, random_state=SEED).split(X_train_raw, np.argmax(Y_train_raw, axis=1), groups)\n",
    "    else: #prediction\n",
    "        splits = TimeSeriesSplit(n_splits=2).split(X_train_raw)\n",
    "    \n",
    "    for i, (train_index, val_index) in enumerate(splits):\n",
    "        X_train = flexible_indexing(X_train_raw, train_index)\n",
    "        X_val = flexible_indexing(X_train_raw, val_index)\n",
    "        Y_train = flexible_indexing(Y_train_raw, train_index)\n",
    "        Y_val = flexible_indexing(Y_train_raw, val_index)\n",
    "        # SPLITS\n",
    "        if is_multivariate and not is_instances_classification:\n",
    "            X_train_band, X_val_band, X_test_band = X_train, X_val, X_test_raw\n",
    "            del X_train, X_val\n",
    "        else:\n",
    "            X_test = X_test_raw\n",
    "\n",
    "    if is_instances_classification:\n",
    "        plot_classes_distribution(Y_train, Y_val, val = True)\n",
    "else: # then we use the test dataset\n",
    "    if is_multivariate:\n",
    "        X_train_band, X_test_band = X_train_raw, X_test_raw\n",
    "        del X_train_raw, X_test_raw\n",
    "        X_val_band = None\n",
    "    else:\n",
    "        X_test, X_train = X_test_raw, X_train_raw\n",
    "        X_val, X_val_band = None, None\n",
    "        del X_train_raw, X_test_raw\n",
    "    Y_train = Y_train_raw\n",
    "    del Y_train_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468ca5c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "Spectrograms_vs_Cochleagrams : https://www.researchgate.net/publication/340510607_Speech_recognition_using_very_deep_neural_networks_Spectrograms_vs_Cochleagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766304bc-3c47-4813-9828-91d8fed5845b",
   "metadata": {},
   "source": [
    "## Multivariate generation (if not multivariate) and train_validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ad0f4-4dbc-4b97-8d28-8ffb357715d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:11:18.377724Z",
     "start_time": "2023-10-20T11:11:18.143305Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets.multivariate_generation import generate_multivariate_dataset, extract_peak_frequencies\n",
    "\n",
    "freq_train_data = X_train_band if is_multivariate else X_train\n",
    "flat_train_data = np.concatenate(freq_train_data, axis=0) if is_instances_classification else freq_train_data\n",
    "\n",
    "WINDOW_LENGTH = 10\n",
    "\n",
    "smoothed_peaks_freqs = extract_peak_frequencies(flat_train_data, sampling_rate, smooth=True, window_length=WINDOW_LENGTH, threshold=1e-5, nperseg=1024, visualize=True)\n",
    "nb_smoothed = len(smoothed_peaks_freqs[0]) if is_multivariate else len(smoothed_peaks_freqs)\n",
    "print(\"Smoothed peaks: \", nb_smoothed)\n",
    "\n",
    "peak_freqs = extract_peak_frequencies(flat_train_data, sampling_rate, smooth=True, window_length=1, threshold=1e-5, nperseg=1024, visualize=True)\n",
    "nb_peaks = len(peak_freqs[0]) if is_multivariate else len(peak_freqs)\n",
    "print(\"Non-smoothed peaks: \", nb_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3488bf-2624-44aa-9ca4-7d96c975c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.multivariate_generation import generate_multivariate_dataset, extract_peak_frequencies\n",
    "\n",
    "# if it has use_spectral_representation, then it is multivariate\n",
    "if use_spectral_representation == True: \n",
    "    if is_multivariate==False:\n",
    "        raise ValueError(\"Cannot use spectral representation if it's not multivariate !\")\n",
    "\n",
    "if not is_multivariate:\n",
    "    print(f\"Converting single variate to {spectral_representation}\")\n",
    "    X_train_band = generate_multivariate_dataset(\n",
    "        X_train, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "    )\n",
    "    if use_cross_validation:\n",
    "        X_val_band = generate_multivariate_dataset(\n",
    "            X_val, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "        )\n",
    "    else:\n",
    "        X_test_band = generate_multivariate_dataset(\n",
    "            X_test, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "        )\n",
    "elif is_multivariate and not use_spectral_representation:\n",
    "    print(f\"Converting multivariate to {spectral_representation}\") # if None we convert to temporal\n",
    "    X_train_band = generate_multivariate_dataset(\n",
    "        X_train_band, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "    )\n",
    "    if use_cross_validation:\n",
    "        X_val_band = generate_multivariate_dataset(\n",
    "            X_val_band, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "        )\n",
    "    else:\n",
    "        X_test_band = generate_multivariate_dataset(\n",
    "            X_test_band, sampling_rate, is_instances_classification, peak_freqs, spectral_representation, hop=100\n",
    "        )\n",
    "else:\n",
    "    print(\"Data is already spectral, nothing to do\")\n",
    "\n",
    "if spectral_representation is not None:\n",
    "    use_spectral_representation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b9c52-341b-44b0-a97f-e1a5bf37db60",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5278c-9bf6-42b5-8845-c0c41667924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa \n",
    "\n",
    "if use_spectral_representation is True:\n",
    "    fig, ax = plt.subplots()\n",
    "    img = librosa.display.specshow(librosa.amplitude_to_db(X_train_band[1], ref=np.max), y_axis='log', x_axis='time', ax=ax)\n",
    "    print('Power spectrogram')\n",
    "    plt.xticks(rotation=70)\n",
    "    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "else:\n",
    "    test_x = X_val_band if use_cross_validation else X_test_band\n",
    "    test_y = Y_val if use_cross_validation else Y_test\n",
    "    # Concatenate train and test arrays for plotting\n",
    "    combined_data = np.concatenate((X_train_band, test_x), axis=0)\n",
    "    combined_Y = np.concatenate((Y_train, test_y), axis=0)\n",
    "    \n",
    "    # Calculate the merge point index\n",
    "    merge_point_index = X_train_band.shape[0] if not is_instances_classification else len(X_train_band)\n",
    "    merge_point_index = 250\n",
    "    # Define the range around the merge point to plot\n",
    "    WATCH_FROM = merge_point_index - 250\n",
    "    WATCH_TO = merge_point_index + 250\n",
    "    DIFF = WATCH_TO - WATCH_FROM \n",
    "    flat_peak_freqs = np.hstack(peak_freqs) if is_multivariate else peak_freqs\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    colors = color_palette(\"tab20\")\n",
    "    j = 0\n",
    "    nb_variables = X_train_band.shape[1] if not is_instances_classification else X_train_band[0].shape[1]\n",
    "    for i in np.random.randint(0, nb_variables, size=3): \n",
    "        plt.plot(range(DIFF), combined_data[:, i][WATCH_FROM:WATCH_TO], color=colors[j], label=f'Feature {i} at {round(flat_peak_freqs[i], 1)} Hz')\n",
    "        j += 1\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Feature Value')\n",
    "\n",
    "fontsize=12\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel(plt.gca().get_xlabel(), fontsize=fontsize)\n",
    "plt.ylabel(plt.gca().get_ylabel(), fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b859e5-c6dd-4086-ac56-42a7de3ed25e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Standardizing the amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45e001-4920-47e1-8dbb-6681cf9b32e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T11:12:00.856649Z",
     "start_time": "2023-10-20T11:11:39.230512Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets.preprocessing import scale_data\n",
    "\n",
    "scaler_multi = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_band, X_val_band, X_test_band = scale_data(X_train_band, X_val_band, X_test_band, scaler_multi, is_instances_classification)\n",
    "            \n",
    "if not is_multivariate:\n",
    "    scaler_x_uni = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train, X_val, X_test = scale_data(X_train, X_val, X_test, scaler_multi, is_instances_classification)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde977ab-e2ee-43a7-a916-3ae2f4a95780",
   "metadata": {},
   "source": [
    "## Noizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a01ef-7b49-4493-aa96-3ea65b18b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define noise parameter\n",
    "noise_std = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97747959-8de6-4fdc-85e8-f8e6425b3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.preprocessing import add_noise, duplicate_data\n",
    "\n",
    "#Train/Val/Test\n",
    "if is_instances_classification:\n",
    "    # NOISE\n",
    "    # UNI\n",
    "    if not is_multivariate:\n",
    "        X_train_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_train, desc=\"TRAIN UNI\")]\n",
    "        if X_val is not None:\n",
    "            X_val_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_val, desc=\"VAL UNI\")]\n",
    "        X_test_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_test, desc=\"TEST UNI\")]\n",
    "        \n",
    "    # MULTIX\n",
    "    X_train_band_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_train_band, desc=\"TRAIN MULTI\")]\n",
    "    if X_val_band is not None:\n",
    "        X_val_band_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_val_band, desc=\"VAL MULTI\")]\n",
    "    X_test_band_noisy = [add_noise(instance, noise_std) for instance in tqdm(X_test_band, desc=\"TEST MULTI\")]\n",
    "\n",
    "else:  #if prediction\n",
    "    # NOISE\n",
    "    # UNI\n",
    "    if not is_multivariate:\n",
    "        X_train_noisy = add_noise(X_train, noise_std)\n",
    "        X_test_noisy = add_noise(X_test, noise_std)\n",
    "        if X_val is not None:\n",
    "            X_val_noisy = add_noise(X_val, noise_std)\n",
    "\n",
    "    # MULTI\n",
    "    X_train_band_noisy = add_noise(X_train_band, noise_std)\n",
    "    if X_val_band is not None:\n",
    "        X_val_band_noisy = add_noise(X_val_band, noise_std)\n",
    "    X_test_band_noisy = add_noise(X_test_band, noise_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6f51b-0c6c-4cec-bbd5-1fff0223efed",
   "metadata": {},
   "source": [
    "## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5fc6e-c2cf-4441-817a-6ba38f14e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of instances you want to select\n",
    "x_size = len(X_train_band) if is_multivariate else len(X_train)\n",
    "num_samples_for_pretrain = 500 if x_size >= 500 else x_size\n",
    "if is_instances_classification:\n",
    "    indices = np.random.choice(x_size, num_samples_for_pretrain, replace=False)\n",
    "else:\n",
    "    indices = range(x_size)\n",
    "    \n",
    "if not is_multivariate:\n",
    "    X_pretrain_uni = np.array(X_train, dtype=object)[indices]\n",
    "    X_pretrain_noisy = np.array(X_train_noisy, dtype=object)[indices]\n",
    "X_pretrain_band = np.array(X_train_band, dtype=object)[indices]\n",
    "X_pretrain_band_noisy = np.array(X_train_band_noisy, dtype=object)[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93f662-f197-4b0a-a140-090f3c0909d2",
   "metadata": {},
   "source": [
    "# Generating reservoirs\n",
    "\n",
    "We are interrested in two technique to genereate reservoir. \n",
    "* One is called hag, was studied in previous paper, and recombines inputs based on their activity to reach a given activation target.\n",
    "* The other called DESP recombines inputs in order to reach a given standard deviation of activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b33224-cc12-4edd-948c-235f415a4ab8",
   "metadata": {},
   "source": [
    "## Data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa63393-5c4e-4cce-9ed0-7caf47af8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_instances_classification:\n",
    "    common_index = 1\n",
    "    print(\"Common index for multivariate classification should be 1\")\n",
    "    print(\"\\nCheck it ! \\nFirst array \", X_train_band[0].shape, \" and second array\", X_train_band[2].shape)\n",
    "    common_size = X_train_band[0].shape[common_index]\n",
    "else:\n",
    "    common_index = 1\n",
    "    print(\"Common index for multivariate prediction should be 1\")\n",
    "    print(\"\\nCheck it ! \\nFirst array \", X_train_band.shape, \" and second array\", X_train_band.shape)\n",
    "    common_size = X_train_band.shape[common_index]\n",
    "\n",
    "print(\"Common size:\", common_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24988d03-a16b-46f7-b3f1-372452272905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min window size to get all the dynamics ? \n",
    "min_window_size = sampling_rate/np.max(np.hstack(peak_freqs))\n",
    "max_window_size = sampling_rate/np.min(np.hstack(peak_freqs))\n",
    "\n",
    "print(min_window_size)\n",
    "print(max_window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ddcde6-215a-464a-8634-624f11da4380",
   "metadata": {},
   "source": [
    "### Visualisation checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce2087-6574-440e-a849-cecea6e708b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_instances_classification:\n",
    "    train_band_data =  np.concatenate(X_train_band, axis=0).T\n",
    "else:\n",
    "    train_band_data =  X_train_band.T\n",
    "\n",
    "START = 0\n",
    "END = 500\n",
    "\n",
    "DIFF = END - START\n",
    "flat_peak_freqs = np.hstack(peak_freqs) if is_multivariate else peak_freqs\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "colors = color_palette(\"tab20\")\n",
    "j = 0\n",
    "for i in np.random.randint(0, train_band_data.shape[0], size=3):\n",
    "    plt.plot(range(DIFF), train_band_data[i, START:END], color=colors[j], label=f'Feature {i} at {round(flat_peak_freqs[i], 1)} Hz')\n",
    "    j += 1\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "fontsize=12\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Time', size=fontsize)\n",
    "plt.ylabel('Value', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c6a9b-743a-4664-8fd1-773cd3ce3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFF = WATCH_TO - WATCH_FROM \n",
    "flat_peak_freqs = np.hstack(peak_freqs) if is_multivariate else peak_freqs\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "colors = color_palette(\"tab20\")\n",
    "j = 0\n",
    "nb_variables = X_train_band.shape[1] if not is_instances_classification else X_train_band[0].shape[1]\n",
    "for i in np.random.randint(0, nb_variables, size=3): \n",
    "    plt.plot(range(DIFF), combined_data[:, i][WATCH_FROM:WATCH_TO] color=colors[j], label=f'Feature {i} at {round(flat_peak_freqs[i], 1)} Hz')\n",
    "    j += 1\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Feature Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6435e2-6f19-4987-9309-ab74694678fd",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Shared parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd8001c-ba74-457f-9518-c0afa04f3606",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Reservoir parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a38d2df41a727",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from reservoir.activation_functions import tanh, heaviside, sigmoid, softplus, heaviside\n",
    "\n",
    "# the activation function choosen for the rest of the experiment\n",
    "# activation_function = lambda x : sigmoid(2*(x-0.5))tanh(x)\n",
    "activation_function = lambda x : tanh(x)\n",
    "\n",
    "x=np.linspace(0, 2, 100)\n",
    "plt.plot(x, activation_function(x))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4dc28-34d5-4a98-a047-7fa6e4e9da47",
   "metadata": {},
   "source": [
    "**common_size** : the number of different dimensions in the input data\n",
    "\n",
    "**K** : the number of neurons that will receive a particular time serie as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433d6d3-acc2-4bbc-8373-12d692eda119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "RESERVOIR_SIZE = 500\n",
    "\n",
    "# We want the size of the reservoir to be at least RESERVOIR_SIZE\n",
    "K = math.ceil(RESERVOIR_SIZE / common_size)\n",
    "n = common_size * K\n",
    "print(\"Dimension of our reservoir :\", n)\n",
    "print(\"Copy of each time serie :\", K)\n",
    "print(\"Number of multivariate inputs :\", common_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f3072-3e8d-49f1-a890-5c3a28df3d9e",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc2c01-de54-4bf6-8fa0-1dbde5fe21c2",
   "metadata": {},
   "source": [
    "#### Customs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6c7bf77-e19f-4790-b401-a40d219d8e7c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "time_increment = 82 # int(max_window_size) or int(min_window_size+1)\n",
    "time_increment_span = 14\n",
    "MAX_TIME_INCREMENT = TIME_INCREMENT + time_increment_span #int(max_window_size) or None  or TIME_INCREMENT\n",
    "weight_increment = 0.030000000000000002\n",
    "\n",
    "target_rate = 0.58\n",
    "RATE_SPREAD = 0.34\n",
    "\n",
    "min_variance = 0.014000000000000002\n",
    "variance_spread = 0.003\n",
    "\n",
    "bias_scaling = 0.0\n",
    "input_scaling = 0.060000000000000005\n",
    "\n",
    "intrinsic_saturation = 0.92\n",
    "intrinsic_coef = 0.8\n",
    "\n",
    "max_partners= 20\n",
    "use_full_instance = False\n",
    "\n",
    "# Doesn't move :\n",
    "input_connectivity = 1\n",
    "connectivity = 0\n",
    "leaky_rate = 1\n",
    "if int(max_window_size) < TIME_INCREMENT or TIME_INCREMENT < min_window_size:\n",
    "    raise ValueError(f\"INCREMENT must be greater than {min_window_size} and smaller than {max_window_size}. Current INCREMENT is {TIME_INCREMENT}.\")\n",
    "\n",
    "TIME_INCREMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ba684-539a-4f94-99f0-9bf242717c8b",
   "metadata": {},
   "source": [
    "#### Optuna's bests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae9536-b10e-43b5-9c28-3e66dc8d10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from performances.utility import retrieve_best_model\n",
    "\n",
    "function_name = \"random\"  # \"desp\" ou \"hadsp\" or \"random\"\n",
    "study = retrieve_best_model(\"random\", dataset_name, is_multivariate, variate_type = \"multi\", data_type = \"normal\")\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \")\n",
    "for param_name, param_value in study.best_trial.params.items():\n",
    "    globals()[param_name] = param_value\n",
    "    print(param_name, param_value)\n",
    "\n",
    "if not is_instances_classification:\n",
    "    use_full_instance = None\n",
    "\n",
    "if function_name == \"hadsp\" or function_name == \"desp\":\n",
    "    max_increment_span = int(max_window_size) if int(max_window_size) - 100 < 0 else int(max_window_size) - 100\n",
    "    MAX_TIME_INCREMENT = time_increment + time_increment_span #int(max_window_size) or None or TIME_INCREMENT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fd1fc57ed9157",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Function to initialise and generate reservoir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39af15",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from reservoir.reservoir import init_matrices\n",
    "from connexion_generation.hag import run_algorithm\n",
    "from scipy import sparse\n",
    "\n",
    "def initialise_and_run(input_scaling, n, input_connectivity, connectivity, K, bias_scaling, seed, training_set):\n",
    "    \n",
    "    Win, W, bias = init_matrices(n, input_connectivity, connectivity,  K, seed=seed)\n",
    "    bias *= bias_scaling\n",
    "    Win *= input_scaling\n",
    "\n",
    "    if function_name == \"hadsp\":\n",
    "        W, (state_history, delta_z_history, W_history) = run_algorithm(W, Win, bias, leaky_rate, activation_function, training_set, time_increment, weight_increment,\n",
    "                                target_rate, rate_spread, function_name, is_instance=is_instances_classification, use_full_instance = use_full_instance, \n",
    "                                max_increment=MAX_TIME_INCREMENT, max_partners=max_partners, method = \"pearson\", \n",
    "                                n_jobs = 12, visualize=False, record_history=True)\n",
    "\n",
    "    elif function_name == \"desp\":\n",
    "        W, (state_history, delta_z_history, W_history) = run_algorithm(W, Win, bias, leaky_rate, activation_function, training_set, time_increment, weight_increment,\n",
    "                            min_variance, variance_spread, function_name, is_instance=is_instances_classification, use_full_instance = use_full_instance, \n",
    "                            max_increment=MAX_TIME_INCREMENT, max_partners=max_partners, method = \"pearson\", \n",
    "                            intrinsic_saturation=intrinsic_saturation, intrinsic_coef=intrinsic_coef, n_jobs = 12, visualize=False, record_history=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid function_name: {function_name}, should be 'hadsp' or 'desp'\")\n",
    "    connectivity =  np.count_nonzero(W) / (W.shape[0] * W.shape[1])\n",
    "    eigen = sparse.linalg.eigs(W, k=1, which=\"LM\", maxiter=W.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "    sr = np.max(np.abs(eigen))\n",
    "\n",
    "    \n",
    "    \n",
    "    return Win, W, bias, connectivity, sr, state_history, delta_z_history, W_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4de89f833989d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Multivariate case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54472b03-ca39-4f13-92cf-e71e6d63c135",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(Win_hag_multi, \n",
    " W_hag_multi, \n",
    " bias_hag_multi, \n",
    " connectivity_band, \n",
    " sr_hag_multi, \n",
    " state_history_hag,\n",
    " delta_z_history,\n",
    " W_history) = initialise_and_run(input_scaling, n, input_connectivity, connectivity, K, bias_scaling, SEED, X_pretrain_band)\n",
    "\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "custom_colormap = ListedColormap(np.vstack((plt.cm.cividis(0.0), plt.cm.cividis(np.linspace(0.5, 1, 128)))))\n",
    "vmin = 0\n",
    "vmax = max(np.max(Win_hag_multi), np.max(W_hag_multi))\n",
    "fig, axs = plt.subplots(ncols=3, gridspec_kw=dict(width_ratios=[0.5,6,0.2]))\n",
    "heatmap(Win_hag_multi, cmap=custom_colormap, cbar=False, square=True, ax=axs[0], vmin=vmin, vmax=vmax)\n",
    "heatmap(W_hag_multi, cmap=custom_colormap, yticklabels=False, cbar=False, ax=axs[1], vmin=vmin, vmax=vmax)\n",
    "fig.colorbar(axs[1].collections[0], cax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206b894-d2b6-43a2-8f23-8b62678c44d7",
   "metadata": {},
   "source": [
    "#### Algorithm Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be4078-9172-48f0-b39f-9b9c634c136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if delta_z_history is not None:\n",
    "    fig, ax = plt.subplots(figsize=(16, 5))\n",
    "    print(f'Dynamics')\n",
    "    \n",
    "    # NEURON ACTIVITY PLOT\n",
    "    random_neurons_indices =  np.sort(np.random.randint(RESERVOIR_SIZE, size=7)) #Size max is 19 because there is not enough colors\n",
    "    random_neurons_indices = np.append(random_neurons_indices, 51)\n",
    "    colors = color_palette(\"tab20\")\n",
    "    # NUMBER_OF_STEP_TO_WATCH\n",
    "    WATCH_FROM = 0\n",
    "    WATCH_TO = 3000\n",
    "    neurons_evolution = np.array(delta_z_history)[WATCH_FROM:WATCH_TO]\n",
    "    x =range(len(delta_z_history))[WATCH_FROM:WATCH_TO]\n",
    "    j = 0\n",
    "    for i in random_neurons_indices:\n",
    "        ax.plot(x, neurons_evolution[: ,i], label=str(i), color=colors[j])\n",
    "        ax.text(-3+WATCH_FROM, neurons_evolution[0, i],  str(i), color=\"black\", fontsize=12)\n",
    "        j += 1\n",
    "    fontsize=18\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='both', labelsize=fontsize)\n",
    "    plt.xlabel('Time', size=fontsize)\n",
    "    plt.ylabel('Value', size=fontsize)\n",
    "    plt.legend(fontsize=fontsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef5f04-6a58-46eb-a958-3937fa36947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "print(f'Dynamics')\n",
    "\n",
    "# NEURON ACTIVITY PLOT \n",
    "random_neurons_indices =  np.sort(np.random.randint(RESERVOIR_SIZE, size=4)) #Size max is 19 because there is not enough colors\n",
    "colors = color_palette(\"tab20\")\n",
    "# NUMBER_OF_STEP_TO_WATCH \n",
    "WATCH_FROM = 0\n",
    "WATCH_TO = 5000\n",
    "neurons_evolution = np.array(state_history_hag)[WATCH_FROM:WATCH_TO]\n",
    "x =range(len(state_history_hag))[WATCH_FROM:WATCH_TO]\n",
    "j = 0\n",
    "for i in random_neurons_indices:\n",
    "    ax.plot(x, neurons_evolution[: ,i], label=str(i), color=colors[j])\n",
    "    ax.text(-3+WATCH_FROM, neurons_evolution[0, i],  str(i), color=\"black\", fontsize=12)\n",
    "    j += 1\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Time', size=fontsize)\n",
    "plt.ylabel('Value', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c886405-da86-4b5e-858b-fabeff6f5473",
   "metadata": {},
   "source": [
    "## Random matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d44149-8fd2-4528-9cdf-1e384eff50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reservoir.reservoir import init_matrices\n",
    "from scipy import sparse\n",
    "\n",
    "connectivity_band = connectivity\n",
    "sr_hag_multi = spectral_radius\n",
    "Win_random_multi, W_random_multi, bias_random_multi =  init_matrices(n, 1, connectivity_band, K, sr_hag_multi, seed=19823)\n",
    "bias_random_multi= bias_random_multi*bias_scaling\n",
    "Win_random_multi= Win_random_multi*input_scaling\n",
    "\n",
    "eigen_random_multi = sparse.linalg.eigs(W_random_multi, k=1, which=\"LM\", maxiter=W_random_multi.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "sr_random_multi = np.max(np.abs(eigen_random_multi))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9a1da-cc49-4b49-aa7c-935d8c12cb2c",
   "metadata": {},
   "source": [
    "# Richness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0eae33-4a27-49ce-b7b5-1be0dadd9c7e",
   "metadata": {},
   "source": [
    "## Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24170c46-7cc0-4b20-85bf-e29b22589da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.richness import pearson\n",
    "num_windows = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e041190-1f47-4252-bfe4-6c55e894ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_correlations_hag, std_correlations_hag = pearson(state_history_hag, num_windows=num_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696a2a0-a60f-4704-b947-605e58b7879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reservoir.reservoir import update_reservoir\n",
    "\n",
    "states_history_multi = []\n",
    "neurons_state = np.random.uniform(0, 1, bias_hag_multi.size)\n",
    "inputs = np.concatenate(X_pretrain_band, axis=0) if is_instances_classification else X_pretrain_band\n",
    "for input_value in inputs:\n",
    "    neurons_state = update_reservoir(W_hag_multi, Win_hag_multi, input_value, neurons_state, leaky_rate, bias_hag_multi, activation_function)\n",
    "    states_history_multi.append(neurons_state)\n",
    "\n",
    "mean_correlations_multi, std_correlations_multi = pearson(states_history_multi, num_windows=num_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386591fb-1869-4f7a-8a0b-fd12acee7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(num_windows)\n",
    "plt.plot(time_windows, mean_correlations_multi, marker='.', linestyle='-', color='b',  label=\"with same input after training\")\n",
    "plt.fill_between(time_windows, np.array(mean_correlations_multi) - np.array(std_correlations_multi),\n",
    "                 np.array(mean_correlations_multi) + np.array(std_correlations_multi), color='blue', alpha=0.2)\n",
    "\n",
    "plt.plot(time_windows, mean_correlations_hag, marker='.', linestyle='-', color='red', label=\"during training\")\n",
    "plt.fill_between(time_windows, np.array(mean_correlations_hag) - np.array(std_correlations_hag),\n",
    "                 np.array(mean_correlations_hag) + np.array(std_correlations_hag), color='red', alpha=0.2)\n",
    "plt.ylim(0, 1)\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Average Correlation', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42acc5c-4ab9-42a2-bf63-194210f62198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reservoir.reservoir import update_reservoir\n",
    "\n",
    "states_history_random = []\n",
    "neurons_state = np.random.uniform(0, 1, bias_random_multi.size)\n",
    "inputs = np.concatenate(X_pretrain_band, axis=0) if is_instances_classification else X_pretrain_band\n",
    "for input_value in inputs:\n",
    "    neurons_state = update_reservoir(W_random_multi, Win_random_multi, input_value, neurons_state, leaky_rate, bias_random_multi, activation_function)\n",
    "    states_history_random.append(neurons_state)\n",
    "\n",
    "mean_correlations_random, std_correlations_random = pearson(np.array(states_history_random), num_windows=num_windows)\n",
    "\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(num_windows)\n",
    "plt.plot(time_windows, mean_correlations_random, marker='.', linestyle='-', color='g',  label=\"with same input for random matrice\")\n",
    "plt.fill_between(time_windows, np.array(mean_correlations_random) - np.array(std_correlations_random),\n",
    "                 np.array(mean_correlations_random) + np.array(std_correlations_random), color='g', alpha=0.2)\n",
    "plt.ylim(0, 1)\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Average Correlation', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03592584-13e0-4abb-b06e-b68cc6b3a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_correlations_random, std_correlations_random = pearson(inputs, num_windows=num_windows)\n",
    "\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(num_windows)\n",
    "plt.plot(time_windows, mean_correlations_random, marker='.', linestyle='-', color='y',  label=\"inputs correlation\")\n",
    "plt.fill_between(time_windows, np.array(mean_correlations_random) - np.array(std_correlations_random),\n",
    "                 np.array(mean_correlations_random) + np.array(std_correlations_random), color='y', alpha=0.2)\n",
    "plt.ylim(0, 1)\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Average Correlation', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42557f32-cb89-45ce-810c-15a35cb5d775",
   "metadata": {},
   "source": [
    "## IPC"
   ]
  },
  {
   "cell_type": "raw",
   "id": "358478ca-5834-43ea-affb-9e94d0a84e48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:11:47.398677Z",
     "iopub.status.busy": "2024-09-09T11:11:47.398377Z",
     "iopub.status.idle": "2024-09-09T11:12:00.366036Z",
     "shell.execute_reply": "2024-09-09T11:12:00.362075Z",
     "shell.execute_reply.started": "2024-09-09T11:11:47.398655Z"
    },
    "scrolled": true
   },
   "source": [
    "from reservoir.reservoir import update_reservoir\n",
    "import analysis.capacities as CAP\n",
    "\n",
    "MAX_DEG = 8\n",
    "MAX_DEL = 100\n",
    "\n",
    "def cap2vec(capacities,maxdel=MAX_DEL,maxdeg=MAX_DEG):\n",
    "    vec = np.zeros((maxdel,maxdeg))\n",
    "    for idx in range(len(capacities)):\n",
    "        delay=capacities[idx]['delay']\n",
    "        degree=capacities[idx]['degree']\n",
    "        if (delay<=maxdel) and (degree<=maxdeg):\n",
    "            vec[delay-1,degree-1]+=capacities[idx]['score']\n",
    "    return vec\n",
    "\n",
    "Win_capa = Win_hag_multi[Win_hag_multi != 0]\n",
    "Win_capa = Win_capa.reshape(Win_capa.size, -1)\n",
    "\n",
    "\n",
    "V_adsp = []\n",
    "for j, W in enumerate(W_history[0:30:4]):    \n",
    "    warmup_steps = 500\n",
    "    warmups = 2.0*np.random.rand(warmup_steps,1)-1.0\n",
    "\n",
    "    # Important: capacity theory with Legendre polynomials assumes uniform random inputs in [-1,1]\n",
    "    steps = 1000000\n",
    "    inputs = 2.0*np.random.rand(steps, 1)-1.0\n",
    "\n",
    "    state = np.random.uniform(0, 1, n)\n",
    "    for i in range(warmup_steps):\n",
    "        state = update_reservoir(W , Win_capa, warmups[i], state, leaky_rate, bias_hag_multi, activation_function)\n",
    "\n",
    "    capa_history = []\n",
    "    for i in range(steps):\n",
    "        state = update_reservoir(W, Win_capa, inputs[i], state, leaky_rate, bias_hag_multi, activation_function)\n",
    "        capa_history.append(state)\n",
    "    capa_history = np.array(capa_history)\n",
    "\n",
    "    # Measallcapsure capacities on inputs and states after removing a \"warmup period\"\n",
    "    # If you require more extensive printed output of individual capacities:\n",
    "    # set verbose = 1\n",
    "    Citer=CAP.capacity_iterator(maxdeg=MAX_DEG, maxdel=MAX_DEL)#, verbose = 1)\n",
    "    totalcap_adsp,allcaps_adsp,numcaps_adsp,nodes = Citer.collect(inputs,capa_history)\n",
    "\n",
    "    print(\"\\nMeasured \",numcaps_adsp,\" capacities above threshold.\\nTotal capacity = \",totalcap_adsp)\n",
    "    V_adsp.append(cap2vec(allcaps_adsp,maxdel = MAX_DEL, maxdeg = MAX_DEG))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2244fa0-fb6b-4dc1-aa43-112e2ac98047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T11:12:01.961425Z",
     "iopub.status.busy": "2024-09-09T11:12:01.960646Z",
     "iopub.status.idle": "2024-09-09T11:12:01.997951Z",
     "shell.execute_reply": "2024-09-09T11:12:01.997259Z",
     "shell.execute_reply.started": "2024-09-09T11:12:01.961368Z"
    }
   },
   "source": [
    "\n",
    "# Assuming V_adsp is defined somewhere and MAX_DEG is set\n",
    "V_adsp_array = np.array(V_adsp)  # This should be a 3D array where each slice [i,:,:] corresponds to a 'degree'\n",
    "delrange = np.arange(1, MAX_DEG + 1)\n",
    "\n",
    "# Create a figure for the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# This will keep track of the cumulative height of the bars (starting at zero)\n",
    "cumulative_height = np.zeros(8)\n",
    "\n",
    "# Loop through each component and plot\n",
    "for component in range(V_adsp_array.shape[2]):\n",
    "    component_cap = np.sum(V_adsp_array,axis=1)[:,component]\n",
    "    plt.bar(range(0,30,4), component_cap, bottom=cumulative_height, label=f'{component + 1}')\n",
    "    cumulative_height += component_cap\n",
    "\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('Information procssing capacity')\n",
    "plt.title('Information procssing capacity over time')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272473cc-1309-4a35-978e-d6197c3a8f31",
   "metadata": {},
   "source": [
    "## LUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b6cd7-d060-4ba4-add7-13e0a492ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.richness\n",
    "reload(analysis.richness)\n",
    "from analysis.richness import uncoupled_dynamics\n",
    "\n",
    "uds_hag = uncoupled_dynamics(np.array(state_history_hag).T, num_windows=num_windows, A=0.99)\n",
    "uds_multi = uncoupled_dynamics(np.array(states_history_multi).T, num_windows=num_windows, A=0.99)\n",
    "\n",
    "max_value = np.max([np.max(uds_hag), np.max(uds_hag)])\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(len(uds_hag))\n",
    "plt.plot(range(len(uds_hag)), uds_hag, marker='.', linestyle='-', color='r',  label=\"during training\")\n",
    "plt.plot(range(len(uds_multi)), uds_multi, marker='.', linestyle='-', color='b',  label=\"with same input after training\")\n",
    "plt.ylim(0, max_value)\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Average Uncoupled Dynamics', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8580e57-d01b-4f38-b535-6b47e2850277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.richness\n",
    "reload(analysis.richness)\n",
    "from analysis.richness import uncoupled_dynamics\n",
    "\n",
    "uds_random = uncoupled_dynamics(np.array(states_history_random).T, num_windows=num_windows, A=0.99)\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(len(uds_random))\n",
    "plt.plot(time_windows, uds_random, marker='.', linestyle='-', color='g',  label=\"with same input for random matrice\")\n",
    "plt.ylim(0, max_value)\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Average Uncoupled Dynamics', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc60097-34c7-4c5e-9ff9-a4e50bd8cee0",
   "metadata": {},
   "source": [
    "## LUD True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6a8db-77c0-461e-b65a-319a9fa362b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.richness\n",
    "reload(analysis.richness)\n",
    "from analysis.richness import eigen_value_spread\n",
    "num_windows = 150\n",
    "\n",
    "uds_hag = eigen_value_spread(np.array(state_history_hag), num_windows=num_windows, theta=0.9)\n",
    "uds_multi = eigen_value_spread(np.array(states_history_multi), num_windows=num_windows, theta=0.9)\n",
    "\n",
    "max_value = np.max([np.max(uds_hag), np.max(uds_hag)])\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(len(uds_hag))\n",
    "plt.plot(range(len(uds_hag)), uds_hag, marker='.', linestyle='-', color='r',  label=\"during training\")\n",
    "plt.plot(range(len(uds_multi)), uds_multi, marker='.', linestyle='-', color='b',  label=\"with same input after training\")\n",
    "plt.ylim(0, max_value)\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Average Uncoupled Dynamics', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c8fce-80db-4d55-87b6-02a14002da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_data = np.array(state_history_hag).T[:, 1000:2000]\n",
    "window_data_std = (window_data - np.mean(window_data, axis=0)) / np.std(window_data, axis=0)\n",
    "\n",
    "U, S, Vt = np.linalg.svd(window_data_std, full_matrices=False)\n",
    "\n",
    "# Calculate the normalized relevance R_j\n",
    "R = S / np.sum(S)\n",
    "\n",
    "# Cumulative sum of R_j\n",
    "cumulative_R = np.cumsum(R)\n",
    "\n",
    "# Find the minimum d such that the cumulative sum >= theta\n",
    "num_components = np.searchsorted(cumulative_R, 0.9) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086ba68-ffa6-404b-81c4-3c553e92c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.richness\n",
    "reload(analysis.richness)\n",
    "from analysis.richness import uncoupled_dynamics\n",
    "\n",
    "uds_random = eigen_value_spread(np.array(states_history_random),  num_windows=num_windows, theta=0.9)\n",
    "\n",
    "# Plotting the mean correlations with the standard deviation area\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "time_windows = range(len(uds_random))\n",
    "plt.plot(time_windows, uds_random, marker='.', linestyle='-', color='g',  label=\"with same input for random matrice\")\n",
    "fontsize=18\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.xlabel('Steps', size=fontsize)\n",
    "plt.ylabel('Average Uncoupled Dynamics', size=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9b3d83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:36:57.885053Z",
     "start_time": "2023-10-09T13:36:57.882050Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Univariate case"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70e02ca4-fea0-464f-b21b-29d141aa79b8",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-07-22T12:57:10.271291Z",
     "iopub.status.busy": "2024-07-22T12:57:10.271110Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "if not is_multivariate:\n",
    "    # hag + uni\n",
    "    (Win_hag_uni, \n",
    "     W_hag_uni, \n",
    "     bias_hag_uni, \n",
    "     connectivity_hag_uni, \n",
    "     sr_hag_uni,\n",
    "     state_history_hag_uni,\n",
    "     _,\n",
    "     _) = initialise_and_train(input_scaling, n, input_connectivity, connectivity, n, bias_scaling, SEED, X_pretrain_uni)\n",
    "\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    \n",
    "    custom_colormap = ListedColormap(np.vstack((plt.cm.cividis(0.0), plt.cm.cividis(np.linspace(0.5, 1, 128)))))\n",
    "    vmin = 0\n",
    "    vmax = max(np.max(Win_hag_uni), np.max(W_hag_uni))\n",
    "    fig, axs = plt.subplots(ncols=3, gridspec_kw=dict(width_ratios=[0.5,6,0.2]))\n",
    "    heatmap(Win_hag_uni, cmap=custom_colormap, cbar=False, square=True, ax=axs[0], vmin=vmin, vmax=vmax)\n",
    "    heatmap(W_hag_uni, cmap=custom_colormap, yticklabels=False, cbar=False, ax=axs[1], vmin=vmin, vmax=vmax)\n",
    "    fig.colorbar(axs[1].collections[0], cax=axs[2])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73fe1a2c-c54e-4ae8-80fe-f57447c8dbe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T12:41:26.032884Z",
     "iopub.status.busy": "2024-07-22T12:41:26.032101Z",
     "iopub.status.idle": "2024-07-22T12:43:14.065679Z",
     "shell.execute_reply": "2024-07-22T12:43:14.065383Z",
     "shell.execute_reply.started": "2024-07-22T12:41:26.032821Z"
    }
   },
   "source": [
    "from analysis.richness import pearson\n",
    "\n",
    "pearson(state_history_hag_uni)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d205ad0-0c4e-4e76-9a21-d69d1b963c54",
   "metadata": {},
   "source": [
    "if not is_multivariate:    \n",
    "    # random + uni\n",
    "    Win_normal, W_normal, bias_normal = init_matrices(n, 1, connectivity_hag_uni, n, sr_hag_uni)\n",
    "    bias_normal= bias_normal*bias_scaling\n",
    "    Win_normal= Win_normal*input_scaling\n",
    "    \n",
    "    eigen_normal = sparse.linalg.eigs(W_normal, k=1, which=\"LM\", maxiter=W_normal.shape[0] * 20, tol=0.1, return_eigenvectors=False)\n",
    "    sr_normal = np.max(np.abs(eigen_normal))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "084e5dfe-8d4d-4e7e-89af-01fbe9e436d1",
   "metadata": {},
   "source": [
    "print(connectivity_band)\n",
    "if not is_multivariate:\n",
    "    print(connectivity_hag_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b447f6-e964-4f6c-9116-597cd29c1755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:13:26.845564Z",
     "start_time": "2023-10-09T13:13:26.821527Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ffd9c-ffc5-47ea-885d-f2f2a3e549fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "N_JOBS = -1\n",
    "RIDGE_COEF= 1e-9 if ridge is None else 10**ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27380a6-3976-442d-8bb0-34bfc142985a",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08537a67-9097-4930-b5af-29659c8cfdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_instances_classification:\n",
    "    raise ValueError(\"This is not the right Classification section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b3a4e-b06e-450a-b7c5-68802be8013d",
   "metadata": {},
   "source": [
    "### Classification for multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3534f99-25b5-4653-8dcf-7073183e0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_band_duplicated example shape :\", X_train_band[1].shape)     \n",
    "print(\"We should have :\", X_train_band[0].shape[1], \"==\", common_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9130e-a834-4256-8f29-e42add2eee19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import init_and_train_model_for_classification\n",
    "# To remember : \n",
    "#  For reservoirpy   pre_s = W @ r + Win @ (u + noise_gen(dist=dist, shape=u.shape, gain=g_in)) + bias\n",
    "\n",
    "train_data_multi = X_train_band # X_train_band_noisy or X_train_band\n",
    "Y_data = Y_val if use_cross_validation else Y_test\n",
    "\n",
    "reservoir_hag_multi, readout_hag_multi = init_and_train_model_for_classification(W_hag_multi, Win_hag_multi, bias_hag_multi, leaky_rate, activation_function, train_data_multi, Y_train, N_JOBS, RIDGE_COEF, mode=\"sequence-to-vector\")\n",
    "\n",
    "reservoir_random_multi, readout_random_multi = init_and_train_model_for_classification(W_random_multi, Win_random_multi, bias_random_multi, leaky_rate, activation_function, train_data_multi, Y_train, N_JOBS, RIDGE_COEF, mode=\"sequence-to-vector\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e344e-de91-4709-bfce-467b1d049546",
   "metadata": {},
   "source": [
    "#### noisy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3dc743-36c8-4821-948d-f14f43af8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import predict_model_for_classification, compute_score\n",
    "\n",
    "test_data_multi_noisy = X_test_band_noisy # X_test_band_noisy or X_test_band\n",
    "\n",
    "Y_pred_hag_multi = predict_model_for_classification(reservoir_hag_multi, readout_hag_multi, test_data_multi_noisy, N_JOBS)\n",
    "score = compute_score(Y_pred_hag_multi, Y_data, is_instances_classification, function_name + \" multi\", verbosity=1)\n",
    "\n",
    "Y_pred_random_multi = predict_model_for_classification(reservoir_random_multi, readout_random_multi, test_data_multi_noisy, N_JOBS)\n",
    "score = compute_score(Y_pred_random_multi, Y_data, is_instances_classification, \"random multi\", verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6287d16-2f61-4d2a-97e3-0a0844918879",
   "metadata": {},
   "source": [
    "#### normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49735fb0-8701-4a4f-8d0c-a319c142f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_multi = X_test_band # X_test_band_noisy or X_test_band\n",
    "\n",
    "Y_pred_hag_multi = predict_model_for_classification(reservoir_hag_multi, readout_hag_multi, test_data_multi, -1)\n",
    "score = compute_score(Y_pred_hag_multi, Y_data, is_instances_classification, function_name + \" multi\", verbosity=1)\n",
    "\n",
    "Y_pred_random_multi = predict_model_for_classification(reservoir_random_multi, readout_random_multi, test_data_multi, N_JOBS)\n",
    "score = compute_score(Y_pred_random_multi, Y_data, is_instances_classification, \"random multi\", verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f3553-1d28-4340-b4b9-f3dc673bdcac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T13:59:05.097521Z",
     "start_time": "2023-10-09T13:59:04.992489Z"
    },
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Classification for univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50e2ca-93d7-48bb-bcff-4f885e8d5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_multivariate: \n",
    "    # Create a list to store the arrays with the same shape as the expected input of the reservoir\n",
    "\n",
    "    train_data_uni = [ts.reshape(-1, 1) for ts in X_train]\n",
    "    test_data_uni = [ts.reshape(-1, 1) for ts in X_test]\n",
    "\n",
    "    print(\"number of instances in train_data_uni :\", len(train_data_uni), \"should be equal to\", len(X_train))     \n",
    "    print(\"example of train_data_uni train shape :\", train_data_uni[0].shape)     \n",
    "    print(\"We should have :\", train_data_uni[0].shape[1], \"==\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d01e8-8763-4bf3-ae0a-4c4569231dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    reservoir_hag_uni, readout_hag_uni = init_and_train_model_for_classification(W_hag_uni, Win_hag_uni, bias_hag_uni, leaky_rate, activation_function, train_data_uni, Y_train, N_JOBS, RIDGE_COEF, mode=\"sequence-to-vector\")\n",
    "\n",
    "    reservoir_random_uni, readout_random_uni = init_and_train_model_for_classification(W_normal, Win_normal, bias_normal, leaky_rate, activation_function, train_data_uni, Y_train, N_JOBS, RIDGE_COEF, mode=\"sequence-to-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f4028-f457-46b4-a219-7f5fed575af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    Y_pred_hag_uni = predict_model_for_classification(reservoir_hag_uni, readout_hag_uni, test_data_uni, N_JOBS)\n",
    "    score = compute_score(Y_pred_hag_uni, Y_test, is_instances_classification, function_name + \" uni\", verbosity=1)\n",
    "    \n",
    "    Y_pred_normal = predict_model_for_classification(reservoir_random_uni, readout_random_uni, test_data_uni, N_JOBS)\n",
    "    score = compute_score(Y_pred_normal, Y_test, is_instances_classification, \"random uni\", verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7ab1a-e020-489a-8ff9-9bbd689de15e",
   "metadata": {},
   "source": [
    "## Prediction ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c06e5-71c1-4f72-ac0f-7e4f1f15afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_instances_classification:\n",
    "    raise ValueError(\"This is not the right Classification section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11c5fc-45b0-4180-9953-bb13800da95b",
   "metadata": {},
   "source": [
    "### Plot datasets\n",
    "Noisy or normal dataset can be ploted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d45a7-49c9-4e1e-a473-652bc90d64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test arrays for plotting\n",
    "combined_data = np.concatenate((X_train_band, X_val_band), axis=0)\n",
    "\n",
    "# noisy version\n",
    "combined_data_noisy = np.concatenate((X_train_band, X_val_band_noisy), axis=0)\n",
    "combined_Y =np.concatenate((Y_train, Y_val), axis=0)\n",
    "\n",
    "# Calculate the merge point index\n",
    "merge_point_index = X_train_band.shape[0]\n",
    "\n",
    "# Define the range around the merge point to plot\n",
    "start_index = merge_point_index - 100\n",
    "end_index = merge_point_index + 100\n",
    "\n",
    "# Plot for a subset N features within a range arround transition from train to test\n",
    "N = 3\n",
    "plt.figure(figsize=(16, 5))\n",
    "for i in [1, 13, 17]: \n",
    "    plt.plot(range(start_index, end_index), combined_data_noisy[start_index:end_index, i], label=f'Feature {i}')\n",
    "plt.plot(range(start_index, end_index), combined_Y[start_index:end_index], label=\"Prediction\")\n",
    "plt.title('Feature Values Around Merge Point')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Feature Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bcdbf",
   "metadata": {},
   "source": [
    "### Training\n",
    "Noisy or normal dataset can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d012581-4559-4924-8ba1-9d1b6dea2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import performances.esn_model_evaluation\n",
    "reload(performances.esn_model_evaluation)\n",
    "from performances.esn_model_evaluation import init_and_train_model_for_prediction\n",
    "\n",
    "if not is_multivariate:\n",
    "    train_data_uni = X_train # X_train_noisy or X_train\n",
    "    \n",
    "    # Training random + MG\n",
    "    esn_random_uni = init_and_train_model_for_prediction(W_normal, Win_normal, bias_normal, leaky_rate, activation_function, train_data_uni, Y_train, RIDGE_COEF)\n",
    "    \n",
    "    # Training for hag + MG\n",
    "    esn_hag_uni = init_and_train_model_for_prediction(W_hag_uni, Win_hag_uni, bias_hag_uni, leaky_rate, activation_function, train_data_uni, Y_train, RIDGE_COEF)\n",
    "\n",
    "    \n",
    "train_data_multi = X_train_band # X_train_band_noisy or train_band_inputs\n",
    "\n",
    "# Training random + bandfilter\n",
    "esn_random_multi = init_and_train_model_for_prediction(W_random_multi, Win_random_multi, bias_random_multi, leaky_rate, activation_function, train_data_multi, Y_train, RIDGE_COEF)\n",
    "\n",
    "# Training output HASDP + bandfilter\n",
    "esn_hag_multi = init_and_train_model_for_prediction(W_hag_multi, Win_hag_multi, bias_hag_multi, leaky_rate, activation_function, train_data_multi, Y_train, RIDGE_COEF)\n",
    "                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcfe07-1ad0-4d8f-827f-bb9776e8086e",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Noisy or normal dataset can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dac84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    test_data_uni = X_val # X_val_noisy or X_val\n",
    "\n",
    "    # Prediction for random + MG\n",
    "    y_pred_random_uni = esn_random_uni.run(test_data_uni, reset=False) \n",
    "\n",
    "    # Prediction for hag + MG\n",
    "    y_pred_hag_uni = esn_hag_uni.run(test_data_uni, reset=False) \n",
    "\n",
    "\n",
    "test_data_multi = X_val_band # X_test_band_noisy_duplicated or X_test_band_duplicated\n",
    "\n",
    "# Prediction for random + bandfilter\n",
    "y_pred_random_multi = esn_random_multi.run(test_data_multi, reset=False)\n",
    "\n",
    "# Prediction for hag + bandfilter\n",
    "y_pred_hag_multi = esn_hag_multi.run(test_data_multi, reset=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61165568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from plots.plots import plot_results\n",
    "from performances.esn_model_evaluation import compute_score\n",
    "\n",
    "START_STEP = 30\n",
    "END_STEP = 500\n",
    "slice_range = slice(START_STEP, END_STEP)\n",
    "\n",
    "if not is_multivariate:\n",
    "    print(\"nrmse normal         :\", compute_score(Y_val[slice_range], y_pred_random_uni[slice_range], is_instances_classification))\n",
    "    print(\"nrmse hag          :\", compute_score(Y_val[slice_range], y_pred_hag_uni[slice_range], is_instances_classification))\n",
    "\n",
    "print(\"nrmse random + band  :\", compute_score(Y_val[slice_range], y_pred_random_multi[slice_range], is_instances_classification))\n",
    "print(\"nrmse hag + band   :\", compute_score(Y_val[slice_range], y_pred_hag_multi[slice_range], is_instances_classification))\n",
    "\n",
    "plot_results(y_pred_hag_multi, Y_test, 0, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645bd84-e26a-497c-9053-6f6dd4e69845",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_multivariate:\n",
    "    test_data_uni = X_val_noisy # X_val_noisy or X_val\n",
    "\n",
    "    # Prediction for random + MG\n",
    "    y_pred_random_uni = esn_random_uni.run(test_data_uni, reset=False) \n",
    "\n",
    "    # Prediction for hag + MG\n",
    "    y_pred_hag_uni = esn_hag_uni.run(test_data_uni, reset=False) \n",
    "\n",
    "test_data_multi = X_val_band_noisy # X_test_band_noisy_duplicated or X_test_band_duplicated\n",
    "\n",
    "# Prediction for random + bandfilter\n",
    "y_pred_random_multi = esn_random_multi.run(test_data_multi, reset=False)\n",
    "\n",
    "# Prediction for hag + bandfilter\n",
    "y_pred_hag_multi = esn_hag_multi.run(test_data_multi, reset=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d132a1-0766-497f-bd98-635048b14b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots.plots import plot_results\n",
    "\n",
    "START_STEP = 30\n",
    "END_STEP = 500\n",
    "slice_range = slice(START_STEP, END_STEP)\n",
    "\n",
    "if not is_multivariate:\n",
    "    print(\"nrmse normal         :\", compute_score(y_pred_random_uni[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "    print(\"nrmse hag          :\", compute_score(y_pred_hag_uni[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse random + band  :\", compute_score(y_pred_random_multi[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse hag + band   :\", compute_score(y_pred_hag_multi[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "\n",
    "plot_results(y_pred_hag_multi, Y_test, 0, 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb0591-4213-425e-92ff-a5da80e718dc",
   "metadata": {},
   "source": [
    "#### Moving average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5bf2d-368f-4e55-b068-c42c36efe2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performances.esn_model_evaluation import compute_score\n",
    "\n",
    "# moving average of the y\n",
    "span=7\n",
    "pad_width = span // 2\n",
    "\n",
    "if not is_multivariate:\n",
    "    ave_y_random_uni = np.convolve(np.pad(y_pred_random_uni.flatten(), pad_width, mode='edge'), np.ones(span), 'valid') / span\n",
    "    ave_y_hag_uni = np.convolve(np.pad(y_pred_hag_uni.flatten(), pad_width, mode='edge') , np.ones(span), 'valid') / span\n",
    "ave_y_random_multi = np.convolve(np.pad(y_pred_random_multi.flatten(), pad_width, mode='edge'), np.ones(span), 'valid') / span\n",
    "ave_y_hag_multi = np.convolve(np.pad(y_pred_hag_multi.flatten(), pad_width, mode='edge'), np.ones(span), 'valid') / span\n",
    "\n",
    "if not is_multivariate:\n",
    "    print(\"nrmse normal         :\", compute_score(ave_y_random_uni[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "    print(\"nrmse hag          :\", compute_score(ave_y_hag_uni[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse random + band  :\", compute_score(ave_y_random_multi[slice_range], Y_val[slice_range], is_instances_classification))\n",
    "print(\"nrmse hag + band   :\", compute_score(ave_y_hag_multi[slice_range], Y_val[slice_range], is_instances_classification))\n",
    " \n",
    "plot_results(ave_y_hag_multi.reshape(-1,1), Y_test, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc19b97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nrmse_array_random_uni = []\n",
    "nrmse_array_hag_uni = []\n",
    "nrmse_array_random_multi = []\n",
    "nrmse_array_hag_multi = []\n",
    "\n",
    "for i in range(len(Y_val)-100- step_ahead):\n",
    "    Y_val_i = Y_val[i:100+i]\n",
    "    nrmse_array_random_uni.append(compute_score(Y_val_i, y_pred_random_uni[i:100+i], is_instances_classification))\n",
    "    nrmse_array_hag_uni.append(compute_score(Y_val_i, y_pred_hag_uni[i:100+i], is_instances_classification))\n",
    "    nrmse_array_random_multi.append(compute_score(Y_val_i, y_pred_random_multi[i:100+i], is_instances_classification))\n",
    "    nrmse_array_hag_multi.append(compute_score(Y_val_i, y_pred_hag_multi[i:100+i], is_instances_classification))\n",
    "    \n",
    "log10_nrmse_random_uni= np.log10(nrmse_array_random_uni)\n",
    "log10_nrmse_hag_uni = np.log10(nrmse_array_hag_uni)\n",
    "log10_nrmse_random_multi = np.log10(nrmse_array_random_multi)\n",
    "log10_nrmse_hag_multi = np.log10(nrmse_array_hag_multi)\n",
    "plt.figure()\n",
    "plt.plot(log10_nrmse_random_uni[:1000])\n",
    "plt.plot(log10_nrmse_hag_uni[:1000])\n",
    "plt.plot(log10_nrmse_random_multi[:1000])\n",
    "plt.plot(log10_nrmse_hag_multi[:1000])\n",
    "\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Log10 NRMSE')\n",
    "plt.legend([\"hag+band\", \"random\", \" random + bandfilter\", \"hag\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27da37",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2478c-bb79-4bc0-855f-b028109c8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# pretrain\n",
    "# Be really carefull of the column order here !\n",
    "df_data = scaler.fit_transform(X_pretrain_band.T)\n",
    "df_data = df_data.T\n",
    "df = pd.DataFrame(df_data.T)\n",
    "# Initialize a progress bar for total number of series\n",
    "progress_bar = tqdm(total=df.shape[1]**2, position=0, leave=True)\n",
    "\n",
    "# Initialize an empty correlation matrix\n",
    "correlation_matrix = pd.DataFrame(index=df.columns, columns=df.columns)\n",
    "\n",
    "# Calculate the correlation for each pair of series\n",
    "for col1 in df.columns:\n",
    "    progress_bar.set_description(f\"Processing {col1}\")\n",
    "    for col2 in df.columns:\n",
    "        correlation_matrix.loc[col1, col2] = df[col1].corr(df[col2], method='pearson', min_periods=5)\n",
    "\n",
    "        progress_bar.update(1)  # Update the progress bar after processing each series\n",
    "    \n",
    "progress_bar.close()\n",
    "\n",
    "# Convert correlation_matrix to numeric as it is stored as objects due to tqdm\n",
    "correlation_matrix = correlation_matrix.apply(pd.to_numeric)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linked = linkage(correlation_matrix, 'single')\n",
    "\n",
    "# Get the order of rows/columns after hierarchical clustering\n",
    "row_order = leaves_list(linked)\n",
    "\n",
    "# Reorder the correlation matrix\n",
    "sorted_corr_matrix = correlation_matrix.iloc[row_order, :].iloc[:, row_order]\n",
    "\n",
    "# Visualize the sorted correlation matrix with a heatmap\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(sorted_corr_matrix, annot=False, cmap='vlag', vmin=-1, vmax=1)\n",
    "plt.title('Clustered Pairwise Correlation of Time Series')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec25bb-f3e8-4cdb-87ee-9772460a8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "row_order_r = np.array([i + k for i in row_order*K for k in range(K)])\n",
    "\n",
    "# Convert the sparse matrix to a dense format (if memory allows)\n",
    "dense_matrix = W_hag_multi.toarray()\n",
    "\n",
    "# Reorder the dense matrix using the repeated ordering\n",
    "reordered_matrix = dense_matrix[np.ix_(row_order_r, row_order_r)]\n",
    "\n",
    "# Convert the reordered dense matrix back to a sparse format if needed\n",
    "sparse_reordered_matrix = coo_matrix(reordered_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901730be-d407-4d6d-973d-9fc24951bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(sparse_reordered_matrix.todense(), cmap=color_palette(\"vlag\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e46145-b737-4e62-a8ed-9860be34eb2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T12:52:00.989091Z",
     "start_time": "2023-10-10T12:52:00.984122Z"
    }
   },
   "source": [
    "## Motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78de13-7b2d-4ea0-9854-c7ef05c9780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis.topology\n",
    "reload(analysis.topology)\n",
    "from analysis.topology import motif_distribution, draw_motifs_distribution\n",
    "\n",
    "motifs_count = motif_distribution(W_hag_multi.A)\n",
    "draw_motifs_distribution(motifs_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7d48e-892f-44a0-9f33-a0b561e3a1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4480d-5e4f-4603-90c0-e2f0576e0288",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson, binom\n",
    "\n",
    "def analyze_connectivity_matrix(matrix):\n",
    "    # Extract weights from the matrix (ignoring the diagonal and zeros)\n",
    "    weights = matrix.flatten()\n",
    "    weights = weights[weights != 0]\n",
    "    bin_centers, counts = np.unique(weights, return_counts=True)\n",
    "    \n",
    "    # Calculate the difference for all centers\n",
    "    diffs = np.diff(bin_centers)\n",
    "    # Add the last difference for the last bin\n",
    "    diffs = np.append(diffs, diffs[-1])\n",
    "    \n",
    "    # Calculate the bin edges based on bin centers and differences\n",
    "    bin_edges = bin_centers - diffs/2\n",
    "    # Add the last bin edge\n",
    "    bin_edges = np.append(bin_edges, bin_centers[-1] + diffs[-1]/2)\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.bar(bin_centers, counts, align='center', alpha=0.6, width=np.diff(bin_centers).min())\n",
    "    \n",
    "    # Fit to Poisson distribution\n",
    "    lambda_est = np.mean(weights)\n",
    "    plt.plot(bin_centers, poisson.pmf(range(len(bin_centers)), lambda_est)*counts[0], 'r-', label='Poisson fit')\n",
    "    \n",
    "    # Fit to Binomial distribution using derived relations\n",
    "    mean = np.mean(weights)\n",
    "    variance = np.var(weights)\n",
    "    \n",
    "    # Calculate p and n estimates\n",
    "    p_est = mean ** 2 / (n * mean - variance) if (n * mean - variance) != 0 else 0\n",
    "    n_est = int(round(mean / p_est)) if p_est != 0 else 0  # n should be integer\n",
    "\n",
    "    # Check parameter validity\n",
    "    if not(0 < p_est < 1):\n",
    "        print(\"Estimated parameters are not valid for the Binomial distribution.\")\n",
    "    else:\n",
    "        x_vals = range(len(bin_centers))\n",
    "        plt.plot(bin_centers, binom.pmf(x_vals, n_est, p_est) * counts[0], 'g-', label='Binomial fit')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return {\"Poisson\": lambda_est, \"Binomial\": (n_est, p_est)}\n",
    "\n",
    "\n",
    "# Assuming W_hag_multi.A is your connectivity matrix\n",
    "analyze_connectivity_matrix(W_hag_multi.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbe35b-18fa-4dc7-b598-dfd345e90ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_centers = np.array([1.5, 3.5, 5.5, 7.5])\n",
    "poisson.pmf(np.arange(len([1, 2, 3, 4, 5, 6])), 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6dbf8-e09f-4062-a3ee-6c320981b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6614db-e2e1-4e5c-8d97-567646d6a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hag_multi.A.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995e386-8847-4da0-8197-c73ff68179bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hadsp_env",
   "language": "python",
   "name": "hadsp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
